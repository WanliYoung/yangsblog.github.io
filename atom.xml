<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yangyy&#39;s Life</title>
  
  <subtitle>这里是杨万里的生活</subtitle>
  <link href="https://yangyy.top/atom.xml" rel="self"/>
  
  <link href="https://yangyy.top/"/>
  <updated>2023-02-20T08:14:21.708Z</updated>
  <id>https://yangyy.top/</id>
  
  <author>
    <name>yangyy</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>推荐系统论文调研（SIGKDD、SIGIR）</title>
    <link href="https://yangyy.top/posts/3747160458.html"/>
    <id>https://yangyy.top/posts/3747160458.html</id>
    <published>2023-02-09T05:39:04.000Z</published>
    <updated>2023-02-20T08:14:21.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SIGIR-2022"><a class="header-anchor" href="#SIGIR-2022">¶</a>SIGIR 2022</h2><h3 id="Interpolative-Distillation-for-Unifying-Biased-and-Debiased-Recommendation"><a class="header-anchor" href="#Interpolative-Distillation-for-Unifying-Biased-and-Debiased-Recommendation">¶</a>Interpolative Distillation for Unifying Biased and Debiased Recommendation</h3><p>（用于统一<strong>有偏</strong>和去偏推荐的<strong>插值蒸馏</strong>）</p><p>Abstract：大多数推荐系统通过以下任一方式<strong>离线评估模型性能</strong>：1）对事实交互的正常偏差测试；或 2) 使用<strong>随机对照试验</strong>的记录进行去偏试验。事实上，这两个测试都只反映了全貌的一部分：事实交互是<strong>从推荐策略中收集</strong>的，更好地拟合它们意味着有利于平台获得<font color="red">更高的点击率或转化率</font>；相反，<font color="red">去偏测试消除了系统引起的偏差</font>，因此更能反映用户的真实偏好。尽管如此，我们发现现有模型在<strong>两个测试中表现出权衡</strong>，并且缺乏在<strong>两个测试中都表现良好</strong>的方法。 在这项工作中，我们的目标是开发一种双赢的推荐方法，该方法在两个测试中都很强大。这是非常重要的，因为它需要学习一个模型，该模型可以在<font color="red">事实环境（即正常的有偏测试）</font>和<font color="red">反事实环境（即去偏测试）</font>中做出准确的预测。为了实现这一目标，我们通过考虑这两种环境来执行环境感知推荐建模。特别是，我们提出了一个<font color="red">插值蒸馏 (InterD) 框架</font>，它通过蒸馏学生模型在用户-项目对级别对有偏和去偏模型进行插值。我们对三个真实世界的数据集进行了两个测试的实验。实证结果证明了 InterD 的合理性和有效性，它在两个测试中都脱颖而出，特别是在不太受欢迎的项目上表现出显着的进步。</p><p><font color="orange">（插值蒸馏+有偏测试+去偏测试）</font></p><h3 id="Graph-Trend-Filtering-Networks-for-Recommendation"><a class="header-anchor" href="#Graph-Trend-Filtering-Networks-for-Recommendation">¶</a>Graph Trend Filtering Networks for Recommendation   ***</h3><p>（用于推荐的图趋势过滤网络）</p><p>Abstract：推荐系统旨在为用户提供个性化服务，在我们的日常生活中发挥着越来越重要的作用。<strong>推荐系统的关键是根据用户的历史在线行为（例如点击、添加到购物车、购买等）来预测用户与项目交互的可能性有多大</strong>。为了利用这些用户-项目交互，越来越多的努力考虑<font color="red">将用户-项目交互作为用户-项目二分图</font>，然后通过图神经网络 (GNN) 在图中执行信息传播。鉴于 GNN 在图形表示学习中的强大功能，这些<strong>基于 GNN 的推荐方法显着提高了推荐性能</strong>。尽管取得了成功，但大多数现有的基于 GNN 的推荐系统都<font color="red">忽略了由不可靠行为（例如，随机/诱饵点击）产生的特征交互并统一处理所有交互</font>，这可能导致次优和不稳定的性能。在本文中，我们<font color="red">研究</font>了现有的基于 GNN 的推荐方法的<font color="red">缺点</font>（例如，非自适应传播和非鲁棒性）。为了解决这些缺点，我们<font color="red">引入了一种有原则的图趋势协同过滤方法，并提出了图趋势过滤网络推荐（GTN）</font>，它可以捕获交互的自适应可靠性。提出了综合实验和消融研究，以验证和理解所提出框架的有效性。我们基于 PyTorch 的实现可用：<a href="https://github.com/wenqifan03/GTN-SIGIR2022%E3%80%82">https://github.com/wenqifan03/GTN-SIGIR2022。</a></p><p><font color="orange">（用户-项目二分图、不可靠行为、CTN）</font></p><h3 id="HIEN-Hierarchical-Intention-Embedding-Network-for-Click-Through-Rate-Prediction"><a class="header-anchor" href="#HIEN-Hierarchical-Intention-Embedding-Network-for-Click-Through-Rate-Prediction">¶</a>HIEN: Hierarchical Intention Embedding Network for Click-Through Rate Prediction  ***</h3><p>（用于点击率预测的分层意图嵌入网络）</p><p>Abstract：点击率 (CTR) 预测在在线广告和推荐系统中起着重要作用，旨在估计用户点击特定项目的概率。<font color="red">特征交互建模和用户兴趣建模方法是CTR预测中的两个热门领域</font>，近年来得到了广泛的研究。然而，这些方法仍然有两个**<font color="red">局限性</font><strong>。首先，传统方法将项目属性视为ID特征，而<font color="red">忽略了属性之间的</font></strong>结构信息<strong>和</strong>关系依赖性**。其次，当从用户-项目交互中挖掘用户兴趣时，当前模型<font color="red">忽略了不同属性的用户意图和项目意图</font>，这<font color="red">缺乏可解释性</font>。基于这一观察，在本文中，我们提出了一种新方法<font color="red">层次意图嵌入网络 (HIEN)</font>，它在构造的属性图中基于自下而上的树聚合考虑属性的依赖关系。HIEN 还<font color="red">捕获</font>用户对不同项目属性的意图以及基于我们提出的分层注意机制的项目<font color="red">意图</font>。在公共和生产数据集上进行的大量实验表明，所提出的模型明显优于最先进的方法。此外，HIEN 可以<font color="red">用作最先进的 CTR 预测方法的输入模块</font>，为这些可能已经在实际系统中大量使用的现有模型带来<font color="red">进一步的性能提升</font>。</p><p><font color="orange">（分层(考虑结构信息、关系依赖)、意图(属性代表的意图)）</font></p><h3 id="NAS-CTR-Efficient-Neural-Architecture-Search-for-Click-Through-Rate-Prediction"><a class="header-anchor" href="#NAS-CTR-Efficient-Neural-Architecture-Search-for-Click-Through-Rate-Prediction">¶</a>NAS-CTR: Efficient Neural Architecture Search for Click-Through Rate Prediction</h3><p>（用于点击率预测的高效神经架构搜索）</p><p>Abstract：点击率（CTR）预测已广泛应用于许多机器学习任务，例如在线广告和个性化推荐。不幸的是，给定特定领域的数据集，从<font color="red">巨大的候选空间</font>中搜索有效的特征交互操作和组合需要大量的<font color="red">专家经验和计算成本</font>。最近，神经架构搜索（<font color="red">NAS</font>）在自动<font color="red">发现高质量网络架构</font>方面取得了巨大成功。然而，由于特征交互操作和组合的多样性，现有的基于 NAS 的工作将架构搜索视为离散搜索空间上的<font color="red">黑盒优化问题，效率低下</font>。因此，探索一种更有效的架构搜索方法至关重要。为了实现这个目标，我们提出了NAS-CTR，<font color="red">一种用于 CTR 预测的可区分神经架构搜索方法</font>。首先，我们设计了一个新颖且富有表现力的架构搜索空间和一个连续的松弛方案，使搜索空间可微。其次，我们将用于 CTR 预测的架构搜索制定为对架构具有离散约束的联合优化问题，并利用近端迭代来解决受约束的优化问题。此外，还提出了一种简单而有效的方法来消除跳跃连接的聚集。广泛的实验结果表明，NAS-CTR 在测试准确性和搜索效率方面都优于 SOTA 人工架构和其他基于 NAS 的方法。</p><p><font color="orange">（主要是优化寻找有效特征交互的过程，具体方法没看懂）</font></p><h3 id="Enhancing-CTR-Prediction-with-Context-Aware-Feature-Representation-Learning"><a class="header-anchor" href="#Enhancing-CTR-Prediction-with-Context-Aware-Feature-Representation-Learning">¶</a>Enhancing CTR Prediction with Context-Aware Feature Representation Learning  ***</h3><p>（通过上下文感知的特征表示学习增强CTR预测）</p><p>Abstract：CTR预测在现实世界中得到了广泛的应用。许多方法对特征交互进行建模以提高其性能。然而，大多数方法只为每个特征学习固定的表示，而没有考虑每个特征在不同<font color="red">上下文下的不同重要性</font>，导致性能较差。最近，有几种方法尝试<font color="red">学习特征表示的向量级权重</font>，以解决固定表示问题。然而，它们<font color="red">只产生线性变换来改进<strong>固定的特征表示</strong></font>，这些表示仍然不够灵活，无法捕捉不同上下文下每个特征的不同重要性。在本文中，我们提出了一个名为<font color="red">特征细化网络 (FRNet)</font> 的新模块，它在不同上下文中为每个特征<font color="red">学习位级（bit-level）的上下文感知特征表示</font>。FRNet 由两个关键组件组成：1）<font color="red">信息提取单元（IEU）</font>，它捕获上下文信息和交叉特征关系以指导上下文感知特征细化；2）<font color="red">互补选择门（CSGate）</font>，它自适应地将在 IEU 中学习的<font color="red">原始和互补特征表示与比特级权重相结合</font>。值得注意的是，FRNet 与现有的 CTR 方法正交，因此可以应用于许多现有的方法以提高其性能。进行了全面的实验来验证FRNet的有效性、效率和兼容性。</p><p><font color="orange">（特征在不同上下文中的重要性，不满足于权重向量，而通过FRNet进行特征的重构，可以直接加在现有的模型上）</font></p><h3 id="ESCM2-Entire-Space-Counterfactual-Multi-Task-Model-for-Post-Click-Conversion-Rate-Estimation"><a class="header-anchor" href="#ESCM2-Entire-Space-Counterfactual-Multi-Task-Model-for-Post-Click-Conversion-Rate-Estimation">¶</a>ESCM2: Entire Space  Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation.</h3><p>（用于点击后转化率估计的全空间反事实多任务模型）</p><p><font color="orange">（完全没看懂摘要，暂时搁置）</font></p><h3 id="On-Device-Next-Item-Recommendation-with-Self-Supervised-Knowledge-Distillation"><a class="header-anchor" href="#On-Device-Next-Item-Recommendation-with-Self-Supervised-Knowledge-Distillation">¶</a>On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation</h3><p>（具有自监督知识蒸馏的设备上的下一项推荐）</p><p>Abstract：基于会话的推荐系统 (SBR) 越来越受欢迎，因为它们可以在<font color="red">不依赖长期用户配置文件</font>的情况下预测用户兴趣，并支持<font color="red">免登录推荐</font>。现代推荐系统以完全基于服务器的方式运行。为了满足<font color="red">数百万用户的需求</font>，需要频繁的模型维护和对并发用户请求的高速处理，这是以巨大的碳足迹为代价的。同时，用户需要将他们的行为<font color="red">数据</font>甚至包括即时环境上下文<font color="red">上传到服务器</font>，引发公众对隐私的关注。设备上的推荐系统通过<font color="red">注重成本的设置和本地推理</font>来规避这两个问题。然而，由于内存和计算资源有限，设备端推荐系统面临着两个基本挑战：(1) <font color="red">如何缩小常规模型</font>的尺寸以适应边缘设备？(2)如何<font color="red">保留原有容量</font>？</p><p>以往的研究大多采用张量分解技术以低压缩率压缩常规推荐模型，以避免性能急剧下降。在本文中，我们通过<font color="red">放松张量分解中维度一致性的约束</font>，探索用于下一项推荐的<font color="red">超紧凑模型</font>。<strong>为了补偿压缩造成的容量损失，我们开发了一个自我监督的知识蒸馏框架</strong>，使压缩模型（学生）能够提取原始数据中的基本信息，并通过嵌入改进长尾项目推荐。与原始模型（教师）的重组策略。在两个基准测试中进行的大量实验表明，在尺寸减小 30 倍的情况下，压缩模型几乎没有精度损失，甚至优于其未压缩的对应物。代码发布在https://github.com/xiaxin1998/OD-Rec。</p><p><font color="orange">（主要问题就是缩小模型尺寸的情况下尽可能保留原有容量，通过放松约束+自监督的知识蒸馏框架）</font></p><h3 id="Single-shot-Embedding-Dimension-Search-in-Recommender-System"><a class="header-anchor" href="#Single-shot-Embedding-Dimension-Search-in-Recommender-System">¶</a>Single-shot Embedding Dimension Search in Recommender System  ***</h3><p>（推荐系统中的单次嵌入维度搜索）</p><p>Abstract：作为大多数现代<font color="red">深度</font>推荐系统的重要组成部分，<font color="red">特征嵌入</font>将<font color="red">高维稀疏</font>用户/项目特征映射到<font color="red">低维密集</font>嵌入。然而，这些嵌入<font color="red">通常被分配一个统一的维度</font>，存在以下问题：（1）高内存使用和计算成本。(2) 由于次优维度分配导致的次优性能。为了缓解上述问题，一些工作通过将其表述为<font color="red">超参数优化</font>或<font color="red">嵌入剪枝问题</font>来<font color="red">关注自动嵌入维度搜索</font>。但是，它们要么需要为超参数设计良好的搜索空间，要么需要耗时的优化过程。在本文中，我们提出了一种称为 SSEDS 的 Single-Shot Embedding Dimension Search 方法，它可以通过<font color="red">单次嵌入修剪操作</font>有效地<font color="red">为每个特征字段分配维度</font>，同时保持模型的推荐准确性。具体来说，它<font color="red">引入了一个标准</font>来识别每个特征字段的<font color="red">每个嵌入维度的重要性</font>。因此，SSEDS 可以根据相应的维度重要性排名和预定义的参数预算，通过<font color="red">显式减少冗余嵌入维度</font>来自动获得混合维度嵌入。此外，拟议的 SSEDS 与模型无关，这意味着它可以<font color="red">集成到不同的基础推荐模型</font>中。在 CTR（点击率）预测任务的两个广泛使用的公共数据集上进行了广泛的离线实验，结果表明，即使减少了 90% 的参数，SSEDS 仍然可以实现强大的推荐性能。此外，SSEDS还部署在微信订阅平台上，提供实用的推荐服务。为期7天的在线A/B测试结果表明，SSEDS能够显着提升在线推荐模型的性能，同时降低资源消耗。</p><p><font color="orange">（通过单次嵌入修剪为不同特征选择合适的维度，从而很大程度减少了模型的参数，引入了一个标准表示模型嵌入维度的重要性）</font></p><p><font color="red"><strong>可以发现，基本上对嵌入层的修改都是可以直接加在现有模型上进行优化的，目前已经看到三篇这种类型的文章了。</strong></font></p><h3 id="Forest-based-Deep-Recommender"><a class="header-anchor" href="#Forest-based-Deep-Recommender">¶</a>Forest-based Deep Recommender</h3><p>（基于森林的深度推荐系统）</p><p>随着深度学习技术的发展，深度推荐模型在推荐准确率方面也取得了显着提升。然而，由于实践中<font color="red">候选项目数量众多</font>，<font color="red">偏好计算成本高</font>，这些方法也存在推荐效率低下的问题。最近提出的基于树的深度推荐模型通过<font color="red">在推荐目标的指导下直接学习树结构</font>和表示来缓解这个问题。然而，这样的模型有两个缺点。首先，分层树中的最大堆假设，其中父节点的偏好应该是其子节点偏好中的最大值，在他们的二元分类目标中很难满足。其次，学习索引只包括一棵树。</p><p>为此，我们提出了一种基于深度森林的推荐器（简称 DeFoRec）来进行高效推荐。在 DeFoRec 中，保留训练过程中生成的所有树以形成森林。在学习每棵树的节点表示时，我们必须尽可能满足最大堆假设，并在训练阶段模仿树上的波束搜索行为。这是通过 DeFoRec 将训练任务视为同一级别树节点上的多分类来实现的。然而，树节点的数量随着级别呈指数增长，这使得我们可以在 sampled-softmax 技术的指导下训练偏好模型。实验是在真实世界的数据集上进行的，验证了所提出的偏好模型学习方法和树学习方法的有效性。</p><h3 id="Explainable-Fairness-in-Recommendation"><a class="header-anchor" href="#Explainable-Fairness-in-Recommendation">¶</a>Explainable Fairness in Recommendation</h3><p>（推荐中的可解释公平性）</p><p>Abstract：现有关于公平感知推荐的研究主要集中在<font color="red">公平的量化</font>和<font color="red">公平推荐模型</font>的开发上，两者都没有研究一个更实质性的问题——识别<font color="red">推荐中模型差异的根本原因</font>。此信息对于推荐系统设计者了解内在推荐机制并提供有关如何提高决策者模型公平性的见解至关重要。幸运的是，随着 Explainable AI 的快速发展，我们可以使用模型可解释性来深入了解模型（不）公平性。在本文中，我们研究了可解释的公平性问题，这<font color="red">有助于深入了解系统为何公平或不公平</font>，并以更明智和统一的方法指导公平推荐系统的设计。特别，我们关注具有特征感知推荐和<font color="red">暴露不公平</font>的常见设置，但所提出的可解释公平框架是通用的，可以应用于其他推荐设置和公平定义。我们提出了一个称为 CEF 的反事实可解释公平性框架，它生成关于<font color="red">模型公平性的解释</font>，可以在不显着损害性能的情况下提高公平性。CEF框架制定了一个优化问题来学习输入特征的“最小”变化，从而将推荐结果<font color="red">改变到一定程度的公平性</font>。基于每个特征的反事实推荐结果，我们根据fairness-utility trade-off 对所有基于特征的解释进行排序，并选择顶部的作为公平性解释。</p><h3 id="User-controllable-Recommendation-Against-font-color-red-Filter-font-Bubble"><a class="header-anchor" href="#User-controllable-Recommendation-Against-font-color-red-Filter-font-Bubble">¶</a>User-controllable Recommendation Against <font color="red">Filter</font> Bubble</h3><p>（针对过滤气泡的用户可控推荐）</p><p>Abstract：推荐系统通常面临过滤气泡的问题：基于用户特征和历史交互<font color="red">过度推荐同类项目</font>。过滤气泡会沿着反馈循环增长，无意中<font color="red">缩小用户兴趣范围</font>。现有工作通常通过<font color="red">结合准确性以外的目标（例如多样性和公平性</font>）来减轻过滤气泡。然而，它们通常会<font color="red">牺牲准确性</font>，损害模型保真度和用户体验。更糟糕的是，<font color="red">用户</font>不得不<font color="red">被动地接受</font>推荐策略并以具有高延迟的<font color="red">低效方式影响系统</font>，例如，不断提供反馈（例如，喜欢和不喜欢）直到系统识别出用户意图。</p><p>这项工作提出了一种称为<font color="red">用户可控推荐系统 </font>(UCRS) 的新推荐器原型，它使<font color="red">用户能够主动控制过滤气泡的缓解</font>。在功能上，1) UCRS 可以在用户深深陷入过滤气泡时<font color="red">提醒</font>用户。2) UCRS 支持<font color="red">四种控制命令</font>供用户在不同粒度上<font color="red">缓解气泡</font>。3) UCRS 可以响应控制并即时调整建议。<font color="red">调整的关键在于阻止过时的用户表征对推荐的影响</font>，其中包含与控制命令不一致的历史信息。因此，我们开发了一个<font color="red">因果关系增强的用户可控推理 (UCI) 框架</font>，它可以在推理阶段根据用户控制快速修改推荐，并利用反事实推理来减轻过时用户表示的影响。在三个数据集上的实验验证了 UCI 框架可以根据用户控制有效地推荐更多想要的项目，在<font color="red"><strong>准确性和多样性</strong></font>方面都表现出了良好的性能。</p><p><font color="orange">（过滤气泡问题、多样性牺牲公平性、用户被动低效反馈，提出<strong>用户可控</strong>的推荐系统、修改历史表征的影响）</font></p><h3 id="Unify-Local-and-Global-Information-for-Top-N-Recommendation"><a class="header-anchor" href="#Unify-Local-and-Global-Information-for-Top-N-Recommendation">¶</a>Unify Local and Global Information for Top-N Recommendation</h3><p>（统一本地和全局信息用于Top-N推荐）</p><p>Abstract：<font color="red">知识图谱 (KG)</font> 集成了复杂的信息并包含丰富的语义，被广泛认为是增强推荐系统的辅助信息。然而，大多数现有的基于知识图谱的方法都<font color="red">集中在对图中的结构信息进行编码</font>，而<font color="red">没有利用用户-项目交互数据中的协作信号</font>，这对于理解用户偏好很重要。因此，这些模型学习到的表示不足以表示推荐环境中用户和项目的语义信息。<font color="red">两种数据的结合</font>为解决这个问题提供了一个很好的机会，但它面临以下挑战：i）用户-项目交互数据中的内在关联很难从用户或项目的一侧捕获；ii) 捕获<font color="red">整个 KG 上的知识关联会引入噪声</font>并对推荐结果产生不同程度的影响；iii) 两种数据之间的<font color="red">语义鸿沟</font>难以消除。</p><p>为了解决这一研究差距，我们提出了一种名为 KADM 的新型二重奏表示学习框架，<font color="red">以融合局部信息（用户-项目交互数据）和全局信息（外部知识图）以进行前 N 推荐</font>，它由<font color="red">两个独立的子模型</font>组成楷模。一种通过使用知识感知共同注意机制发现局部信息中的内部相关性来学习局部表示，另一种通过使用关系感知注意网络对全局信息中的知识关联进行编码来学习全局表示。这两个子模型作为语义融合网络的一部分被<font color="red">联合训练</font>以计算用户偏好，这区分了两个子模型在特殊上下文下的贡献。我们对两个真实世界的数据集进行实验，评估表明，KADM 明显优于最先进的方法。进一步的消融研究证实，二重架构在推荐任务上的<font color="red">表现明显优于任一子模型</font>。</p><p><font color="orange">（目前专注于知识图谱的全局信息学习，但是缺乏对用户项目特征交互的关注，目前来看这两者有一定冲突、会互相影响，提出一个双重架构的模型，两个独立的子模型联合训练）</font></p><h3 id="Less-is-More-Reweighting-Important-Spectral-Graph-Features-for-Recommendation"><a class="header-anchor" href="#Less-is-More-Reweighting-Important-Spectral-Graph-Features-for-Recommendation">¶</a>Less is More: Reweighting Important Spectral Graph Features for Recommendation</h3><p>(为推荐重新赋权重要的光谱图特征)</p><p>Abstract：尽管图卷积网络 (GCN) 在推荐系统和协同过滤 (CF) 方面取得了巨大的成功，但它们，尤其是<font color="red">核心组件</font>（\textiti.e.，<font color="red">邻域聚合</font>）如何促进推荐的机制并没用被理想地研究。为了揭示 GCN 在推荐方面的有效性，我们首先从谱的角度对其进行分析，并发现了两个重要发现：（1）只有一小部分强调邻域平滑度和差异的谱图特征有助于推荐的准确性，而<font color="red">大多数图信息可以被视为甚至降低性能的噪声，</font>并且（2）邻域聚合的重复强调平滑的特征并以无效的方式滤除噪声信息。基于以上两个发现，我们提出了一种新的 GCN 学习方案，通过用简单而有效的<font color="red">图去噪编码器 (GDE) 代替邻域聚合来进行推荐</font>，它充当带通滤波器来捕获重要的图特征。我们表明，我们提出的方法减轻了过度平滑，并且与可以考虑任意跳邻域的无限层 GCN 相当。最后，我们在不引入额外复杂性的情况下动态调整负样本的梯度以加快模型训练。对五个真实世界数据集的广泛实验表明，我们提出的方法不仅优于最先进的技术，而且比 LightGCN 实现了 12 倍的加速。</p><h3 id="A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation"><a class="header-anchor" href="#A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation">¶</a>A Review-aware Graph Contrastive Learning Framework for Recommendation</h3><p>（一个用于推荐的评论感知图对比学习框架）</p><p>Abstract：大多数现代推荐系统通过两个部分预测用户的偏好：<font color="red">用户和项目嵌入学习</font>，然后是用户-项目<font color="red">交互建模</font>。通过利用伴随<font color="red">用户评分的辅助评论信息</font>，许多现有的基于评论的推荐模型<font color="red">通过</font>历史评论或在可用的用户-项目目标<font color="red">评论</font>的帮助下<font color="red">更好地建模</font>用户-项目交互来丰富用户/项目嵌入学习能力。尽管取得了重大进展，但我们认为当前基于评论的推荐解决方案存在两个缺点。首先，由于基于评论的推荐<font color="red">可以自然地形成一个用户-项目二分图</font>，具有来自相应用户-项目评论的边缘特征，如何更好地利用这种独特的图结构进行推荐？第二，虽然当前大多数模型的用户行为有限，但我们能否利用<font color="red">评论感知图中独特的自我监督信号</font>来更好地指导两个推荐组件？为此，在本文中，我们提出了一种新颖的<font color="red">评论感知图对比学习 (RGCL) 框架</font>，用于基于评论的推荐。具体来说，我们首先构建一个具有<font color="red">来自评论的特征增强边的评论感知用户-项目图</font>，其中每个边缘特征由<font color="red">用户-项目评分和相应的评论语义组成</font>。这个带有<font color="red">特征增强边的图</font>可以帮助仔细学习每个邻居节点的权重，用于用户和项目表示学习。之后，我们设计了两个额外的对比学习任务（即 Node Discrimination and Edge Discrimination）为推荐过程中的两个组件提供自监督信号。最后，对五个基准数据集的广泛实验证明了我们提出的 RGCL 与最先进的基线相比的优越性。</p><p><font color="orange">（用户-项目二分图+评论感知的特征增强边+对比学习）</font></p><h3 id="Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation"><a class="header-anchor" href="#Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation">¶</a>Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation</h3><p>（图扩充是否必要？：用于推荐的简单图对比学习）</p><p>Abstract：对比学习 (CL) 最近在推荐领域激发了一系列富有成果的研究，因为它<font color="red">从原始数据中提取自我监督信号的能力</font>与推荐系统解决<font color="red">数据稀疏性问题的需求</font>完全一致。基于 CL 的推荐模型的典型流程是<font color="red">首先使用结构扰动扩充用户-项目二部图</font>，然后最大化不同图扩充之间的节点表示一致性。尽管这种范式被证明是有效的，但性能提升的基础仍然是个谜。在本文中，我们首先通过实验揭示，在基于 CL 的推荐模型中，<font color="red">CL 通过学习更统一的用户/项目表示来运作</font>，这可以隐含地减轻流行偏差。同时，我们揭示了<font color="red">图形扩充</font>，曾经被认为是必要的，<font color="red">只是起到了微不足道的作用</font>。基于这一发现，我们提出了一种<font color="red">简单的 CL 方法</font>，该方法丢弃图形扩充，而是<font color="red">向嵌入空间添加均匀噪声以创建对比视图</font>。对三个基准数据集的综合实验研究表明，虽然它看起来非常简单，但所提出的方法可以平滑地调整学习表示的均匀性，并且在推荐准确性和训练效率方面比基于图增强的方法具有明显的优势。代码发布在 <a href="https://github.com/Coder-Yu/QRec%E3%80%82">https://github.com/Coder-Yu/QRec。</a></p><p><font color="orange">（对比学习通常被认为和图扩充相关，但是实际并不相关，因此丢弃图形扩充）</font></p><h3 id="AutoLossGen-Automatic-Loss-Function-Generation-for-Recommender-Systems"><a class="header-anchor" href="#AutoLossGen-Automatic-Loss-Function-Generation-for-Recommender-Systems">¶</a>AutoLossGen: Automatic Loss Function Generation for Recommender Systems</h3><p>（推荐系统的自动损失函数生成）</p><p>Abstract：在推荐系统中，损失函数的选择至关重要，因为良好的损失可能会显着提高模型性能。然而，由于问题的复杂性，<font color="red">手动设计一个好的损失是一个很大的挑战</font>。以前的大部分工作都集中在手工制作的损失函数上，这需要大量的专业知识和人力。在本文中，受最近<font color="red">自动化机器学习发展</font>的启发，我们提出了一种<font color="red">自动损失函数生成框架 AutoLossGen</font>，它能够直接从基本数学运算符构建生成损失函数，而<font color="red">无需先验损失结构知识</font>。更具体地说，我们开发了一个由<font color="red">强化学习驱动</font>的控制器模型来生成损失函数，并<font color="red">制定迭代和交替优化计划</font>来更新控制器模型和推荐模型的参数。推荐系统中自动损失生成的一个挑战是推荐<font color="red">数据集的极度稀疏性</font>，这导致了损失生成和搜索的稀疏奖励问题。为了解决这个问题，我们进一步开发了一种<font color="red">奖励过滤机制</font>，以有效地产生损失。实验结果表明，我们的框架设法为不同的推荐模型和数据集创建定制的损失函数，并且生成的损失比常用的基线损失提供更好的推荐性能。此外，<font color="red">大部分产生的损失是可转移</font>的，即基于一个模型和数据集产生的损失也适用于另一个模型或数据集。</p><h1>对比学习专题</h1><p>（感觉数据/特征增强似乎是其中一个很常见的技术）</p><h2 id="SIGIR2022"><a class="header-anchor" href="#SIGIR2022">¶</a>SIGIR2022</h2><h3 id="Hypergraph-Contrastive-Collaborative-Filtering"><a class="header-anchor" href="#Hypergraph-Contrastive-Collaborative-Filtering">¶</a>Hypergraph Contrastive Collaborative Filtering</h3><p>（超图对比协同过滤）</p><p>Abstract：<font color="red">协同过滤 (CF) </font>已成为将用户和项目参数化到潜在表示空间的基本范例，以及来自交互数据的相关模式。在各种 CF 技术中，基于 GNN 的推荐系统（例如 PinSage 和 LightGCN）的开发提供了最先进的性能。然而，现有解决方案尚未很好地探索两个关键挑战：i）更深层次的基于图的 CF 架构的<font color="red">过度平滑效应</font>可能导致无法区分的用户表示和推荐结果的退化。ii) 监督信号（即用户-项目交互）在现实中通常稀缺且分布不均，这限制了 CF 范式的表示能力。为了应对这些挑战，<font color="red">我们提出了一种新的自监督推荐框架超图对比协同过滤（HCCF）</font>，以<font color="red"><strong>通过超图增强的跨视图对比学习架构共同捕获本地和全球协作关系</strong></font>。特别是，设计的超图结构学习增强了基于 GNN 的 CF 范式的辨别能力，<font color="red">全面捕获用户之间复杂的高阶依赖关系</font>。此外，我们的 HCCF 模型有效地将超图结构编码与自我监督学习相结合，以基于超图自判别来增强推荐系统的表示质量。在三个基准数据集上进行的大量实验证明了我们的模型优于各种最先进的推荐方法，以及针对稀疏用户交互数据的鲁棒性。实现代码可在 <a href="https://github.com/akaxlh/HCCF">https://github.com/akaxlh/HCCF</a> 获得。</p><h3 id="A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation-v2"><a class="header-anchor" href="#A-Review-aware-Graph-Contrastive-Learning-Framework-for-Recommendation-v2">¶</a>A Review-aware Graph Contrastive Learning Framework for Recommendation</h3><p>（一个用于推荐的评论感知图对比学习框架）</p><p>Abstract：大多数现代推荐系统通过两个部分预测用户的偏好：<font color="red">用户和项目嵌入学习</font>，然后是<font color="red">用户-项目交互建模</font>。通过利用伴随用户评分的<font color="red">辅助评论信息</font>，许多现有的基于评论的推荐模型通过历史评论来<font color="red">丰富用户/项目嵌入学习能力</font>或在可用的用户-项目目标评论的帮助下<font color="red">更好地建模用户-项目交互</font>。尽管取得了重大进展，但我们认为当前基于评论的推荐解决方案存在两个缺点。首先，由于基于评论的推荐<font color="red">可以自然地形成一个用户-项目二分图</font>，具有来自相应<font color="red">用户-项目评论的边缘特征（edge features）</font>，如何更好地利用这种独特的图结构进行推荐？第二，虽然当前大多数模型的用户行为有限，但我们能否利用<font color="red">评论感知图中独特的自我监督信号来更好地指导两个推荐组件</font>？为此，在本文中，我们提出了一种新颖的<font color="red">评论感知图对比学习 (RGCL) 框架</font>，用于基于评论的推荐。具体来说，我们<font color="red">首先构建一个</font>具有来自评论的特征增强边的<font color="red">评论感知用户-项目图</font>，其中每个边缘特征由用户-项目评分和相应的评论语义组成。这个带有特征增强边的图可以帮助仔细学习每个邻居节点的权重，用于用户和项目表示学习。之后，我们设计了两个额外的对比学习任务（即 Node Discrimination and Edge Discrimination）为推荐过程中的两个组件提供自监督信号。最后，对五个基准数据集的广泛实验证明了我们提出的 RGCL 与最先进的基线相比的优越性。</p><h3 id="Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation-v2"><a class="header-anchor" href="#Are-Graph-Augmentations-Necessary-Simple-Graph-Contrastive-Learning-for-Recommendation-v2">¶</a>Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation</h3><p>（图扩充是否必要？：用于推荐的简单图对比学习）</p><p>Abstract：对比学习 (CL) 最近在推荐领域激发了一系列富有成果的研究，因为<font color="red">它从原始数据中提取自我监督信号的能力与推荐系统解决数据稀疏性问题的需求完全一致。</font>基于 CL 的推荐模型的典型流程是<font color="red">首先使用结构扰动扩充用户-项目二部图</font>，然后<font color="red">最大化不同图扩充之间的节点表示一致性</font>。尽管这种范式被证明是有效的，但性能提升的基础仍然是个谜。在本文中，我们首先通过实验揭示，在基于 CL 的推荐模型中，<font color="red">CL 通过学习更统一的用户/项目表示来运作，这可以隐含地减轻流行偏差</font>。同时，我们揭示了<font color="red">图形扩充</font>，曾经被认为是必要的，<font color="red">只是起到了微不足道的作用</font>。基于这一发现，我们提出了一种简单的 CL 方法，该方法<font color="red">丢弃图形扩充</font>，而是向嵌入空间添加均匀噪声以创建对比视图。对三个基准数据集的综合实验研究表明，虽然它看起来非常简单，但所提出的方法可以平滑地调整学习表示的均匀性，并且在推荐准确性和训练效率方面比基于图增强的方法具有明显的优势。代码发布在 <a href="https://github.com/Coder-Yu/QRec%E3%80%82">https://github.com/Coder-Yu/QRec。</a></p><h3 id="Multi-level-Cross-view-Contrastive-Learning-for-Knowledge-aware-Recommender-System"><a class="header-anchor" href="#Multi-level-Cross-view-Contrastive-Learning-for-Knowledge-aware-Recommender-System">¶</a>Multi-level Cross-view Contrastive Learning for Knowledge-aware Recommender System</h3><p>（知识感知推荐系统的多层交叉试图对比学习）</p><p>Abstract：<font color="red">知识图谱（KG）</font>在推荐系统中扮演着越来越重要的角色。最近，基于图神经网络（GNNs）的模型逐渐成为知识感知推荐（KGR）的主题。然而，基于 GNN 的 KGR 模型<font color="red">存在一个天然的缺陷，即稀疏监督信号问题</font>，这可能使其实际性能有所下降。受近期<font color="red">对比学习在从数据本身挖掘监督信号方面取得的成功</font>的启发，在本文中，我们专注于<font color="red">探索 KG 感知推荐中的对比学习</font>，并提出了一种新的<font color="red">多层次跨视图对比学习机制</font>，称为 MCCLK。与传统的对比学习方法不同，<font color="red">传统的对比学习方法</font>通过统一的数据增强方案（如损坏或丢弃）生成两个图视图，我们综合考虑了 KG 感知推荐的三种不同图视图，<font color="red">包括全局级结构视图、局部级协作和语义视图</font>。具体来说，我们将用户-项目图视为协作视图，将项目-实体图视为语义视图，将用户-项目-实体图视为结构视图。因此，<font color="red">MCCLK 在本地和全局级别的三个视图中执行对比学习，以自我监督的方式挖掘综合图形特征和结构信息。</font>此外，在语义视图中，提出了一种k-最近邻（k NN）项-项语义图构建模块，以捕获先前工作通常忽略的重要项-项语义关系。在三个基准数据集上进行的大量实验表明，我们提出的方法优于现有技术。这些实现可在以下位置获得：<a href="https://github.com/CCIIPLab/MCCLK%E3%80%82">https://github.com/CCIIPLab/MCCLK。</a></p><h3 id="Knowledge-Graph-Contrastive-Learning-for-Recommendation"><a class="header-anchor" href="#Knowledge-Graph-Contrastive-Learning-for-Recommendation">¶</a>Knowledge Graph Contrastive Learning for Recommendation</h3><p>（用于推荐的知识图对比学习）</p><p>Abstract：知识图 (KG) 已被用作有用的辅助信息来提高推荐质量。在这些推荐系统中，知识图谱信息通常<font color="red">包含丰富的事实</font>和<font color="red">项目之间固有的语义相关性</font>。然而，此类方法的成功依赖于高质量的知识图谱，并且可能无法学习具有两个挑战的质量表示：i）<font color="red">实体的长尾分布导致 KG 增强项目表示的<strong>监督信号稀疏</strong></font>；ii) 真实世界的知识图通常是嘈杂的，并且包含项目和实体之间<font color="red">与主题无关的连接</font>。这种 KG 稀疏性和噪声使得项目-实体依赖关系偏离反映其真实特征，这显著放大了噪声效应并阻碍了用户偏好的准确表示。</p><p>为了填补这一研究空白，我们设计了一个<font color="red">通用的知识图谱对比学习框架 (KGCL)</font>，它可以减轻知识图谱增强推荐系统的信息噪声。具体来说，我们提出了一种<font color="red">知识图增强模式来抑制信息聚合中的 KG 噪声</font>，并为项目推导出更稳健的知识感知表示。此外，我们利用来自 KG 增强过程的额外监督信号来指导交叉视图对比学习范式，在梯度下降中为无偏见的用户-项目交互提供更大的作用，并进一步抑制噪声。在三个公共数据集上进行的广泛实验证明了我们的 KGCL 优于最先进技术的一贯优势。KGCL 还在具有稀疏用户-项目交互的推荐场景中实现了强大的性能，长尾和嘈杂的 KG 实体。我们的实现代码可在 <a href="https://github.com/yuh-yang/KGCL-SIGIR22">https://github.com/yuh-yang/KGCL-SIGIR22</a> 获得。</p><h2 id="KDD2022"><a class="header-anchor" href="#KDD2022">¶</a>KDD2022</h2><h3 id="CrossCBR-Cross-view-Contrastive-Learning-for-Bundle-Recommendation"><a class="header-anchor" href="#CrossCBR-Cross-view-Contrastive-Learning-for-Bundle-Recommendation">¶</a>CrossCBR: Cross-view Contrastive Learning for Bundle Recommendation</h3><p>（用于捆绑推荐的跨视图对比学习）</p><p>Abstract：捆绑推荐旨在向用户推荐一组相关的物品，能够<font color="red">一站式地方便地满足用户的各种需求</font>。最近的方法通常利用<font color="red">用户-捆绑</font>和<font color="red">用户-项目交互信息</font>来获得用户和捆绑的信息表示，分别对应于捆绑视图和项目视图。然而，他们要么使用没有差异化的统一视图，要么松散地结合两个独立视图的预测，而<font color="red">忽略了两个视图表示之间至关重要的协作关联。</font></p><p>在这项工作中，我们建议<font color="red">通过跨视图对比学习来模拟两种不同视图之间的合作关联</font>。通过鼓励两个单独学习的视图对齐，每个视图都可以从另一个视图中提取互补信息，<font color="red">从而实现相互增强</font>。此外，通过扩大不同用户/捆绑的分散度，增强了表示的自辨别能力。在三个公共数据集上进行的大量实验表明，我们的方法大大优于 SOTA 基线。同时，我们的方法需要最少的三组嵌入（用户、捆绑和项目）参数，并且由于更简洁的图结构和图学习模块，计算成本大大降低。此外，各种消融和模型研究揭开了工作机制的神秘面纱并证明了我们的假设。代码和数据集可在 <a href="https://github.com/mysbupt/CrossCBR">https://github.com/mysbupt/CrossCBR</a> 获得。</p><h3 id="COSTA-Covariance-Preserving-Feature-Augmentation-for-Graph-Contrastive-Learning"><a class="header-anchor" href="#COSTA-Covariance-Preserving-Feature-Augmentation-for-Graph-Contrastive-Learning">¶</a>COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning</h3><p>（图对比学习的协方差保持特征增强）</p><p>Abstract：<font color="red">图对比学习 (GCL) </font>改进了图表示学习，导致各种下游任务的 SOTA。<font color="red">图扩充步骤是 GCL 的一个重要但很少被研究的步骤</font>。在本文中，我们表明通过<font color="red">图扩充获得的节点嵌入是高度有偏的</font>，在一定程度上限制了对比模型从学习下游任务的判别特征。因此，我们不是研究输入空间中的图扩充，而是建议对隐藏的特征增强。受所谓的矩阵草图的启发，我们提出了 COSTA，这是一种用于 GCL 的新型协方差保留<font color="red">特征空间增强框架</font>，它通过维护原始特征的“良好草图”来<font color="red">生成增强特征</font>。为了突出使用 COSTA 进行特征增强的优越性，我们研究了一种节省内存和计算的单视图设置（除了多视图设置）。我们表明，使用 COSTA 进行的特征扩充与基于图扩充的模型相比取得了可比/更好的结果。</p><h3 id="Contrastive-Cross-domain-Recommendation-in-Matching"><a class="header-anchor" href="#Contrastive-Cross-domain-Recommendation-in-Matching">¶</a>Contrastive Cross-domain Recommendation in Matching</h3><p>（匹配中的对比跨域推荐）</p><p>Abstract：<font color="red">跨域推荐（CDR）旨在借助源域在目标域中提供更好的推荐结果</font>，在现实系统中被广泛使用和探索。然而，<font color="red">匹配（即候选生成）模块中的 CDR 在表示学习和知识迁移中都与<strong>数据稀疏性</strong>和<strong>流行偏差</strong>问题作斗争</font>。在这项工作中，我们提出了一种用于 CDR 匹配的新型对比跨域推荐 (CCDR) 框架。具体来说，我们构建了一个巨大的多元化偏好网络来捕获反映用户不同兴趣的多种信息，并设计了<font color="red">一个域内对比学习（intra-CL）</font>和<font color="red">三个域间对比学习（inter-CL）</font>任务以更好地表示学习和知识转移。intra-CL 通过图形增强在目标域内实现更有效和平衡的训练，而 inter-CL 从用户、分类和邻居方面构建不同类型的跨域交互。在实验中，CCDR 在现实世界系统中的离线和在线评估上取得了显着改进。目前，我们已经在微信头条上部署了CCDR，影响了很多用户。源代码在 <a href="https://github.com/lqfarmer/CCDR%E3%80%82">https://github.com/lqfarmer/CCDR。</a></p><h2 id="WSDM2022"><a class="header-anchor" href="#WSDM2022">¶</a>WSDM2022</h2><h3 id="Contrastive-Learning-for-Representation-Degeneration-Problem-in-Sequential-Recommendation"><a class="header-anchor" href="#Contrastive-Learning-for-Representation-Degeneration-Problem-in-Sequential-Recommendation">¶</a>Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation</h3><p>（顺序推荐中表征退化问题的对比学习）</p><p>Abstract：<font color="red">Transformer 和 BERT</font> 等顺序深度学习模型的最新进展极大地促进了顺序推荐。然而，根据我们的研究，这些模型生成的<font color="red">项目嵌入的分布倾向于退化为各向异性形状</font>，<font color="red">这可能导致嵌入之间的语义相似度很高</font>。在本文中，首先提供了这种表示<font color="red">退化问题的实证和理论研究</font>，在此基础上提出了一种新的<font color="red">推荐模型 DuoRec </font>来改善项目嵌入分布。具体来说，鉴于对比学习的均匀性，为 DuoRec 设计了<font color="red">对比正则化以重塑序列表示的分布</font>。鉴于推荐任务是<font color="red">通过点积测量同一空间中序列表示和项目嵌入之间的相似性</font>来执行的约定，正则化可以隐式应用于项目嵌入分布。<font color="red">现有的对比学习方法主要依赖于通过项目裁剪、屏蔽或重新排序对用户-项目交互序列进行数据级增强</font>，并且很难提供语义一致的增强样本。在 DuoRec 中，<font color="red">提出了一种基于 Dropout 的模型级增强，以实现更好的语义保</font><font color="red">留</font>。此外，还开发了一种新的抽样策略，其中具有相同目标项的序列被选为硬阳性样本。对五个数据集进行的大量实验表明，与基线方法相比，所提出的 DuoRec 模型具有优越的性能。学习表示的可视化结果证实 DuoRec 可以在很大程度上缓解表示退化问题。</p><h3 id="Contrastive-Meta-Learning-with-Behavior-Multiplicity-for-Recommendation"><a class="header-anchor" href="#Contrastive-Meta-Learning-with-Behavior-Multiplicity-for-Recommendation">¶</a>Contrastive Meta Learning with Behavior Multiplicity for Recommendation</h3><p>Abstract：一个消息灵通的推荐框架不仅可以<font color="red">帮助用户识别他们感兴趣的项目</font>，而且有利于各种在线平台（例如电子商务、社交媒体）的收入。传统的推荐模型<font color="red">通常假设用户和物品之间只存在单一类型的交互</font>，而<font color="red">无法</font>从多类型的用户行为数据（例如页面浏览、添加到收藏夹和购买）中<font color="red">对多重用户-物品关系进行建模</font>。虽然最近的一些研究建议捕获不同类型行为之间的依赖关系，但很少探索两个重要的挑战：i) <font color="red">处理目标行为（例如，购买）下的稀疏监督信号</font>。ii) 通过定制的依赖建模来捕捉个性化的多行为模式。为了应对上述挑战，我们设计了一个新的模型 CML，<font color="red">对比元学习 (CML)</font>，为不同用户维护专用的跨类型行为依赖性。特别是，我们<font color="red">提出了一个多行为对比学习框架</font>，<font color="red">通过构建的对比损失来提炼跨不同类型行为的可迁移知识</font>。此外，为了捕捉多样化的多行为模式，我们设计了一个对比元网络来为不同用户编码定制的行为异质性。对三个真实世界数据集的广泛实验表明，我们的方法始终优于各种最先进的推荐方法。我们的实证研究进一步表明，对比元学习范式为捕捉推荐中的行为多样性提供了巨大的潜力。我们在以下位置发布了我们的模型实现：<a href="https://github.com/weiwei1206/CML.git%E3%80%82">https://github.com/weiwei1206/CML.git。</a></p><h3 id="Bringing-Your-Own-View-Graph-Contrastive-Learning-without-Prefabricated-Data-Augmentations"><a class="header-anchor" href="#Bringing-Your-Own-View-Graph-Contrastive-Learning-without-Prefabricated-Data-Augmentations">¶</a>Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations</h3><p>（提出自己的观点：无需预制数据增强的图形对比学习）</p><p>Abstract：自我监督最近在其图形学习的新领域蓬勃发展。它促进了对下游任务有益的图形表示；但它的成功可能<font color="red">取决于手工艺领域的知识或通常代价高昂的反复试验</font>。即使是<font color="red">它最先进的代表，图对比学习 (GraphCL)</font>，也不能完全摆脱这些需求，因为 GraphCL 使用预制的先验，这反映在图数据增强的临时手动选择上。我们的工作旨在通过回答以下问题来<font color="red">推进 GraphCL</font>：如何表示图增强视图的空间？可以依靠什么原则来学习该空间的先验知识？可以构建什么框架来学习先验与对比学习？因此，我们扩展了增强集中的预制离散先验，到图形生成器参数空间中的可学习连续先验，假设图形先验本身，类似于图像流形的概念，可以通过数据生成来学习。此外，为了形成对比视图而不因先验可学习性而崩溃为琐碎的解决方案，我们利用了信息最小化 (InfoMin) 和信息瓶颈 (InfoBN) 的原则来规范学习的先验。最终，对比学习、InfoMin和InfoBN有机地融入到一个双层优化框架中。我们的原则性和自动化方法已被证明在小图基准上与最先进的图自监督方法（包括 GraphCL）相比具有竞争力；并在大规模图表上表现出更好的普适性，无需借助人类专业知识或下游验证。我们的代码公开发布在 <a href="https://github.com/Shen-Lab/GraphCL_Automated%E3%80%82">https://github.com/Shen-Lab/GraphCL_Automated。</a></p><h3 id="C²-CRS-Coarse-to-Fine-Contrastive-Learning-for-Conversational-Recommender-System"><a class="header-anchor" href="#C²-CRS-Coarse-to-Fine-Contrastive-Learning-for-Conversational-Recommender-System">¶</a>C²-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System</h3><p>Abstract：会话推荐系统（CRS）旨在通过自然语言对话向用户推荐合适的项目。为了开发有效的 CRS，一个主要的技术问题是<font color="red">如何从非常有限的对话上下文中准确推断用户偏好</font>。为了解决这个问题，一个有前途的解决方案是<font color="red">合并外部数据以丰富上下文信息</font>。然而，之前的研究主要集中在为某些特定类型的外部数据设计融合模型，这对于建模和利用多类型外部数据并不通用。<font color="red">为了有效地利用多类型外部数据</font>，我们提出了一种新颖的<font color="red">从粗到精的对比学习框架</font>，以改进 CRS 的数据语义融合。在我们的方法中，我们首先从不同的数据信号中提取和表示多粒度语义单元，然后以由粗到细的方式对齐关联的多类型语义单元。为了实现这个框架，我们设计了用于建模用户偏好的粗粒度和细粒度过程，其中前者侧重于更一般、粗粒度的语义融合，后者侧重于更具体、细粒度的语义融合。这种方法可以扩展到合并更多种类的外部数据。对两个公共 CRS 数据集的大量实验证明了我们的方法在推荐和对话任务中的有效性。细粒度语义融合。这种方法可以扩展到合并更多种类的外部数据。</p><p><font color="orange">对比学习、图学习、自监督学习热度比较高</font></p>]]></content>
    
    
    <summary type="html">多阅读、多学习</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="科研实习" scheme="https://yangyy.top/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/"/>
    
    <category term="推荐系统" scheme="https://yangyy.top/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="论文阅读" scheme="https://yangyy.top/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>微服务相关知识学习</title>
    <link href="https://yangyy.top/posts/1289748485.html"/>
    <id>https://yangyy.top/posts/1289748485.html</id>
    <published>2023-02-08T10:51:53.000Z</published>
    <updated>2023-02-20T08:15:14.252Z</updated>
    
    <content type="html"><![CDATA[<p>由于校内课题组分配任务的需要，学习一下微服务架构、调用链数据等知识。</p><h4 id="为什么需要分布式追踪系统？"><a class="header-anchor" href="#为什么需要分布式追踪系统？">¶</a>为什么需要分布式追踪系统？</h4><p>为了应对各种复杂的业务，开发工程师开始采用敏捷开发、持续集成等开发方式。系统架构也从<strong>单机大型软件演化成微服务架构</strong>。微服务构建在不同的软件集上，这些软件模块可能是由不同团队开发的，可能使用不同的编程语言来实现，还可能发布在多台服务器上。因此，如果一个服务出现问题，可能导致几十个应用都出现服务异常。</p><p><strong>分布式追踪系统</strong>可以记录请求范围内的信息，例如一次远程方法调用的执行过程和耗时，<strong>是我们排查系统问题和系统性能的重要工具</strong>。</p><h4 id="什么是调用链（Trace）？"><a class="header-anchor" href="#什么是调用链（Trace）？">¶</a>什么是调用链（Trace）？</h4><p>在广义上，一个调用链代表一个事务或者流程在（分布式）系统中的执行过程。在OpenTracing标准中，调用链是多个Span组成的一个有向无环图（Directed Acyclic Graph，简称DAG），每一个Span代表调用链中被命名并计时的连续性执行片段。</p><p>下图是一个分布式调用的例子：客户端发起请求，请求首先到达负载均衡器，接着经过认证服务、计费服务，然后请求资源，最后返回结果。</p><img src="/posts/1289748485/i1.png" alt="分布式调用实例" style="zoom:50%;"><img src="/posts/1289748485/i2.png" alt="时间轴链路图" style="zoom:80%;"><h4 id="OpenTracing数据模型"><a class="header-anchor" href="#OpenTracing数据模型">¶</a>OpenTracing数据模型</h4>]]></content>
    
    
    <summary type="html">学习一下微服务架构、调用链数据等知识。</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="异常检测" scheme="https://yangyy.top/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
    <category term="微服务" scheme="https://yangyy.top/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>回顾在RecBole复现模型的这段经历</title>
    <link href="https://yangyy.top/posts/1979743824.html"/>
    <id>https://yangyy.top/posts/1979743824.html</id>
    <published>2023-02-05T14:50:13.000Z</published>
    <updated>2023-02-05T15:42:50.985Z</updated>
    
    <content type="html"><![CDATA[<p>在RecBole当中复现KD_DAGFM模型是科研实习的第一份正式工作。想来也算是比较的曲折。</p><p>在工作的初期我对于PyTorch、RecBole、推荐系统可谓是一无所知。</p><p>最开始老师问我，会PyTorch吗？我说不会，但是我可以学。于是老师让我学完告诉他。</p><p>记得那天是周五，我就花了一个周末的时间在b站恶补了PyTorch，然后告诉了老师。</p><p>接着老师让我了解RecBole，熟悉的差不多了可以在这上面复现一篇模型。（其实老师的意思应该是我先了解、熟悉RecBole，好了再联系他，然后开始复现模型。我理解成我自己熟悉RecBole，然后自己找模型进行复现，检验自己的学习效果。然后当时还纠结这个复现的要求是什么样的）当时对RecBole也是一无所知。那段时间因为考虑到后面要完成结课作业，也是尽量给自己设ddl（老师完全没提ddl）。于是自己就开始看伯乐的官方主页、官方文档以及一些相关的帖子，感觉差不多了找了一个已经实现的DCN模型进行模仿复现。然后应该是花了一段时间，可能一周左右吧。去找老师说复现了模型。老师还蛮惊讶的，问我提pr了吗。后来老师可能意识到我理解错了，于是就给我推了一个师姐，让她给我分配具体的工作。于是和师姐聊了一下之后，给我分配了KD_DAGFM的复现工作。</p><p>后来要赶着把课内几门课的大作业都做一下，所以就暂时搁置了复现工作。大概花了两周的时间完成课内的结课任务，又碰巧需要回将乐一周，所以中间算是耽误了三周时间。回到上海已是1.13，距离过年一周左右的时间。</p><p>就算是除夕前做完，都算花了一个月。况且除夕前我也没有把握做完。当时想的是必须争取除夕前完成任务，毕竟年后还有复习开学考等其他的工作，堆到年后势必非常焦虑、难受。</p><p>虽然说回到上海已经1.13了，但是在将乐我还是挤出时间把KD_DAGFM模型的论文读懂了（这里也感慨学长写的真的非常好，论文读起来深感思路奇妙，而且具体的细节也是属于讲的通俗易懂的）。也就是说这一周的时间就是全力复现了。</p><p>具体细节就不再赘述了，帖子<a href="http://yangyy.top/posts/2907034089.html">在RecBole当中实现DAGFM模型 | Yangyy’s Life</a>记录了我的复现过程和当时的心路历程。</p><p>大体上应该是：先看了论文，弄懂KD_DAGFM模型；看KD_DAGFM模型的源代码，尽量弄懂代码细节；看RecBole开发的handbook，弄清楚要求；开始在RecBole框架下复现。</p><p>总体还是蛮曲折的。也费了很大的力气让模型跑了起来。</p><p>实际上需要考虑以下方面的问题：</p><ol><li>是否会引起整个框架的冲突（大忌）？</li><li>能否在现有框架下顺利运行？</li><li>是否在多个数据集上达到了与论文描述相当的效果？</li><li>能否顺利进行超参数搜索？</li><li>能否通过CI测试？</li><li>代码风格是否符合规范？</li></ol><p>因为添加的文件比较的少，没有动其他地方，所以问题1不太担心。</p><p>当时也是可以在框架下顺利运行的。</p><p>但是效果并不甚理想、超参数搜索也有隐患、CI测试也跑不完。主要是这三方面问题。</p><p>其实想想当时是非常心酸的。因为校内的服务器没有校园网不能用，实习那边不方便给我服务器。我自己也没有可用的计算资源。就是靠自己的CPU跑啊。CPU跑深度学习模型啊。记得当时超参数搜索跑了一晚上都没有进展。</p><p>不过也是我自己消息闭塞，后来和朱子林聊天，他告诉了我AutoDL这个平台，真正让我工作进度推进速度快了不少。GPU一小时0.88，我是完全可用接受的。前后大概花了20元左右吧，完全是可用接受的。</p><p>当时在CI模型测试没完成，大数据集跑不完所以效果未知，超参数搜索存在隐患的情况下（都没和师姐说），师姐让我先提pr放上去，我只好硬着头皮提了pr。所幸当时学长没空给我review。</p><p>有了GPU之后我就可以很高效地炼丹了。修改了CrossNet模型部分。然后整体通过调参达到了论文描述的效果。但是超参数设置和学长大不相同。（因为实际上有一处细节错误），但是能达到论文效果我相对有了不少信信心。</p><p>利用RecBole的官方CI测试我也得知模型可以顺利通过CI测试。</p><p>于是担心就变成了：对玄学的效果相当的担心；对超参数搜索隐患的担心。也就这样先交了。等待学长的review。</p><p>等学长忙完给我review大概过了半个月。所以即使我基本完成了这项工作，我内心的石头也没有彻底放下，内心还是不安。也不敢在自己的博客上发表自己完成任务的感言。但是也算是一个阶段的结束吧，就开始忙其他的了。</p><p>昨天（2/4）我终于和老师沟通上了，也请学长帮忙review我的代码。当时真是抱着忐忑的心等待学长的检查。不过学长真的非常nice，态度非常温柔，也让我放松不少。学长说我写的非常好，也让我更安心些。当时在等待期间想通了超参数搜索的隐患问题（见博客<a href="http://yangyy.top/posts/1310382946.html">对在RecBole当中复现KD_DAGFM模型的超参数搜索环节的思考 | Yangyy’s Life</a>），也就是其实没有问题。</p><p>那么就只剩下玄学的效果的问题了。确实玄学的达成效果没有用。但是学长还是帮我发现了问题。就是embedding_layer没有冻结。不过其实我自己还真不知道应该怎么冻结，学长教了我加上一行代码解决了这个问题。</p><p>最终、最终！把玄学的效果问题也解决了。那么目前来看似乎没什么问题了。果然是这样，学长说没有其他问题了让我提pr！</p><p>在昨天晚上十一点左右时收到GitHub的邮件，看到了让我十分十分开心的"mergerd"字眼，我当时真是过于开心。因为小刘正在和我视频，她录下了我的反应，真是十分有意义的记录。</p><p>也许对于技术大佬来说，这不值一提，但是对我个人而言，真是有莫大的成就感。其实我在做之前，我是毫无把握的，至少对于在春节之前做完毫无把握。</p><p><img src="/posts/1979743824/i1.png" alt="说说"></p><p>这是在1.15心怀焦虑不安地在博客记录了自己的心情。</p><p>一件毫无把握的事，但是却又是我必须要完成的事，我此前很少面对这样的事情。但是我最终通过努力实现了。（我不以努力、勤奋标榜自己，在大部分情况下，我不认为自己是一个勤奋、努力、拼搏的人，我想我还不配，但是对于过去的这段日子，我觉得自己努力了、奋斗了）</p><p>这也告诉我，虽然很多东西是注定无法达到的。但是努力可以让我拿到更多的东西。</p><p>当然任务的顺利完成，我不觉得均源于我自己。我十分感谢自己的幸运。首先老师和师姐没有给我定ddl，甚至师姐还劝我别急，慢慢做；其次我感觉分配给我的模型KD_DAGFM模型是老师实验室的学长的作品，其源码实现基于PyTorch，风格我想也和RecBole接近（毕竟学长也是RecBole的主要开发者），这对我的复现有很大的帮助，如果源码是用TensorFlow实现，自然就没有PyTorch这么顺利；如果是国外作者写的，自然也更难迁移到RecBole上；学长是作者还有巨大的优势便是，当我出现了上述的问题时，学长能较为轻松的帮我纠正；同时学长本人还非常nice，在赶论文期间，我邮件问他问题他都非常耐心地解答了；甚至学长还用了“辛苦了”、“感谢我的贡献”等词汇，我真是非常非常感激。</p><p>伯乐工作已经告一段落啦。我很幸运，我真的非常感谢这份幸运。我也非常感谢帮助我的人。希望自己能继续努力。继续前进！</p>]]></content>
    
    
    <summary type="html">能顺利完成这项工作真是让我非常的开心呀！</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="科研实习" scheme="https://yangyy.top/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/"/>
    
    <category term="RecBole" scheme="https://yangyy.top/tags/RecBole/"/>
    
    <category term="推荐算法" scheme="https://yangyy.top/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>对在RecBole当中复现KD_DAGFM模型的超参数搜索环节的思考</title>
    <link href="https://yangyy.top/posts/1310382946.html"/>
    <id>https://yangyy.top/posts/1310382946.html</id>
    <published>2023-02-04T10:53:54.000Z</published>
    <updated>2023-02-05T15:24:57.131Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a class="header-anchor" href="#前言">¶</a>前言</h4><p>这篇文章主要是记录我对题述知识点的理解。但是我写博客习惯于讲清来龙去脉，就算是知识点，我也想顺便记录下产生知识点的背景。</p><p>科研实习的第一个任务就是在RecBole当中复现论文《Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation》当中提出的KD_DAGFM模型。大抵在1.20日我完成了模型的复现，提交给学长review。其实当时有预训练过程的模型进行超参数搜索这一块是有隐患的（正文详细描述），但是也只是隐患。我参考了相似模型的实现，发现似乎他们也都没有解决这个隐患。所以我想先给学长review，如果会有问题再说。所以我实际上1.20做完了这项工作，但是我一直非常担心自己的复现有很大的问题，所以一直没敢有任务完成松一口气的动作。在2.4学长帮我review code之后，在review的过程中，我自己再次思考了题述的问题，应该是达到了正确的理解，所以解决了这一隐患。</p><p>这篇博客主要就是阐述这个理解。</p><h4 id="正文"><a class="header-anchor" href="#正文">¶</a>正文</h4><h5 id="预训练："><a class="header-anchor" href="#预训练：">¶</a>预训练：</h5><p>我们都知道，深度学习的许多模型都有<strong>预训练</strong>这一过程以达到更好的实验效果。在预训练之后，我们通常会在预训练的基础上训练模型，使模型表现得更好。因此有预训练的模型在训练阶段，通常需要设置参数pretrain_path这一参数，代表存储预训练得到的模型参数的文件路径。</p><p>KD_DAGFM模型中同样有类似预训练的过程。该模型的训练分为三个阶段：teacher_training、distillation、finetuning。首先teacher_training阶段训练得到教师模型的参数设置；distillation阶段，加载训练好的教师模型参数，通过教师模型进行知识蒸馏，训练学生模型DAGFM；finetuning阶段，加载训练好的学生模型参数，进行微调，达到DAGFM模型最好的效果。</p><p>因此，distillation阶段，我们就需要设置warm_up参数为teacher_training阶段存储模型参数的文件路径；在finetuning阶段，我们就要设置warm_up参数为distillation阶段存储模型参数的文件路径。</p><p>这看起来似乎没什么问题。</p><h5 id="超参数搜索："><a class="header-anchor" href="#超参数搜索：">¶</a>超参数搜索：</h5><p><strong>超参数搜索</strong>同样是AI领域非常常见的概念。batch_size、embedding_size、learning_rate等模型超参数会非常大程度的影响模型的性能。因此，为了充分发挥模型的潜力，我们需要寻找最佳的超参数设置。如果人为的根据模型训练效果逐个修改超参数进行比较选取，比较浪费时间和精力，也就是所谓的炼丹。</p><p>超参数搜索，就是我们提前设定好一系列的超参数及其选择范围，程序会自己去尝试这些超参数的匹配并报告效果。</p><p>这样看似乎也没什么问题。</p><h5 id="有预训练过程模型的超参数搜索："><a class="header-anchor" href="#有预训练过程模型的超参数搜索：">¶</a>有预训练过程模型的超参数搜索：</h5><p>当这两个概念放到一起，似乎就会引发问题了。我们针对KD_DAGFM模型进行讨论。</p><p>首先在teacher_training阶段，进行embedding_size、learning_rate的超参数搜索不会产生问题。比如按照(embedding_size, learning_rate)=(10, 0.001)、(10, 0.005)、(10, 0.0001)、(16, 0.001)…的搭配进行模型训练效果的对比，这样没什么问题。</p><p>再看distillation阶段。前文已述，distillation阶段需要设置warm_up为teacher_training所得模型的存储位置，比如我们设置为./saved/xxx.pth，假设该模型的embedding_size=16。</p><p>接下来我们就要对distillation阶段进行超参数搜索了，要对embedding_size、learning_rate、α、β等进行搜索。<strong>这时候程序就会报错了</strong>。因为超参数搜索程序每一次跑模型使用的warm_up参数都是同一个路径，也就是说在distillation阶段进行embedding_size=8、10、16等尝试时，我们导入的模型却始终都是embedding_size=16，那么显然就会冲突了。</p><p>当时想的解决方案是：</p><p>既然不同的embedding_size要对应不同的warm_up，那么超参数搜索程序就要能根据不同的embedding_size来设置warm_up。这想想都让我望而却步。首先伯乐是一个有80+个模型的项目，我不认为我应该改动它统一的超参数搜索程序。其次，就算能让我自己写，我又要怎么写呢。伯乐存储模型也是统一的程序，根据模型、日期、时间进行存储文件命名。我又如何根据这些信息搜索指定embedding_size的模型文件呢？那我还要改模型存储的部分。牵一发而动全身，当时距离我自己给自己定的ddl也不远了，我实在没有精力和勇气去做这样一件事。</p><p>所幸，teacher_training阶段是模型训练的第一步，也是参数文件默认的训练阶段，所以项目进行CI测试就算会测试超参数搜索，也是进行teacher_training的超参数搜索，所以并不会出现问题。</p><p>我当时也参考了RecBole当中其他有预训练过程的模型复现，他们似乎都没有解决这一问题。</p><p>当时的理解是，可能只需要把预训练/teacher_training阶段的超参数搜索好就行（实际当然不是这样了）。当时只好硬着头皮先交了，想着也许确实不需要解决这个问题。所以虽然算是做完了，当时内心还是十分忐忑的。</p><h5 id="最终的解决方案："><a class="header-anchor" href="#最终的解决方案：">¶</a>最终的解决方案：</h5><p>在昨天（也就是2/4），学长忙完之后帮我review code。我当时想着描述清楚这个问题，然后问学长，在思考怎么把问题描述清楚的时候，我突然想到了这个问题的解决方案（我个人的拙见）。</p><p>首先，teacher_training阶段没有问题，正常超参数搜索就好了。</p><p>主要就是要解决distillation和finetuning的超参数搜索。实际上会起冲突的就是embedding_size。实际上embedding_size不是一个需要重复进行超参数搜索的参数。不管是teacher_training还是pretraining，它们都会预先找到性能最好的embedding_size，然后distillation或者training就会在此基础上进行调整。所以teacher_training找好了最佳embedding_size之后，后续阶段不必再搜索。如果实在要搜索也可以，但是应该不用自动超参数搜索了。</p><p>而对于真正影响知识蒸馏或者训练效果的learning_rate、α、β，我们是单独进行搜索的。也就是确定了embedding_size之后，那么我们学习的最佳教师模型的路径也是确定的（也就是warm_up）不用变。根据固定的教师模型，修改超参数，提高知识蒸馏的效果。finetuning过程同样也是这样。</p><p>所以根据这个思路，并不需要改动项目的超参数搜索程序，也不需要改变模型的存储过程。只需要使用模型的人稍加注意就行。</p><h5 id="结果"><a class="header-anchor" href="#结果">¶</a>结果</h5><p>最后当然也没有问学长这个问题，而review code虽然出现了另一个问题（很顺利解决），也没有再出现别的问题。</p><p>这个知识点还是非常值得记录的，所以有了这篇博客。</p><p>我也深感自己的幸运和误打误撞，我自己都不觉得review会这么顺利。学长还夸我写的非常好。真的是非常开心的。</p><p>接下来应该还会写一篇博客回忆、记录我这段过程的经历和心路历程。</p>]]></content>
    
    
    <summary type="html">时隔多日，突然对有预训练的超参数搜索有了新的认识。</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="科研实习" scheme="https://yangyy.top/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/"/>
    
    <category term="RecBole" scheme="https://yangyy.top/tags/RecBole/"/>
    
    <category term="推荐算法" scheme="https://yangyy.top/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>分享新概念——Yak Shaving</title>
    <link href="https://yangyy.top/posts/2551240651.html"/>
    <id>https://yangyy.top/posts/2551240651.html</id>
    <published>2023-01-31T09:48:55.000Z</published>
    <updated>2023-01-31T10:34:24.348Z</updated>
    
    <content type="html"><![CDATA[<h2 id="注："><a class="header-anchor" href="#注：">¶</a>注：</h2><p>原文链接：</p><p><a href="https://antfu.me/posts/about-yak-shaving-zh">关于 Yak Shaving (antfu.me)</a></p><h2 id="正文："><a class="header-anchor" href="#正文：">¶</a>正文：</h2><p>休息时漫无目的地翻阅其他人的博客，看到了这篇文章，其中提到的Yak Shaving这个概念我觉得非常有趣。</p><p>原文写到：</p><p><a href="https://americanexpress.io/yak-shaving">Yak Shaving</a> 的字面意思是为剪牦牛毛，而引申出来的意思是，当你在进行一个工作时，发现另一个工作还没有完成，你便先去解决那个工作，在进行那个工作时，你又发现另一个工作… 如此往复，让你偏离了原本本该完成的工作，最终却也什么都没有完成。一个实际的例子：</p><blockquote><p>You want to bake an apple pie, so you head to the kitchen.<br>In the hallway, you notice some paint chipping on the wall.<br>So you walk to the hardware store for some paint.<br>On the way, you pass a bakery and stop in for a cupcake.<br>While eating the cupcake, you feel a pain in your mouth. It’s that cavity that you’ve been putting off.<br>You pick up your phone to call the dentist to make an appointment, but you see a notification from your friend Cher, who’s having a party.<br>You don’t want to show up empty-handed, so you stop for a bottle of wine…</p></blockquote><blockquote><p>你想烤一个苹果派，所以你去了厨房。<br>在走廊里，你注意到墙上有一些油漆剥落。<br>于是你走去五金店去买一些油漆。<br>在路上，你经过一家面包店，停下来吃了一个小蛋糕。<br>在吃蛋糕时，你感到嘴里很痛。这是你一直在拖延的蛀牙。<br>当你拿起手机想给预约牙医时，你看到了你朋友 Cher 正在举行一个聚会的通知。<br>你不想空手去参加聚会，所以你停下来买了一瓶酒…</p></blockquote><p>其实Yak Shaving本身应该是一种负面的例子，喻指做事情没有坚定的方向，总是被其他的事情所干扰，最终一事无成。就像上面的例子，“你”没有烤出苹果派、也没有补好家里的漆、“你”没有解决拖延的蛀牙，可能也耽误了朋友的聚会。这么看倒是非常不值得提倡了。</p><p>但是实际上原文的作者是在推崇Yak Shaving的思想。他以自己的亲身经历从另一个角度理解了这个概念。</p><blockquote><p>作者想做一款关于换汇与记账的APP，所以他找了学外语的同学<br>学外语的同学不会用JSON，所以他找到了 <a href="https://github.com/think2011/vscode-vue-i18n"><code>think2011/vscode-vue-i18n</code></a><br>这个项目缺少一些需要的功能，他基于此做出了i18in Ally<br>开发中发现常用的通用函数，他借由 <a href="https://github.com/streamich/react-use"><code>react-use</code></a> 的思想，做出了<a href="https://github.com/vueuse/vueuse">VueUse</a><br>…</p></blockquote><p>作者也是在解决一个目标问题时发现了一个新的问题，从而去解决新的问题的反复过程中。但是不同的地方在于，他发现了新的问题并且把新的问题解决了甚至在解决的基础上做了完善。</p><p>所以本质的差别还是在于是否有在解决问题。两个角度的Yak Shaving实际都是在解决一个问题的过程中不断发现新的问题的过程。对于原本语义中的人，新的问题打断了他解决原有问题的过程，并且他也没有解决新的问题，因此问题的产生是负面的；对于该作者而言，他不断遇到问题，并不断地解决问题、完善解决方案，成就了更好的自己。不断发现问题对于一个执着的问题解决者应该算是一件好事。</p><p>之前在汪定老师在分享科研经历的讲座当中提到，（大意）不应该等到做好所有的准备之后再开始做科研，因为“准备好”这个状态是不太现实的，应该做的是在确定一个方向之后，缺什么补什么，需要学什么再去学什么。我想这二者应该有异曲同工之妙。</p><p>简单来说，坏的Yak Shaving是从1个问题出发，解决过程中遇到了99个问题，但是最终100个问题的解决进度可能都是50%，所以什么都没有解决；好的Yak Shaving是从1个问题出发，解决过程中遇到了99个问题，但是最终100个问题中绝大多数问题的解决进度都是100%或者100+%。</p><p><strong>遇到</strong>问题不一定是坏事，<strong>发现</strong>问题也不一定是好事。问题的出现是好是坏，恐怕最终还是取决于我们是问题的解决者还是问题是我们的解决者。</p><p>解决一个目标问题时遇到了其他的问题，有时候可以忽视突然出现的问题（比如去厨房烤苹果派时，不要在乎油漆的脱落），这样至少我们解决了目标问题；当然如果补完了油漆也吃上了苹果派自然更好；但有的时候必须解决后期出现的问题才能解决目标问题（比如某个工具出现了问题或者需要新增什么功能）；还有时候可能在解决一个又一个的问题时，丢掉了最初的问题（像作者最终没有去开发那款APP），这也不一定是坏事，毕竟后期出现的问题也可能更具价值又或者最初的问题已经被颠覆了。</p><p>Yak Shaving的情况有很多。<strong>所以或许可以说，成功并不源自Yak Shaving，最终还是取决于解决问题的态度、能力，以及对形式和自身的判断和把握。</strong></p>]]></content>
    
    
    <summary type="html">分享一个逛博客时看到的新奇概念——Yak Shaving</summary>
    
    
    
    <category term="来自互联网的收获" scheme="https://yangyy.top/categories/%E6%9D%A5%E8%87%AA%E4%BA%92%E8%81%94%E7%BD%91%E7%9A%84%E6%94%B6%E8%8E%B7/"/>
    
    
    <category term="分享" scheme="https://yangyy.top/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>二刷TSCP2论文</title>
    <link href="https://yangyy.top/posts/4183696190.html"/>
    <id>https://yangyy.top/posts/4183696190.html</id>
    <published>2023-01-27T03:13:13.000Z</published>
    <updated>2023-02-03T08:40:29.276Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a class="header-anchor" href="#前言">¶</a>前言</h2><p>TSCP2模型来自论文《Time Series Change Point Detection with Self-Supervised Contrastive Predictive Coding》，译作《用自监督的对比预测编码进行时间序列变化点检测》。</p><p>其实这篇论文我已经阅读过，并且“复现”过。这篇论文是我接触科研的第一篇论文（已经是四个月之前的事了）。当时对科研工作的了解几乎为零，所以实际上阅读和复现都不甚规范，了解异常检测及其方法的目的倒是达到了。</p><p>四个月后虽然我科研能力并没有提升多少，了解也并没有增加很多，但是好歹比此前进步了。主要也是课题组需要复现一些MTS AD的方法，进行效果的对比。此前由于没有GPU资源，只是简单地跑了一下。这次实实在在地复现一次，记录一下效果。</p><h2 id="正文"><a class="header-anchor" href="#正文">¶</a>正文</h2><h3 id="二刷论文"><a class="header-anchor" href="#二刷论文">¶</a>二刷论文</h3><h4 id="摘要"><a class="header-anchor" href="#摘要">¶</a>摘要</h4><p><strong>CPD</strong>识别与时间序列数据的趋势和属性变化相关的时间以描述系统的基本行为。</p><p>提出一种基于对比预测编码的自监督的时间序列变化点检测方法——<strong>TS-CP2</strong>。</p><p>TS-CP2是第一个在CPD中使用<strong>对比学习策略</strong>的方法，通过将<strong>相邻</strong>的时间区间与<strong>跨时间</strong>区间的嵌入表征的对比实现。</p><p>证明优于五个最先进的CPD方法（包括无监督和半监督）。</p><h4 id="简介"><a class="header-anchor" href="#简介">¶</a>简介</h4><p>大量处理能力和存储容量的提供意味着我们目前拥有前所未有的<font color="red">访问和分析数据的能力</font>。数据存储和共享的规模、速度意味着缺乏数据整理。数据注释昂贵。因此自监督和无监督学习很火热。</p><p>CPD是一种识别突变的时间点的方法。</p><p>变化点可以为系统基本行为提供重要理解，比如变化点代表系统状态的改变和故障或者紧急情况的发生。</p><p>CPD被广泛应用于多变量时间序列数据，以及其他具有时间特征的数据模式。</p><p>变化点通常是从时间序列的不同属性中的一个来估计的，包括时间连续性、分布或形状。<font color="red">无监督的CPD</font>方法通常是为了<font color="red">识别某一个特定属性</font>的变化而开发的。</p><p>目前CPD方法未能有效推广，因为<font color="red">不同应用的语义边界通常与不同的事件序列属性相关</font>。比如心电图的时间形状模式体现心脏节律异常、统计特征的突然变化可以体现人类姿势的变化。用统计学的CPD检测心电图，性能就会下降。</p><p>缓慢变化的时间形状和统计属性也是一个挑战。</p><p>因此提出了TS-CP2。提出问题：<strong>自我监督学习</strong>是否可以为CPD提供一个有效的、通用的表示。直觉是<font color="red">利用时间序列的局部相关性</font>，学习一个表征，使连续时间区间共享信息最大化，使分离的时间区间共享信息最小化。相邻时间区间学习到的<font color="red">表征有明显不同</font>就可能出现变化点。</p><h4 id="相关工作和背景"><a class="header-anchor" href="#相关工作和背景">¶</a>相关工作和背景</h4><h5 id="时间序列变化点检测"><a class="header-anchor" href="#时间序列变化点检测">¶</a>时间序列变化点检测</h5><p>目前CPD的方法<font color="red">大多是基于非深度学习方法</font>。现有的方法可以根据变化点检测考虑的<font color="red">时间序列的特征</font>来分类。</p><p>1、统计学方法通常在确定的时间序列中<font color="red">相邻短区间的统计差异</font>的基础上计算变化点。区间之间的<font color="red">统计差异</font>通常用参数或者非参数的方法测量。</p><p>2、还有一类统计学的CPD方法，将<font color="red">变化点确定为分段边界</font>，通过动态编程、贪婪搜索等方法<font color="red">使得统计学成本函数最优化</font>。</p><p>3、利用时间序列的时间形状模式，通过识别<font color="red">时间序列中与形状模式突出变化</font>相关的未知检测变化点。</p><p>4、<font color="red">混合的CPD方法</font>，利用时间序列的时间形状模式和统计分布。</p><p>5、基于深度学习的CPD方法最近也被提出来了。</p><p>6、CPD在视频处理应用也非常实用。</p><h5 id="表征学习（自监督表征学习）"><a class="header-anchor" href="#表征学习（自监督表征学习）">¶</a>表征学习（自监督表征学习）</h5><h6 id="对比学习"><a class="header-anchor" href="#对比学习">¶</a>对比学习</h6><p>对比学习是一种使用一组由<font color="red">正面样本对</font>和<font color="red">负面样本对</font>组成的训练实例来制定数据集中样本相似或不相似的原因。</p><h6 id="基于对比的表征学习"><a class="header-anchor" href="#基于对比的表征学习">¶</a>基于对比的表征学习</h6><p>前面这些都属于比较宽泛的概括性知识，建立起对CPD领域的大致了解即可。</p><p>主要还是学习TS-CP2模型的具体方法论。</p><hr><h4 id="方法论"><a class="header-anchor" href="#方法论">¶</a>方法论</h4><h5 id="问题定义"><a class="header-anchor" href="#问题定义">¶</a>问题定义</h5><p>给定多变量时间序列**{X1,X2,…,XT}有T个观测值，Xi∈Rd**。试图估计与时间序列属性变化相关的时间t。<font color="red">未来表征和预期表征的异同</font>可以看作过渡到下一个片段的措施。</p><h5 id="TS-CP2概述"><a class="header-anchor" href="#TS-CP2概述">¶</a>TS-CP2概述</h5><p>有一些异常检测方法使用<font color="red">自动回归模型进行预测</font>，在这些方法中，变化点是在与<font color="red">预测误差显著增加</font>相关的样本中检测出来的。</p><p>建议使用表征学习提取紧凑的潜在表征，其对原始数据分布不变。</p><p>论文采用类似CPC的方法来学习<font color="red">使连续时间窗口之间相互信息最大化</font>的表示方法。</p><p>首先，一个自动回归的深度卷积网络<font color="red">WaveNet</font>被用来对<font color="red">每个时间序列窗口进行编码</font>；其次，在编码的基础上采用一个3层的全连接网络产生更紧凑的嵌入表征。连续时间窗口的嵌入表征之间的余弦相似度被计算出来以估计变化点。</p><p><strong><font color="red">对比学习的方法被用来训练编码器</font></strong>，in each batch用<strong>一对</strong>连续的时间窗口和<strong>一组</strong>跨越时间的窗口对。</p><h5 id="表征学习"><a class="header-anchor" href="#表征学习">¶</a>表征学习</h5><p><strong><font color="red">TS-CP2方法的核心就是编码器</font></strong>，将连续的时间窗口转换成紧凑的嵌入表征。通过最大化<font color="red">相邻时间窗口的交互信息</font>训练该表征以学习短时间尺度上的相似性。</p><p>使用WaveNet而不使用LSTM。</p><p>具体的编码器的架构就不复述了，主要就是将时间窗口映射到低维度空间。</p><p>其实<font color="red">最主要的是如何训练编码器</font>，我们输入<font color="red">一对正样本和一组负样本</font>，关键还是设计好成本函数。</p><h6 id="成本函数"><a class="header-anchor" href="#成本函数">¶</a>成本函数</h6><p>采用**<font color="red">InfoNCE loss function</font>**。</p><p>计算每个批次的阳性样本对概率：</p><img src="/posts/4183696190/p1.png" alt="计算ρi" style="zoom:50%;"><p>Sim代表计算余弦相似度。t是缩放参数。</p><p>损失函数：</p><img src="/posts/4183696190/p2.png" alt="损失函数" style="zoom:50%;"><h6 id="负向采样"><a class="header-anchor" href="#负向采样">¶</a>负向采样</h6><p>随机抽出阳性样本对，用于构建每个批次的阴性样本对。</p><p>基本原理如下图所示：</p><img src="/posts/4183696190/p3.png" alt="负向采样" style="zoom:40%;"><p><font color="red">个人理解</font>：</p><p>选出K个相邻的时间窗口对(x1, y1)、(x2, y2)、…、(xk, yk)，但是每一对时间窗口之间的时间间隔要足够长。这样一来:</p><p>x1和y1构成正样本对，(x1, y2)、(x1, y3)、…、(x1, yk)就构成一组k-1个负样本对。</p><p>因此k个连续时间窗口，实际上生成了k组训练实例。</p><p><font color="red"><strong>简单又巧妙！</strong></font></p><h5 id="变化点检测模块"><a class="header-anchor" href="#变化点检测模块">¶</a>变化点检测模块</h5><p><strong>现在已经完成了编码器的构建以及训练的设计，理论上编码器已经能够实现较好的自监督效果了，接下来的问题就是如何检测变化点了。</strong></p><p>将成对的历史和未来窗口转换为嵌入表征，计算余弦相似度；和平均值做差；用寻峰算法寻找差值函数的局部最大值，与之相关的点通常被认为是变化点。</p><h4 id="实验"><a class="header-anchor" href="#实验">¶</a>实验</h4><p>（来到了最重要的实验阶段。一方面实验才是真正的任务需求(作为基线方法对比；另一方面实验能使得我更好地理解论文idea的真正落实)</p><h5 id="数据集"><a class="header-anchor" href="#数据集">¶</a>数据集</h5><p>分析方法在各种应用中的有效性：网络服务流量分析、人类活动识别和移动应用程序使用分析。</p><ul><li>雅虎基准数据集是最广泛引用的异常检测基准之一。（网络服务流量分析）</li><li>HASC挑战赛的数据集。（多个传感器收集的人类活动数据集）</li><li>USC-HAD。也是收集人类活动的数据集。</li></ul><img src="/posts/4183696190/p4.png" alt="数据集属性" style="zoom:50%;"><h5 id="Baseline方法"><a class="header-anchor" href="#Baseline方法">¶</a>Baseline方法</h5><p>提出的TS-CP2方法的性能与五种最先进的**<font color="red">无监督</font>**变化点检测技术进行比较。</p><ul><li>ESPRESSO</li><li>FLOSS</li><li>aHSIC</li><li>RuLSIF</li><li>KL-CPD</li></ul><h5 id="评价指标"><a class="header-anchor" href="#评价指标">¶</a>评价指标</h5><p>模型性能以F1 Score来评价。</p><p>注意！注意！变化点的检测是有误差范围的，而不是直接对比标签和预测结果的。比如t1是变化点，t2不是；预测的结果是t1不是，t2是；不一定是预测错了两次，如果t1和t2间隔在允许的误差范围内，那么我们会认为这次预测是正确的。</p><h5 id="微调和敏感度分析"><a class="header-anchor" href="#微调和敏感度分析">¶</a>微调和敏感度分析</h5><p>主要分析TS-CP2对以下方面的敏感性：</p><ul><li><p>Window size。窗口大小，也就是历史和未来间隔的长度。这个数值不宜大也不宜小，并且通常由数据集决定。</p><p><font color="red">较长的时间窗口拥有最高的F1分数</font>。</p></li><li><p>Batch size。也就是前面提到的数值K。在4~128之间。</p><p><font color="red">通常情况下，批量大小和检测性能之间有一个单调的递增关系。</font></p></li><li><p>Code size。编码长度。就是输出的嵌入表征的维度，在4~20之间。</p><p>最佳代码大小取决于窗口大小和批大小</p></li></ul><h5 id="Baseline-Comparison"><a class="header-anchor" href="#Baseline-Comparison">¶</a>Baseline Comparison</h5><img src="/posts/4183696190/p5.png" alt="Baseline Comparison" style="zoom:50%;"><p><strong>非常非常重要的一个优势在于</strong>：</p><p><font color="red">TS-CP2模型一旦训练出来，CPD的实现就非常简单，只涉及到历史和未来窗口的学习表征之间的比较（余弦相似度的计算），很可能在低资源设备上实现在线操作。基准方法不能在线应用。</font></p><h4 id="复述论文核心思想"><a class="header-anchor" href="#复述论文核心思想">¶</a>复述论文核心思想</h4><p>本文的名字叫用于时间序列变化点检测的自监督对比预测编码，我认为核心就是两个点：**<font color="red">自监督</font>**和<font color="red"><strong>对比</strong></font>。</p><p>实际上时间序列异常检测如果有充足的成本进行异常点的标注（也就是标签的生成），那么用监督学习的方法效果会非常好（之前看的论文当中提到的）。但是由于这个成本太大了，不现实，所以大家被迫探索统计方法和无监督学习的方法。</p><p>本论文说的自监督就是自我生成表征（类似标签的作用），因此需要一个编码器。但是由于缺乏标签，我们没有实际值来验证编码的效果。所以引入了第二个重要的点，对比学习。也就是通过相邻的时间窗口和远隔的时间窗口学习正面和负面的信息（生成样本的负向采样方法也十分有趣）。因此采用了InfoNCE计算损失函数进行学习。学习得到一个良好的编码器之后，可以通过计算相邻时间点的余弦相似度进行变化点的检测。</p><p>最终变化点的验证还会涉及到误差范围的设置。</p><p><strong>以上完成了论文的阅读，接下来开始复现模型效果。</strong></p><h3 id="复现模型效果"><a class="header-anchor" href="#复现模型效果">¶</a>复现模型效果</h3><img src="/posts/4183696190/p6.png" alt="GPU利用率" style="zoom:50%;"><img src="/posts/4183696190/p7.png" alt="显存利用率" style="zoom:50%;"><p>可以发现，在训练的过程中：显存的利用率非常大，但是GPU的利用率却不到10%，非常奇怪。</p><p>看到一种说法是“如果图片的分辨率太大，即使一张图片（batch_size为1）放入显存，也直接爆了，GPU利用率当然也无从谈起，外在表现就是显存几乎占满，但同时GPU等不到数据，一直空闲。解决方法是将图片分辨率改小就OK了。”</p><p>我把batch_size从64调整为16，显存还是撑满，GPU使用率降低了</p><img src="/posts/4183696190/p8.png" alt="GPU使用率" style="zoom:50%;"><p>利用率反而只有一半了。</p><p>再把batch_size从16调到256</p><img src="/posts/4183696190/p10.png" alt="GPU使用率" style="zoom:50%;"><p>GPU使用率大幅度提高。但是预计训练用时没怎么变化（还是需要20小时左右，CPU是621个小时😥）属实离谱。</p><img src="/posts/4183696190/p11.png" alt="GPU使用率" style="zoom:50%;"><p>果然batch_size越大使用率越高，1024的时候已经到了60%左右了，提升了10倍，反正显存都是撑爆。但是总时间还是19小时啊。</p><p>总共时间点的数量就是T，不会反复训练，大抵是epoch = batch /4，然后会随机在一定时候停止一下训练，进行验证。验证之后会存储模型。</p><p>修改了一处代码错误：</p><p>原代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">history = prep_model(x_test[:, <span class="number">0</span>:WIN].reshape((num, <span class="number">1</span>, WIN)))</span><br><span class="line">future = prep_model(x_test[:, WIN:].reshape((num, <span class="number">1</span>, WIN)))</span><br></pre></td></tr></tbody></table></figure><p>但是这样会报错</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"main.py"</span>, line 127, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="built_in">history</span> = prep_model(x_test[:, 0:WIN].reshape((num, 1, WIN)))</span><br><span class="line">  File <span class="string">"/root/miniconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py"</span>, line 67, <span class="keyword">in</span> error_handler</span><br><span class="line">    raise e.with_traceback(filtered_tb) from None</span><br><span class="line">  File <span class="string">"/root/miniconda3/lib/python3.8/site-packages/keras/engine/input_spec.py"</span>, line 264, <span class="keyword">in</span> assert_input_compatibility</span><br><span class="line">    raise ValueError(f<span class="string">'Input {input_index} of layer "{layer_name}" is '</span></span><br><span class="line">ValueError: Input 0 of layer <span class="string">"model"</span> is incompatible with the layer: expected shape=(None, 100, 1), found shape=(3738, 1, 100)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>所以按照提示，我将代码修改为</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">history = prep_model(x_test[:, <span class="number">0</span>:WIN].reshape((num, WIN, <span class="number">1</span>)))</span><br><span class="line">future = prep_model(x_test[:, WIN:].reshape((num, WIN, <span class="number">1</span>)))</span><br></pre></td></tr></tbody></table></figure><p>​这样代码就可以跑通了。</p><p><font color="red"><strong>可是我是原封不动地clone整个项目的呀，为什么会出现这样的问题，原作者没有遇到吗？</strong></font></p><p>但是因为训练非常短暂，所以</p><h4 id="终于知道为什么训练会提前终止了"><a class="header-anchor" href="#终于知道为什么训练会提前终止了">¶</a>终于知道为什么训练会提前终止了</h4><p>因为它设置了提前终止条件</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch &gt; <span class="number">5</span>:</span><br><span class="line">            <span class="keyword">if</span> np.<span class="built_in">abs</span>(epoch_wise_loss[-<span class="number">1</span>] - epoch_wise_loss[-<span class="number">2</span>]) &lt; <span class="number">0.0001</span> <span class="keyword">or</span> epoch_wise_loss[-<span class="number">2</span>] &lt; epoch_wise_loss[-<span class="number">1</span>]:</span><br><span class="line">                end_condition += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                end_condition = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> end_condition == <span class="number">4</span>:</span><br><span class="line">                <span class="keyword">return</span> epoch_wise_loss, epoch_wise_sim, epoch_wise_neg, model</span><br></pre></td></tr></tbody></table></figure><p>就是训练五轮以上时，<font color="red"><strong>连续出现四次以上</strong></font>的loss没有明显变化或者loss反而增大了，那么就会停止训练。</p><img src="/posts/4183696190/p12.png" alt="GPU使用率" style="zoom:60%;"><p>看训练过程中存储的Loss曲线，确实一下子降下来之后就不怎么变化了，所以很早就终止了。</p><p>但是目前的F1 score非常低，只有0.4，不知道为什么。</p><p><font color="red">主要是不能按照论文的batch_size跑，那样要很久（毕竟是自己花钱租服务器），为了提高GPU利用率，我只能提高batch_size，但是这样可能就没法复现效果。</font></p><p>loss确实是很快就降下来了。</p><p>论文中最佳效果对应的batch_size是8，但是这样的话GPU利用率只有3%左右，训练非常慢，不知道效果怎么样。</p><p>但是训练中loss很低，sim很高，neg也比较低，看起来效果是不错的，为什么F1就不好看。</p><h4 id="2-1的收获："><a class="header-anchor" href="#2-1的收获：">¶</a>2/1的收获：</h4><p>今天主要是改了两个bug，模型可以正常运行了</p><p>也发现了batch_size和GPU利用率的规律</p><p>但是没法达到论文中说明的效果。</p><h4 id="2023-2-2-总结一下当前的进度以及遇到的问题"><a class="header-anchor" href="#2023-2-2-总结一下当前的进度以及遇到的问题">¶</a>2023/2/2  总结一下当前的进度以及遇到的问题</h4><p>阅读论文熟悉TS-CP2模型。</p><p>找到官方源代码尝试实现论文中描述的效果。</p><p>clone的官方源代码会报错，进行了修改，目前模型可以正常运行。</p><p>问题：</p><p>现在复现的方法是不是之后论文中对比实验的基线方法？我自己随便选的会不会不合适。</p><p>目前没有达到论文中提到的效果有两方面原因：</p><p>1、利用的数据集是比较小的简化数据集（为了节省实验时间）</p><p>2、论文中最佳实验效果的batch_size大小是8这样的较小数字，而GPU的使用率和batch_size之间有类似单增的关系，batch_size=8是GPU的使用率只有3%左右，训练模型的预计时间很长。如果提高batch_size，GPU的使用率能得到提升，但是训练的效果不太理想。</p><h4 id="学长对问题的回答"><a class="header-anchor" href="#学长对问题的回答">¶</a>学长对问题的回答</h4><p>1、复现的方法不一定需要很高大上，主要是能通过对比实验看出我们的方法的效果是不是更好，可能在其中再挑一些方法作为baseline methods，所以自己找比较top的会议当中提出的论文进行复现就好了，最后就是要能转换到我们自己的数据集上。<font color="red">复现论文最重要的是要能把模型转换到我们自己的数据集上！！！</font></p><p>2、模型效果调不到理想也是十分正常的，基本上都很难调试到论文中描述的效果。</p>]]></content>
    
    
    <summary type="html">经验相对丰富之后，二刷TSCP2。</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="异常检测" scheme="https://yangyy.top/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
    <category term="论文复现" scheme="https://yangyy.top/tags/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>保研准备工作梳理</title>
    <link href="https://yangyy.top/posts/1726264158.html"/>
    <id>https://yangyy.top/posts/1726264158.html</id>
    <published>2023-01-25T02:11:39.000Z</published>
    <updated>2023-01-25T02:48:25.025Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="1292e04df76c35a7beb3f07f60ff41660c65f9f2d53d269a555a72bf9f09fe12">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">这是一篇私密博客哦~</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">不好意思哦，这是一篇加密文章，逛逛博客的其他文章叭。</summary>
    
    
    
    <category term="生活的点点滴滴" scheme="https://yangyy.top/categories/%E7%94%9F%E6%B4%BB%E7%9A%84%E7%82%B9%E7%82%B9%E6%BB%B4%E6%BB%B4/"/>
    
    
    <category term="规划" scheme="https://yangyy.top/tags/%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>假期中后期的一些回顾与规划</title>
    <link href="https://yangyy.top/posts/2118603635.html"/>
    <id>https://yangyy.top/posts/2118603635.html</id>
    <published>2023-01-21T01:46:27.000Z</published>
    <updated>2023-01-23T04:25:44.255Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章当中提到，目前来看又是一个小阶段性的胜利。今天又是除夕，短暂地给自己放个假，让身心都松下来。</p><p>翻翻相册一看，学校提前遣返、我落地上海是12.6的事情了，今天是1.21，不多不少，正好一个半月。不得不感慨最近这两个寒暑假真是太长了，上次暑假提前遣返，在将乐待了三个月（5.30<sub>8.27），这次寒假提前遣返会在上海待（12.6</sub>2.12）两个月多一周。加起来都在家待半年了。</p><p>到家至今已有一个半月，假期还剩下20天左右，进度条只剩下1/3。刚好又是一个阶段性攻坚战的结束。在这个节点回顾一下此前做了什么，此后需要做什么还是非常有必要的。目的如描述当中所说——“避免时光虚度”。</p><p>写到这里，我十分想讨论一下这个目的。</p><p>其实我理应不是在放假都不允许自己虚度时光的人（事实上前三个寒暑假感觉都是虚度）。我对自己的评估是：我是一个非常典型的劳逸结合的人，不能一直玩也不会一直学，顶多学习到十一点就“打卡下班”，很少会肝到凌晨，但是一直玩我也玩不住。感觉自己是很典型的work life balance的人。没有刻苦与努力，就是正常地学习工作。所以寒暑假休息我是十分认同的。在该休息的时候休息，给自己放放假。但是我想休息≠虚度。我自己不希望的虚度是躺在床上刷手机、看各种碎片化信息和刷短视频，其实自己以前能明显感觉到如果刷短视频的时间长了，整个人就会非常浮躁，耐不下心，追求短暂的刺激。偶尔放松时躺在床上刷视频倒还好，但是放假整日除了吃饭睡觉就是躺着刷视频玩手机，那样的场景似乎和吸大烟无异。理想的放松与“虚度”应该是看看闲书、逛逛有趣的博客看看别人的生活、出去走走逛逛、运动一番、出去骑行一下、听听音乐、看看电影…</p><p>不过这样的休息方式似乎算不上虚度？只是相对专心学习而言的放松而已。其实感觉自己挺喜欢这样的生活的，不紧不慢，不管学习还是休息都在获得一些什么。获取知识、获取风景、获取音乐、获取汗水…玩手机虽然是放松，玩久了也蛮累的，刷短视频对我来说实在想不到能获取到什么有益的东西。</p><p>但是这样一看似乎非常的功利，做什么事都希望有回报吗？其实不是的。对我来说以看书等方式放松，其实并不是为了书中所谓的知识，做这些事情不是为了某些目的，而是单纯的做这些事情能让我感到轻松愉快。是做这些事情本身会让我身心愉悦还是因为正确而导致自身感到愉快，这也很难分辨。不过既然感到愉快和放松不就足够了吗？</p><p>因为需要在店里帮忙的缘故，断断续续写了两天，思路都断了。上面的讨论就草草结束吧。</p><p>回顾一下前半段寒假时光我做了什么。</p><p>12.6到12.16这段时间就是完成细碎的作业和寻找外校的科研实习机会。总体来说应该算是蛮充实的，也在12.16联系到了导师。</p><p>12.16到12.22这几天的时间就是按照导师的要求先恶补了PyTorch的相关知识，接着熟悉了RecBole的框架。这几天是十分专注的，为了给导师留下好的印象和尽早开始接触后面的任务，我都是尽量自己push自己。大约是12.23吧，导师让师姐给我分配了具体的任务，在RecBole复现KD_DAGFM模型。</p><p>（但是其实因为这一周时间完全花在科研实习的准备上，所以课内的大宗结课任务完全搁置了。习概的小论文、大论文、软件工程的结课、编译的实验报告和作业、Python的结课大作业、操作系统的结课大作业，当时压力也算是非常大了。一边准备科研实习要学习全新的知识，一边要焦虑担心后面时间来不及，结课任务完成不了。）</p><p>12.23到12.30。由于很多ddl都在12.30，因此12.30是一个阶段的结束。当时把软工、编译、习概都解决了，留下两个大型结课任务：1.9截止的Python、1.13截止的OS，由于都是将考试改成了大作业结课，重要性可见一斑。</p><p>12.30到1.6这一段时间同样非常焦虑，因为想赶快开始KD_DAGFM的实现，但是苦于要认真完成（甚至卷一卷）两门课的结课作业而无法开始科研实习。当时觉得如果1.13才能开始的话，复现模型还可能需要很长一段时间（我当时对这份工作的工作量完全没有概念，因此没有把握多久能完成，甚至不确定自己是不是能完成），担心太久没有给导师反馈而给导师留下不好的印象。但是好在这段时间找到了上海图书馆，环境非常nice，我太喜欢这里了。于是找到了一种规律的生活，早出晚归。进度也比预想推进的更快。印象里1.6提交了OS，1.7大概解决了Python，比预期提早一周结束了任务。（其实一方面是因为Python是团队项目，组员不是很想做的很复杂，所以工作量比较小。不得不感慨软院的摆，Python是3学分，期末的团队项目的开发工作量比我在计算机时2.5学分的Java的个人结课项目小多了，我感觉工作量甚至没到1/5。是的，四个人完成的工作量没有一个人的1/5。但是也不想挣扎了，毕竟我也想早点结束。OS倒是做的比较满意，因为是自己一个人的项目，所以就深入地做了，主要也是因为在图书馆学习时间很多，所以推进的很快。）</p><p>其实这时候直接上手科研实习的任务应该还是比较从容的。但是变故还是出现了。姐姐三年没回将乐了，申请了年假要回家，问我要不要和她一起。和小刘一个月没见了，我也想回去见她。按照前两年的安排，我通常是年后自己一个人回将乐去。但是由于今年情况特殊，寒假要做的事情太多，我年后并不打算回将乐。因此短暂陪姐姐回去几天感觉也好。其实还是纠结了好一阵子的。具体就不多说了。于是1.7陪姐姐回将乐了。回将乐不用多说，学习的时间自然是不可能有多少的。去叔叔家吃个饭，陪爷爷妹妹们玩一天；姨姨请我们出去吃好吃的；和小刘约会；去外婆家吃饭，时间很自然地就溜走了。姐姐是1.11回的上海，我推迟了一些，1.13回上海。当然1.7~1.13这一周，尤其是最后两天，还是挤出时间学习了。所幸和廖静晨、李延枫的关系维护的很简单、自然，我们一起约着去图书馆学习（他们也要准备考研了），就当作是一块儿出去玩了。本来应该出去散散步、喝喝奶茶、聊聊天的，但是实在是挤不出时间了。姐姐逛街时帮我相中一件过年的新衣，我都没空去买。我实在不想拖着科研实习任务了。</p><p>1.12<sub>1.13这两天把KD_DAGFM模型的论文给看了。1.13</sub>1.20这一周的时间又是早出晚归去图书馆，全力开展科研实习的任务。其实我真的真的非常没有底的，因为做这个任务没有人可以问，基本只能靠自己。我并不觉得自己实力有什么能让我有把握的底气。所以还是很焦虑，一边努力学，一边担心做不出来、做不好，不能在年前完成。因为年后规划的任务同样十分繁重，实在不想留到年后了。所幸！所幸！在1.20大抵是做出来了，验证模型运行效果达到了论文中描述的效果，CI测试也通过了，就等学长的review了（当然不能保证没有bug了，有也只能到时候再说了）。由于拖了一个月，我的内心十分煎熬，所以在1.21（除夕）的上午还是迫不及待地打扰老师汇报了进度完成情况。老师说年后沟通。总算又是一个阶段的告一段落 了。</p><p>这一个半月、六周左右的时间，我想我过的还是十分满意的。目前出了两门课程成绩，软工97、数据科学95，还是非常不错的。科研实习感觉也做的尚可。也就是回将乐有五六天的休息时间吧，我想是可以接受的。寒假前半段可以给自己打个√了。</p><p>在这里感慨一下自己真的十分幸运，这期间其实有很多我自己预设的里程碑，我都幸运地准时或者提前达到了。如果12.30论文写不完、如果OS不能如期结束、如果科研实习年前都没有好的效果，那么我想我会更加痛苦。非常非常感谢！</p><p>目前因为春节的缘故，店铺非常忙（爸妈赚钱太辛苦了），也没有什么过年的概念，都在店里帮忙（就当休息了），最近两天都是三点多才睡觉的。只是有些没有学习的罪恶感，希望能早点回归学习状态。</p><p>1.22~2.12，也就是开学前的这段日子了。规划是要复习两门课的笔试（计算机网络、编译原理），要恶补美赛（现在不知道要不要打了）、还要同时做两份科研工作（一份校内、一份校外实习）、最好能准备一下CSP考试。相比还是十分艰难的。只能尽快回到图书馆的生活状态。打算是上午、下午两个时间段，一半复习、一半做科研。晚上回家的时间准备美赛和CSP。我感觉美赛可能大概率会鸽，目前看起来没什么作用，这样任务会轻松一点。</p><p>当然，这几天我还要在店里帮忙，悲！</p>]]></content>
    
    
    <summary type="html">寒假已经过去一大半了，回望过去、展望未来避免时光虚度。</summary>
    
    
    
    <category term="漫无目的地思考" scheme="https://yangyy.top/categories/%E6%BC%AB%E6%97%A0%E7%9B%AE%E7%9A%84%E5%9C%B0%E6%80%9D%E8%80%83/"/>
    
    
    <category term="自言自语" scheme="https://yangyy.top/tags/%E8%87%AA%E8%A8%80%E8%87%AA%E8%AF%AD/"/>
    
    <category term="反思" scheme="https://yangyy.top/tags/%E5%8F%8D%E6%80%9D/"/>
    
  </entry>
  
  <entry>
    <title>在RecBole当中实现DAGFM模型</title>
    <link href="https://yangyy.top/posts/2907034089.html"/>
    <id>https://yangyy.top/posts/2907034089.html</id>
    <published>2023-01-13T02:46:10.000Z</published>
    <updated>2023-01-21T01:44:19.439Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言-v4"><a class="header-anchor" href="#前言-v4">¶</a>前言</h4><p>如愿地、顺利地完成了上一阶段的任务（课内的诸多结课作业），终于可以全力做RecBole的科研任务了。</p><p>老师和师姐没有给我定ddl，说“大三事情也比较多，看自己安排就好”，但是尽快完成也能给老师留下好的印象，同时也可以早一点推进到后续的科研工作当中。</p><p>暂时给自己定下春节前完成模型开发的ddl，时间刚好一周左右。春节之后就要开始做校内的科研任务和准备美赛以及复习开学考了，到时候四线交织想必会是非常痛苦的一段日子（让我想到了上个暑假末尾同时学车+复习五门课程的艰难）。如果把DAGFM模型的开发留到那个时候，恐怕会非常焦虑和痛苦。所以一定要争取在年前把这个阶段性的任务完成，一个阶段做一个阶段的事情。</p><p>实现难度如何还是未知。看辅助开发人员列表，里面有三个月开发一个模型的，也有一个月开发三个模型的，恐怕还是看个人能力罢。我会用多久的时间呢，没有把握。按照这样算也就是十天左右。不过今天就回上海了，后续一个星期打算泡在上海图书馆，应该学习时间会蛮多的，其他事情我也不打算做。希望能快速攻克吧。</p><p>在1.13、文章的开头立下了flag，希望1.21时回来再看已经达到目标了吧。</p><p>前两天把DAGFM模型的论文读了一遍，应该算是看懂了。现在的规划就是先把作者用PyTorch实现的代码看懂，然后尝试用RecBole实现，再根据代码风格、注释规范等等进行修改，最后联系学姐尝试提交。</p><p>本帖应该会是本站当前战线最长的帖子，计划是把知识相关和自己的感受都记录下来，可能会比较冗杂。</p><p>那么，开始吧。</p><h4 id="源码阅读"><a class="header-anchor" href="#源码阅读">¶</a>源码阅读</h4><h5 id="先看最基础的DAGFM实现"><a class="header-anchor" href="#先看最基础的DAGFM实现">¶</a>先看最基础的DAGFM实现</h5><p><font color="orange">1/14 16:08 现在的情况是已经弄明白DAGFM模型和KD_DAGFM模型的源码实现了，接下来尝试在RecBole当中开发这个模型吧。感觉主要还是数据接口和配置什么的会比较麻烦。</font></p><p>现在非常痛苦的一点就是本机算力十分不足。</p><h4 id="阅读开发RecBole的handbook《伯乐开发手册》"><a class="header-anchor" href="#阅读开发RecBole的handbook《伯乐开发手册》">¶</a>阅读开发RecBole的handbook《伯乐开发手册》</h4><h5 id="开发环境的搭建："><a class="header-anchor" href="#开发环境的搭建：">¶</a>开发环境的搭建：</h5><p>依托Linux系统、使用Python开发、根据requirements.txt进行依赖库的安装，建议用conda虚拟环境。</p><h5 id="代码提交管理："><a class="header-anchor" href="#代码提交管理：">¶</a>代码提交管理：</h5><p>基于GitHub平台</p><h5 id="代码开发规范："><a class="header-anchor" href="#代码开发规范：">¶</a>代码开发规范：</h5><p>PEP8</p><p>（实际上后期好像有直接根据PEP8修整）</p><h5 id="开发流程："><a class="header-anchor" href="#开发流程：">¶</a>开发流程：</h5><h6 id="代码准备："><a class="header-anchor" href="#代码准备：">¶</a>代码准备：</h6><p>fork主仓库到自己的仓库，在自己的仓库上进行开发。</p><h6 id="代码文件header注释："><a class="header-anchor" href="#代码文件header注释：">¶</a>代码文件header注释：</h6><p><font color="red">到时候再看就行</font></p><h6 id="撰写注释："><a class="header-anchor" href="#撰写注释：">¶</a>撰写注释：</h6><p>一定要用英文注释</p><h6 id="代码检查："><a class="header-anchor" href="#代码检查：">¶</a>代码检查：</h6><ol><li><p>代码风格检查：</p><p>conda install yapf工具自动按照PEP8改变代码风格</p></li><li><p>代码正确性检查：</p><p>本地简单测试bash run_test.sh</p></li></ol><h6 id="代码提交："><a class="header-anchor" href="#代码提交：">¶</a>代码提交：</h6><p><font color="red">到时候再看</font></p><h5 id="模型开发："><a class="header-anchor" href="#模型开发：">¶</a>模型开发：</h5><h6 id="模型信息登记："><a class="header-anchor" href="#模型信息登记：">¶</a>模型信息登记：</h6><p><font color="red">等快完成了再登记吧</font></p><h6 id="模型开发：-v2"><a class="header-anchor" href="#模型开发：-v2">¶</a>模型开发：</h6><p>a.模型文件创建：</p><p>模型开发一般创建两个文件：</p><p>。。。。。。。。。手动分割线</p><p>1/14  17:52</p><p>感觉handbook看的差不多了，现在打算上手复现了</p><p>1/15  10:39</p><p>目前复现卡住了一个很难解决的点，其他教师模型的复现当中没有embedding_layer，但是知识蒸馏要求共享embedding_layer。</p><p>1/15  10:51</p><p>似乎有出路，虽然教师模型都没有定义，但是所有教师模型的父类ContextRecommander里面有统一的embedding方法，这应该是出路。</p><p>1/15  11:48</p><p>不知道是机遇还是问题的点出现了。本论文的教师模型选择的是xDeepFM和DCNV2，但是实际上教师模型都不是完整的模型，而是其中的部分模型CrossNet和CIN，这两个模型都是简单的特征交互模型，因此比较简单，我觉得可以直接在我自己的模型文件当中实现，这样的话也解决了embedding_layer的问题。</p><p>1/15  12:39</p><p>embedding_layer的问题还是没有解决，因为参考源码里面的embedding是自己实现的，但是到底需要不需要自己实现这么麻烦呢？能不能直接用nn.Embedding？目前还是未知的。</p><p>1/15  14:48</p><p>目前随着引入的模型增加，整体代码量增大，又没有办法实时debug，目前有些混乱，估计最后形成框架跑起来debug会很痛苦</p><p>1/15  14:57</p><p>总体框架完成了，现在感觉最大的问题是数据集接口。归根结底还是对伯乐框架不够熟悉。</p><p>1/15  15:38</p><p>感觉脑子里一团浆糊，但是似乎也无从改善</p><p>感觉眼前的主要问题就是数据接口和embedding_layer</p><p>这两个解决应该会差不多</p><p>1/15  18:00</p><p>论文源代码的运行似乎还是有问题，在GitHub上提交了issue不知道什么时候会回复。</p><p>其实train.py也是要认真看的，因为在源代码当中是模型和训练分离了，但是在recbole当中复现模型必须考虑train</p><p>1/15  18:35</p><p>对interaction的结构似乎更有了解了。</p><p>interaction实际上就是一次取了batch_size个项目进来。</p><p>而这个batch_size是可以作为超参数设置的。</p><p>现阶段主要需要攻克的目标也很明确了，就是把DAGFM模型源码当中一次一次的特征交互改成一批一批的特征交互。</p><p>1/16  10:22</p><p>现在有了些许进展，但是似乎还差点。</p><p>1/16  13:42</p><p>似乎有了不错的进展，但是目前遇到的问题是config不是简单的字典，config[“teacher”]这样可以取到对应的键值，但是没有办法进行遍历，修改教师的配置参数"t_"</p><p>1/16  19:00</p><p>前面的问题得到了解决，现在的问题是，为什么ml-100k数据集的特征只有7个而不是8个</p><p>1/16  19:48</p><p>遇到了比较严重的问题，之前写代码都是用python库里面的代码作为参考，但是似乎python库里面的代码和GitHub有出入</p><p>1/16  21:53</p><p>现在似乎整体稍微能跑起来，但是还是tensor的维度处理的不是很好。</p><p>现在需要处理的就是维度的问题，传入的是有batch_size的三维的数据，原本处理的是二维数据。</p><p>1/16  22:55</p><p>模型跑起来啦，教师模型的训练估计没什么太大的问题，后续就是完成学生模型的训练和细节的完善。然后就可以进行测试和交付啦！</p><p>阶段性的胜利，更新到博客上。</p><p>1/17  10:50</p><p>没有想象当中的顺利。教师模型CrossNet复现出来了，但是CIN和学生模型DAGFM都还有bug。整合之后还有save_info的问题。</p><p>1/17  11:08</p><p>当前就是三大问题</p><ul><li>CIN教师模型的bug，关于维度</li><li>DAGFM学生模型的bug，同样关于维度</li><li>加载教师模型的bug</li></ul><p>1/17  11:14</p><p>目前来看，CIN教师模型的bug被解决了</p><p>这意味着教师模型的完全胜利</p><p>接下来就主要是知识蒸馏的问题了</p><p>1/17  11:40</p><p>有的时候勇于发问真的能节省不少的时间，问到了所有的feature都是B * F * E大小的，瞬间解决了所有的维度问题，两个教师模型和一个学生模型都已经准备就绪了</p><p>现在唯一的问题就是如何进行训练&amp;知识蒸馏</p><p>我感觉年前应该可以做出来</p><p>1/17  12:10</p><p>现在已经把加载模型的问题给解决了</p><p>现在就是似乎知识蒸馏的代码那里还有一点问题</p><p>我想这两天也许能做出来</p><p>1/17  12:30</p><p>模型完完全全可以跑了，可以进行知识蒸馏了！</p><p>但是就是效果似乎没有理想的那么好。</p><p>我感觉这两天应该可以做出来。</p><p>知识蒸馏的效果似乎还没有普通的DAGFM强。</p><p>auc=0.51，真的就和随便猜差不多</p><p>知识蒸馏训练的AUC反而越来越低了，真服了。</p><p>现在就是对照着检查一下代码吧，看看是不是哪里出问题了</p><p>上午先到这里吧，下午开始查代码，争取今天把实验效果搞好，明天提交测试。</p><p>炼丹的事情就很难说了，呜呜呜。</p><p>1/17  15:56</p><p>检查了一遍完整代码，似乎没什么问题</p><p>1/17  17:11</p><p>目前模型没什么问题，效果也上来了，就是最后的超参数调整和模型测试了，估计明天可以提交</p><p>-------------------------------------手动分割线</p><p>1/20  16:39</p><p>不知不觉间有接近三天没有更新了。</p><p>项目的进展有了非常多的变化。唉，有很多想说的想写的，但是想法一齐喷涌上来，结果就堵住了，一下子不知道如何“落笔”。</p><p>为什么在1/17就断更了呢，因为在那之后就满心都是这个项目了，实在没有分出的心思记录。</p><p>我实在想写下很多东西，但因为思绪纷乱和最近几天的焦虑、压力和疲劳导致我的脑子运转不畅，就当是碎碎念吧，没有逻辑也不去梳理思路，想到什么就写下什么。</p><p>说说眼下的进展吧，目前看起来是做完了，KD_DAGFM模型在RecBole框架下能正常使用，不会产生冲突和影响框架的其他部分，也通过参数的调整在数据集上达到了论文当中描述的效果，基本的测试也没有问题。现在已经提交上去给学长review了。</p><p>但是我并不确定会不会有其他的错误，毕竟谁能保证自己写的代码不会出错呢，希望可用顺利结束开始下一阶段的任务吧，希望一切顺利。但是师兄师姐也需要放假嘛，所以不可能马上有检查的反馈。</p><p>再回顾回顾这几天为项目奋斗的日子。不能说难过，但是也不能说很开心。但是确实很累。</p><p>大概也就是回上海一周左右的时间吧。每天其实过的都一样，早上七点起床，洗漱收拾一下（不得不说我在这一环通常都需要花上三十分钟，相比大部分男生似乎都更“磨蹭”），出门买好早饭，就去健身房健身。一般换好衣服准备开始锻炼时都已经八点了。刚好一周的时间，肩、背、跑步、胸、腿、椭圆机、休息日轮了一遍。（身体适应了这个节奏，今天早晨在七点就自然醒了，不过因为是休息日，就多睡了一个小时。）健身之后坐地铁到图书馆，先吃买的早饭（包子、鸡蛋），那时基本已经冷了，不过我倒是不挑剔。吃完饭在二楼找个位子坐下开始学习。反正就搞项目呗。搞到一点左右点外卖，两点吃饭，两点半左右吃好。开头两天还会找个沙发睡个午觉，后来因为那样也没有睡得很好，再加上想早日把项目做完，因此就不睡午觉了。回到座位继续学习。当然学累了或者炼丹的时候会玩玩手机。学到快晚上八点，就收拾书包坐地铁回店里吃饭了。吃完饭和家里人聊一会天就回家。回到家基本已经很累了，如果是没有午睡的日子，晚上脑子里更是一坨浆糊，可能会做点简单的工作或者刷手机，然后晚上十一点半十二点睡觉。如此循环。其实这样的日子并不困难，但是多少有些令人疲倦。其实我想想，我在学校的时候都没有这样认真学习与工作。读大学已是第三年，寒暑假也已经是第五个了，这一次的假期是最充实的，也由此想到我前几个寒暑假的摆烂与虚度，完全没有好好利用，这也导致了前面被人拉开差距。不过生活总是向前的，倒也不觉得遗憾。</p><p>怎么发现自己似乎疲惫了呢？当然每天晚上回去的时候的疲惫是不必说的。主要最近食欲都下降好多，吃什么都不像以前那么香了。昨天和姐姐去逛街买衣服（实在没衣服穿了），照着镜子看着自己杂乱的头发，变小和无神的眼睛，一脸的油，才真正意识到自己累了。忙着连理发的心思都没有，一晃都大年二十九了，想理发也没处去，主要是人多我也不想排队。过完年正月一般又不能理发。只好再议。</p><p>先写到这里吧，准备回家吃饭了。</p><p>---------------------------------------------------------手动分割线</p><p>休息了一晚上再来码字头脑都清醒了不少。</p><p>项目的进展也回顾完了，最近这几天过的日子也回顾完了。目前还不能说我完全复现了KD_DAGFM模型，所以所谓“获奖感言”以后再说吧，半场开香槟的事情是万万做不得的。</p><p>这篇文章就写到这里吧，另外想说的话已经不符合这个主题了，待会再开一篇博客。</p>]]></content>
    
    
    <summary type="html">一些后续日子的规划&amp;向科研实习发起冲锋！</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="科研实习" scheme="https://yangyy.top/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/"/>
    
    <category term="RecBole" scheme="https://yangyy.top/tags/RecBole/"/>
    
    <category term="推荐算法" scheme="https://yangyy.top/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>通过论文学习DAGFM模型</title>
    <link href="https://yangyy.top/posts/1347074556.html"/>
    <id>https://yangyy.top/posts/1347074556.html</id>
    <published>2023-01-10T00:56:02.000Z</published>
    <updated>2023-01-12T14:03:35.507Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言-v7"><a class="header-anchor" href="#前言-v7">¶</a>前言</h3><p>本次科研实习的任务是利用RecBole框架复现论文《Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation》中的DAGFM模型，因此第一要务是把论文学习明白，把模型熟悉清楚。再开始琢磨如何利用RecBole进行复现。</p><h3 id="正文-v4"><a class="header-anchor" href="#正文-v4">¶</a>正文</h3><p>译作《用于点击率预测的利用知识蒸馏的有向无环图分解机》</p><h4 id="摘要-v4"><a class="header-anchor" href="#摘要-v4">¶</a>摘要</h4><p>网络规模的推荐系统中<strong>高维稀疏数据增长</strong>，在CTR预测任务中<strong>学习高阶特征交互</strong>的计算成本大大增加。</p><p>一些基于<font color="red">知识蒸馏</font>的方法将知识从复杂的教师模型转换成浅层的学生模型，<font color="red">加速模型推理</font>。但是知识蒸馏会导致模型<font color="red">精度下降</font>。如何<font color="red">平衡浅层学生模型的效率和效果</font>是一个挑战。</p><p>为了解决这个问题，我们提出一个有向无环图分解机（DAGFM），通过知识蒸馏从现有的复杂交互模型当中学习高阶特征交互，用于CTR预测。提出的轻量级学生模型DAGFM可以从教师网络当中学习任意明确的特征交互，实现<font color="red">近似无损的性能</font>，通过动态编程算法得到证明。改进的模型DAGFM+可以有效地从任何复杂的教师模型中提炼出显性和隐性特征的相互作用。效果甚佳。</p><h4 id="简介-v3"><a class="header-anchor" href="#简介-v3">¶</a>简介</h4><p>点击率（CTR）预测的目的是预测用户点击某个项目的概率。CTR预测中实现良好性能的<strong>关键</strong>是<font color="red">学习有效的高阶特征交互</font>。最大的挑战之一是<font color="red">原始特征的高阶交互</font>建模的高计算成本：原始特征数量增加之后，特征组合的数量会指数增长。<font color="red">原始特征本身通常都是高度稀疏</font>的（标识符特征被编码为one-hot向量之后很稀疏、从上游任务（视觉信息）建立的多文件向量也是），再在此上计算高阶特征交互非常耗时。</p><p>需要一种轻量级的CTR预测的推荐算法简化在线推理过程，从而避免实际行业中计算成本爆炸。</p><p>目前有一些努力，利用<font color="red">知识蒸馏（KD）技术</font>将知识从复杂的教师模型转移到学习参数减少的浅层的学生模型，加快<font color="red">模型推理速度</font>。但是降低学习参数的KD模型的代价是<font color="red">模型精度下降</font>。</p><p>提出一个轻量级的基于知识蒸馏的DAGFM预测CTR。设计一个具有<font color="red">较少学习参数</font>的影子学生模型，但是<font color="red">保持</font>现有的复杂交互模型<font color="red">提取高阶特征交互的能力</font>。</p><p>具体地说，有向无环图当中每一个<font color="red">节点代表一个特征域</font>，不同节点通过不成环的有向边连接。<font color="red">节点间的连接，代表特征的交互</font>，用DAG中的边的<font color="red">可学习权重</font>（毕竟不同的特征交互作用是不一样的）来模拟。通过特定的交互学习函数（内部、外部、内核），DAG上的特征交互作用可以转化为明确的任意顺序的交互。</p><p>为了从<font color="red">隐性特征交互</font>模型（AutoInt、FibiNet）中提炼知识，进一步改进DAGFM，多层感知机（MLP）被集成进去，因此DAGFM+可以学习任何显性和隐性的特征交互。</p><h4 id="Preliminary（初步的？知识预备？）"><a class="header-anchor" href="#Preliminary（初步的？知识预备？）">¶</a>Preliminary（初步的？知识预备？）</h4><p>首先介绍CTR预测任务，然后介绍其中的显性高阶特征交互和交互学习函数。</p><h5 id="CTR预测"><a class="header-anchor" href="#CTR预测">¶</a>CTR预测</h5><p>点击率预测就是预测用户点击某个项目的概率（给你用户信息和项目信息）。具体说，就是给定m个特征字段，用x={x1,x2,…,xm}表示<font color="red">用户和项目特征</font>。标签y∈{0，1}代表是否点击。根据x进行预测。CTR预测任务中，取得良好性能的<font color="red">关键是学习有效的高阶特征交互</font>。</p><h5 id="高阶特征交互"><a class="header-anchor" href="#高阶特征交互">¶</a>高阶特征交互</h5><p>给定输入特征x={x1,x2,…,xm}，x的嵌入特征表示为{e1,e2,…,em}（向量降维）。</p><p>显示特征交互可以表示为</p><img src="/posts/1347074556/i1.png" alt="显示特征交互公式" style="zoom:50%;"><p>∅代表特征交互函数。</p><img src="/posts/1347074556/i2.png" alt="二阶特征交互公式" style="zoom:50%;"><p>（这两个公式还是比较好理解的）</p><h5 id="交互学习函数"><a class="header-anchor" href="#交互学习函数">¶</a>交互学习函数</h5><p><font color="red">特征交互学习函数</font>定义了计算<font color="red">特征间交互作用的方式</font>。不同的交互模型使用不同的交互学习函数。</p><ol><li><p>基本的内部交互学习函数。</p><p>现有研究中最常用的交互学习函数表述为：</p><img src="/posts/1347074556/i3.png" alt="基本的内部交互学习函数" style="zoom:75%;"><p>代表element-wise product（Hadamard product），含义是两个矩阵对应位置元素进行乘积。其实就是<font color="red">向量点积</font>。</p></li><li><p>加权的内部交互学习函数。</p><p>上述方法<font color="red">不能捕捉到不同场景的特征信息</font>（因为所有特征交互都一视同仁了），导致梯度耦合问题。为了解决这个问题，提出了许多<font color="red">场景感知的变体</font>，使用权重参数来模拟不同场景的特征交互作用。</p><img src="/posts/1347074556/i4.png" alt="加权的内部交互学习函数" style="zoom:60%;"><p>wi,j是i和j的权向量。</p></li><li><p>内核的交互学习函数。</p><p>通过扩展权重向量的维度，最近的研究提出了更为强大的交互学习函数：</p><img src="/posts/1347074556/i5.png" alt="内核的交互学习函数" style="zoom:60%;"><p>Wi,j是学习权重矩阵。</p></li></ol><p>纳入了交互权重之后，有了<font color="red">很大的改进</font>。但是模型的<font color="red">复杂度也大大增加</font>，列举所有不同场景权重的高阶特征交互是NP问题。大多数现有的高阶特征交互模型通过增加参数来提高模型能力，但是<font color="red">代价是极高的计算成本</font>，显然不太符合实际。</p><p>-------------手动分割线，去外婆家吃午饭</p><p>-------------我又回来啦，下午继续干活</p><h4 id="方法论-v2"><a class="header-anchor" href="#方法论-v2">¶</a>方法论</h4><p>为了<font color="red">低计算成本下提高模型性能</font>，提出了一种基于知识蒸馏的有向无环图分解机，其中一个轻量级的学生模型有向无环图分解机设计来从现有的复杂的教师模型当中学习高阶的特征交互。</p><h5 id="有向无环图分解机"><a class="header-anchor" href="#有向无环图分解机">¶</a>有向无环图分解机</h5><p>设计的轻量级学生模型就是有向无环图分解机DAGFM。结构如下图。先介绍其构造和信息交互过程，然后介绍提出的高效外部交互学习函数。</p><img src="/posts/1347074556/i6.png" alt="DAGFM的架构" style="zoom:35%;"><ol><li><p>DAG的构建。</p><p>如架构图所示，给定输入特征x={x1,x2,…,xm}。嵌入层将输入的特征从A到B映射出去（高维稀疏空间到低维稠密空间、ei=xiWi）。</p><p>有向无环图定义为G=(N, E)。其中<font color="red">每个节点</font>ni∈N对应一个<font color="red">特征域</font>，|N|=m。每个节点ni的初始状态设置为ei，即h1i=ei。节点ni到nj（j&gt;i）的<font color="red">边是有指向</font>的，控制着不同特征域的<font color="red">交互权重</font>。</p></li><li><p>在DAG上学习特征的交互。</p><p>特征之间的交互可以以<font color="red">递归的方式</font>建模为DAG上的**<font color="red">信息传播过程</font>**。在每一个传播层，每个节点都会汇总来自<font color="red">邻居的状态</font>与<font color="red">自身的初始状态</font>h1i来更新状态向量。形式上，传播层t的节点ni表示为：</p><img src="/posts/1347074556/i7.png" alt="传播公式" style="zoom:65%;"><p>初始层t=0。其中N (𝑖) = { 𝑗 |𝑛𝑗 → 𝑛𝑖 ∈ E}，也就是<font color="red">j∈i和指向i的所有节点</font>，𝜙 是交互学习函数。</p><p>为了获得用于CTR预测的高阶特征交互信息，首先在每个交互步骤对每个节点的隐藏状态向量应用了sum pooling</p><img src="/posts/1347074556/i8.png" alt="max pooling" style="zoom:70%;"><p>其中ht(i,k)表示hti的第k个元素。<font color="red"><strong>其实就是向量元素全相加</strong></font>（因为d是embedding size）。</p><p>计算完毕之后，把第t层的所有节点的这个值连起来，形成<img src="/posts/1347074556/i9.png" alt="连接" style="zoom:50%;">不同阶的特征交互就表示为p=[p1;p2;…;pl]，传播层的数量为l-1。</p><p>最后用于CTR预测的函数表述为<img src="/posts/1347074556/i10.png" alt="CTR预测函数" style="zoom:50%;">σ是sigmoid激活函数，w是转换向量，b是偏置。</p></li><li><p>外交互学习函数</p><p>DAGFM的特征交互学习函数𝜙很灵活。可以利用最常用的<font color="red">内交互学习函数</font>，也可以用模型能力更强的<font color="red">内核交互学习函数</font>。</p><p>为了<font color="red">降低内核交互</font>学习函数的<font color="red">计算复杂度</font>，提出了一个简化的<font color="red">外部交互学习函数</font>。</p><p>内核交互学习函数中的学习权重矩阵是：</p><img src="/posts/1347074556/i11.png" alt="学习权重矩阵" style="zoom:50%;"><p>p、q都是分解向量。</p><p>（<font color="orange">我个人的理解是比如权重矩阵是m×n的，权重矩阵内部的元素肯定是学习得到了，那么初始化的时候可以用分解向量1×m和1×n矩阵乘得到m×n。实际上矩阵是d×d的</font>）</p><p>那么外部交互函数就可以定义为</p><img src="/posts/1347074556/i12.png" alt="外部交互函数的定义" style="zoom:60%;"><p><font color="red">前者是1×d × d×1，后者是1×d · 1×d，把复杂度从O（n2）降到了O（n）</font>。</p><p>-------------手动分割线，继续学习</p></li></ol><h5 id="DAGFM当中任意阶的交互学习"><a class="header-anchor" href="#DAGFM当中任意阶的交互学习">¶</a>DAGFM当中任意阶的交互学习</h5><p>本节证明了在DAGFM中学习到的特征交互与动态编程算法中不同的唯一加权路径相一致，<font color="red">显示了DAGFM学习任意阶特征交互的能力</font>。</p><p>我们将一个**<font color="red">特征交互的后缀定义为具有最大下标的特征</font>**。比如特征交互：e2e3、e3e4e5，其后缀分别为e3和e5。</p><p>让<font color="red">Sti表示所有以ei为后缀的t阶特征交互</font>。以以e3为后缀的2阶特征交互为例，S23={e1e3,e2e3,e3e3}，以ei为后缀的1阶特征交互e1i={ei}。</p><p>在动态编程算法中，<font color="red">hti就是sti内元素的累加</font>。<strong>事实上，所有以<font color="red">ei</font>为后缀的<font color="red">t+1</font>阶特征交互可以被表示为ei和（以<font color="red">ej（j&lt;=i)</font>为后缀的<font color="red">t阶</font>特征交互的交互）</strong>。</p><p>因此状态转移方程可以改写如下：</p><img src="/posts/1347074556/i13.png" alt="状态转移方程" style="zoom:50%;"><p>（<font color="orange">这个公式应该还是比较好理解的</font>）</p><p>在DAGFM当中，特征的交互是基于有向无环图进行传播的，节点ni的另据N(i)是直连ni的节点nj集合，j&lt;i。因此可以稍加改写上面的公式：</p><img src="/posts/1347074556/i14.png" alt="状态转移方程" style="zoom:50%;"><p>这里的交互学习函数是基本的内交互学习函数。</p><p>从上述两个公式可以看出，DAGFM的<font color="red">每个特征交互都和DP图当中的唯一路径匹配</font>。每个k阶特征交互对应从第一层开始的长度为k-1的唯一路径。对状态向量进行求和，所有路径都可以遍历，这也说明**<font color="red">所有特征交互都可以在DAGFM当中被建模</font>**。</p><p>对于具有内部和内核交互学习函数的图传播过程，通过将权重分配为全1向量和矩阵可以等同。<strong>权重也可以在训练过程中学习</strong>，这使得DAGFM能对<font color="red">不同语义信息的特征交互建模</font>。</p><p>比如特征：男人、工作日、加拿大、晴天，那么男人×工作日×加拿大的特征交互就对应n11-&gt;n22-&gt;n33，边代表权重，控制语义强度。</p><p>DAGFM传播图示例：</p><img src="/posts/1347074556/i15.png" alt="传播图示例" style="zoom:50%;"><h5 id="知识蒸馏的过程"><a class="header-anchor" href="#知识蒸馏的过程">¶</a>知识蒸馏的过程</h5><img src="/posts/1347074556/i16.png" alt="KD-DAGFM整体架构图" style="zoom:50%;"><p>KD-DAGFM的整体架构图如上所示。</p><p>在知识蒸馏的过程中，学生网络DAGFM从教师网络学习特征交互的知识。为了保证学生和教师网络的<font color="red">特征空间相同</font>，将<font color="red">教师网络的嵌入层与学生网络共享</font>。</p><p>知识蒸馏的损失函数：</p><img src="/posts/1347074556/i17.png" alt="知识蒸馏的损失函数" style="zoom:50%;"><p>N是训练实例的总数，T和S是学习函数，𝝍 和𝝋分别是教师和学生网络的参数。（<font color="red">其实也就是衡量教师网络和学生网络的效果差距，利用方差的形式</font>）</p><p>我们采用交叉熵损失函数进行CTR预测：</p><img src="/posts/1347074556/i18.png" alt="交叉熵损失函数" style="zoom:50%;"><p>最终优化目标是：</p><img src="/posts/1347074556/i19.png" alt="最终优化目标" style="zoom:60%;"><p><strong><font color="red">𝛼, 𝛽 是平衡两个损失函数的权重的超参数</font></strong>。</p><p>经过知识蒸馏，学生模型可以很好地从教师网络当中<font color="red">学习到有用的特征交互</font>。<strong><font color="orange">其实也就是同时从现有的教师模型和结果当中学习</font></strong>。</p><p><strong><font color="red">但是但是</font></strong></p><p>学生的特征嵌入继承自教师，教师在蒸馏过程中没有训练，可能导致欠拟合问题，导致学生模型不理想。为了缓解这个问题，他们微调了学生模型（没有说具体方法）。</p><p><font color="red"><strong>以上基本上就把具体的模型和方法全都学完了。</strong></font></p><h4 id="实验"><a class="header-anchor" href="#实验">¶</a>实验</h4><p>接下来就是实验证明KD-DAGFM的效果了。实验当中用了四个真实世界当中的数据集：Criteo、Avazu、MovieLens-1M和微信。</p><p>Criteo是最流行的CTR预测基准数据集，包含7天内的用户日志。Avazu包含7天的用户日志，用于Avazu CTR预测比赛。MoviesLens-1M是推荐系统研究中最受欢迎的数据集。</p><h5 id="学生模型DAGFM的有效性分析"><a class="header-anchor" href="#学生模型DAGFM的有效性分析">¶</a>学生模型DAGFM的有效性分析</h5><p>这一小节，旨在<font color="red">证明学生模型DAGFM能达到复杂教师模型的近似无损性能</font>。</p><p>一方面和教师模型比，是不是性能无损；另一方面和其他学生模型比较是否性能提升。</p><ol><li><p>教师网络。</p><p>使用xDeepFM和DCNV2。</p></li><li><p>学生网络。</p><p>使用常用的轻型模型FwFM、FmFM、tiny MLP作为学生模型进行比较。对于DAGFM，采用了教师和交互学习函数的最佳组合，CIN和DAGFM-inner和DAGFM-minor。</p></li></ol><p>结果显示，和教师模型相比学生模型DAGFM的复杂度大大降低，和其他学生模型相比，DAGFM的复杂度更低，并且损失也更小。</p><h5 id="KD-DAGFM的实验结果"><a class="header-anchor" href="#KD-DAGFM的实验结果">¶</a>KD-DAGFM的实验结果</h5><p>接下来就是利用实验证明KD-DAGFM模型在CTR预测任务当中的有效性了。</p><h6 id="比较方法"><a class="header-anchor" href="#比较方法">¶</a>比较方法</h6><ul><li>FmFM</li><li>FwFM</li><li>xDeepFM</li><li>DCNV2</li><li>FiBiNet</li><li>AutoInt</li><li>FiGNN</li><li>GraphFM</li><li>ECKD</li></ul><h6 id="实施细节-v2"><a class="header-anchor" href="#实施细节-v2">¶</a>实施细节</h6><p>参数设置直接看图就好。</p><img src="/posts/1347074556/i20.png" alt="参数设置" style="zoom:60%;"><h6 id="实验结果-v2"><a class="header-anchor" href="#实验结果-v2">¶</a>实验结果</h6><ol><li>深度方法比浅度方法有效，说明了高阶特征交互的重要性</li><li>显式方法比隐式方法更有效</li><li>基于图的模型很有竞争力</li><li>知识蒸馏也非常有效</li><li>KD-DAGFM比DAGFM更有效</li></ol><p>KD-DAGFM能够有效地学习高阶特征交互的相互作用，较少的参数也更贴近现实应用。</p><h5 id="KD-DAGFM的实验分析"><a class="header-anchor" href="#KD-DAGFM的实验分析">¶</a>KD-DAGFM的实验分析</h5><p>本节分析影响模型性能的因素。</p><h6 id="传播层数的影响"><a class="header-anchor" href="#传播层数的影响">¶</a>传播层数的影响</h6><p>传播层数的影响其实就是特征交互的阶数的影响，传播层数越多，那么特征交互的阶数就越高。</p><p><font color="red">随着层数的增加，模型的性能变得更好，表面学习高阶特征交互信息对于CTR预测的重要性</font>。</p><h6 id="不同交互学习函数的影响"><a class="header-anchor" href="#不同交互学习函数的影响">¶</a>不同交互学习函数的影响</h6><p>利用前文提出的<font color="red">外部交互学习函数的DAGFM可以很好地适用于不同的教师模型</font>。</p><h6 id="通用的蒸馏模型KD-DAGFM"><a class="header-anchor" href="#通用的蒸馏模型KD-DAGFM">¶</a>通用的蒸馏模型KD-DAGFM+</h6><p>考虑到教师模型的复杂性（同时学习显性特征和隐性特征的交互），为了有效地从显性和隐性特征交互中提取知识，提出了KD-DAGFM+，在学生模型DAGFM之后附加一个MLP组件（DAGFM所有节点地状态串联起来输入MLP）。KD-DAGFM+实现了近乎无损的蒸馏性能。</p><h4 id="相关工作"><a class="header-anchor" href="#相关工作">¶</a>相关工作</h4><h5 id="特征交互学习"><a class="header-anchor" href="#特征交互学习">¶</a>特征交互学习</h5><p>学习特征的交互是CTR预测任务当中的基本问题…</p><h5 id="图神经网络"><a class="header-anchor" href="#图神经网络">¶</a>图神经网络</h5><h5 id="知识蒸馏"><a class="header-anchor" href="#知识蒸馏">¶</a>知识蒸馏</h5><p>知识蒸馏（Knowledge Distillation，KD）是一种从复杂模型当中提取知识并将其压缩到一个简单模型中的方法</p><h4 id="总结-v4"><a class="header-anchor" href="#总结-v4">¶</a>总结</h4><p>本论文提出一个轻量级的知识蒸馏模型KD-DAGFM，该模型是一个用于CTR预测的有向无环图神经网络。学习复杂教师网络当中的特征交互、大大降低计算资源成本。效果甚佳…</p><p>这样论文就阅读完了。大致的思路梳理就是：由于特征通常是稀疏的高维度向量，再加上需要学习高阶的特征交互，因此计算量通常比较爆炸。知识蒸馏可以降低模型的复杂度，但是模型精度会下降。因此提出了他们的KD-DAGFM模型。先从CTR任务说起，再说特征交互、交互函数，然后介绍DAGFM的具体工作原理。最后探讨一下性能。DAGFM就是用节点代表特征向量，用边代表特征交互。然后通过多层传播模拟高阶特征交互。</p><p>接下来开始尝试复现模型叭~</p>]]></content>
    
    
    <summary type="html">为了科研实习，冲冲冲！</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="科研实习" scheme="https://yangyy.top/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/"/>
    
    <category term="RecBole" scheme="https://yangyy.top/tags/RecBole/"/>
    
    <category term="推荐算法" scheme="https://yangyy.top/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>近期读书笔记（22/1/5）</title>
    <link href="https://yangyy.top/posts/3620005199.html"/>
    <id>https://yangyy.top/posts/3620005199.html</id>
    <published>2023-01-05T08:44:16.000Z</published>
    <updated>2023-01-12T14:03:35.502Z</updated>
    
    <content type="html"><![CDATA[<p>好久没写读书笔记了，最近在看《海边的卡夫卡》，看到很多值得记录的句子，书是学校借来的，没法做批注什么的，看到喜欢的句子也只能把那一页折起来，喜欢的句子多了就急需整理。此刻，写了一天OS的期末报告有点累了，就整理整理当作放松吧。</p><p>以下摘抄均来自《海边的卡夫卡》（村上春树）。</p><ul><li><p>我自由了。我闭起眼睛，就自己自由了这点思索一阵子。但是，我还不能完全理解自由这东西是怎么回事。现在明白的只是自己成了孤身一人。孤身一人住在陌生的地方，如丢了指南针丢了地图的孤独的探险家。莫非这就是自由的含义？连这点我都稀里糊涂。于是我不再思索。</p></li><li><p>百年之后，置身此处的人们（也包括我）应该从地上荡然无存，化为尘埃化为灰烬。如此一想，我产生了一种不可思议的心情。这里所有的人或物都显得虚无缥缈，仿佛即将被风吹散消失。我伸开自己双手定定地细看。我到底为了什么如此东奔西窜呢？何苦这么苦苦挣扎求生呢？</p><p>但我摇摇头，不再往外看，不再想百年后的事。要想现在的事，图书馆有该看的书，体育馆有要对付的器材。考虑那么远的事又有什么用呢！</p><p>（我倒不觉得生活是苦苦挣扎地求生，我现在倒十分享受“挣扎”的生活。不知道以后的生活会怎么样，最近几年我倒不曾感到生活的苦恼&gt;快乐，人世间还是十分值得来走上一遭的）</p></li><li><p>时间对于他不是主要问题。手表他都没戴。中田自有适合于中田的时间流程。早晨来了即变量，太阳落了即天黑。天黑了就去左近澡堂，从澡堂回来就想睡觉。星期天澡堂有时不开，那是扭头回家即可。吃饭时间到了自然饥肠辘辘，领补贴那天来了（总有人告诉他那天快到了），即知一个月已过。领来补贴的第二天去附近理发店理发。夏天到了，区里的人让他吃鳗鱼；正月来了，区里的人为他送年糕。</p><p>（这样的生活对于中田这种非正常思维的人来说简直比我们大多数人的生活还要快乐，没什么烦恼可言。但是对大多数人来说，过这种生活都不会很快乐吧）</p></li><li><p>…那里有一切，但没有部分。没有部分，也就没必要将什么和什么换来换去。无须卸掉或安上什么。不必冥思苦索，委身于一切即可。对中田来说，那是比什么都值得庆幸的。</p><p>（村上春树的作品总是会突然来上这么一大段高深的描写，以我粗浅的文学水平，有时候实在觉得摸不着头脑，不过我看书向来不求甚解，读不懂的文字也不会反复读，毕竟我个人认为我还在培养阅读兴趣和积攒阅读量的时候，看不懂倒也十分正常。不过有时候，看罢整部作品，会回味过来这些奇怪的描写有何韵味，比如《挪威的森林》当中开篇直子关于“深井”的描述，不过这种情况倒是少数）</p></li><li><p>尽管世界上每一个体的存在是艰辛而孤独的，但就记忆的原型而言我们则密不可分地连在一起——对先生这种一以贯之的世界观我非常理解。因为，在人生旅途中，我本身也有许多同样的感受。请允许我在偏远的地方为你祝福。</p></li><li><p>几乎所有的事情都被淡忘。无论是那场大战，还是无可挽回的人之生死，一切都正在成为遥远的往事。日常生活支配了我们的心，诸多大事如冰冷古老的星球退往意识外围。我们有太多必须日常思考的琐事，有太多必须从头学习的东西：新的样式、新的知识、新的技术、新的话语…可是与此同时，也有的东西无论经历多长时间无论其间发生什么也是绝对忘却不了的。有磨损不掉的记忆，有要石一般存留心中的场景。</p></li><li><p>那是生死攸关的体验。后来好歹离开，重新回到井外生活当中。至于主人公从那场体验中得到了什么教训，生活态度是否因此改变，对人生是否有了深入思考，以及是否对社会形态怀有疑问…凡此种种作品都没有写，他作为一个人成长起来那种类似筋骨的东西也几乎没有。读完后有一种莫名其妙的心情——这部小说到底想说什么呢？不过怎么说呢，这“不知其说什么”的部分奇异地留在了心里。倒是很难表达清楚。</p><p>（这种“奇怪”的作品当然可以从各个角度理解出读它的意义，不过对我来说，倘若我去读那样的作品，最简单粗俗的意义就是见识更多了，见识了不同的人的思考方式与其生活经历，这自然也不坏）</p></li><li><p>三四郎在故事中成长。碰壁，碰壁后认真思考，争取跨越过去。不错吧？而《矿工》的主人公则截然不同，对于眼前出现的东西他只是看个没完没了，原封不动地接受而已。一时的感想之类诚然有，却都不是特别认真的东西，或者不如说他总是在愁眉不展地回顾自己闹出的恋爱风波。至少表面上他下井时和出井后的状态没多大差别。也就是说，他几乎没有自己作出过判断或选择。怎么说呢，他活得十分被动。不过我是这样想的：人这东西实际上恐怕是很难以自己的力量加以选择的。</p><p>（我们通常看的故事的主人公似乎都十分的正派，似乎遇到困难就会努力克服之并且成长，但是实际上很多人也许并不是这样的积极，也许这才符合大部分人的生活写照）</p></li></ul><p>本想以整理阅读笔记作为放松，花了快一小时的时间怎么感觉更累了不少呢。今天先整理到这里，再听歌放松一会，争取今天把OS干掉。</p><p>----------------------------------------------手动分割线</p><p>我又回来了，今天花了一个上午总算把OS的期末考核给了结了，阶段性的小胜利。趁着等外卖的功夫，继续整理读书笔记吧。</p><ul><li><p>不过有一点可以断言：某种具有不完美性的作品因其不完美而强有力地吸引人们的心——至少强有力地吸引某种人的心。比如你为漱石《矿工》所吸引。因为那里边有《心》和《三四郎》那样的完美作品没有的吸引力。你发现了那部作品。换言之，那部作品发现了你。舒伯特的《D大调奏鸣曲》也是如此，那里边具有惟独那部作品才有的拨动人心的方式。</p><p>（听起来很有道理，但是目前的我，甚至还不具备评判作品是否具有不完美性的能力，现阶段的看书似乎只是单方面的输入，顶多对输入的内容加以思考和过滤，但是并不具备评判的能力）</p></li><li><p>而我倾听《D大调奏鸣曲》，从中听出人之活动的局限，得知某种不完美性只能通过无数不完美的聚集方能具体表现出来，这点给我以鼓励。</p></li><li><p>刚听的时候我也感到单调，你那样的年龄那是当然的。但你很快就会领悟。在这个世界上，不单调的东西让人很快厌倦，不让人厌倦的大多是单调的东西。向来如此。我的人生可以有把玩单调的时间，但没有忍受厌倦的余地。而大部分人分不出二者的差别。</p><p>（<strong>不单调的东西让人很快厌倦，不让人厌倦的大多是单调的东西</strong>，此前倒是从来没从这个角度考虑过问题。在直接的潜意识里，单调和厌倦似乎是挂钩的，单调的东西容易惹人厌倦。但是似乎我们真正一直坚持的都是单调的事情，日复一日的上学/上班，坚持跑步/健身，不过这究竟是因为单调事物本身更易坚持还是因为外界的压力导致，也很难说清楚。不单调的东西会让人很快厌倦吗？很难说，比如打游戏，如果把一个人学习/上班的时间全都换成打游戏，他会快乐吗？可以长久坚持吗？我不知道其他人是何反应，但是我肯定是无法坚持的）</p></li><li><p>“你已习惯孤独了。”大岛说。我点头。“不过，孤独的种类也林林总总，其中很可能有你预想不到的孤独。”“比如什么样的？”大岛用指尖顶了一下眼镜桥：“无可奉告。因为<strong>孤独因你本身而千变万化。</strong>”</p></li><li><p>可是中田君，世上讲不通这种道理的地方也是有的，谁也不为你考虑什么合适不合适的情况也是存在的，这东西你必须理解。</p></li><li><p>但没有办法。没有痛苦是不行的。注定如此。又是注定。喏喏，这里面注定的事委实太多了，奈何奈何！</p><p>（注定的事情确实太多。但是为什么没有痛苦是不行的呢？人人都有痛苦是必然的了。但是这必然性又是什么，苦其心志劳其筋骨吗？</p></li><li><p>“闭眼睛不行！”琼尼-沃克斩钉截铁地说，“这也是注定事项，不能闭眼睛。<strong>闭了眼睛情况也丝毫不会好转</strong>。不是说闭起眼什么就会消失，恰恰相反，睁开眼时事情变得更糟。我们居住的就是这样的世界。中田君，要好好睁开眼睛。**闭眼睛是怯懦的表现，把眼睛从现实移开是胆小鬼的行为。**即使在你闭眼捂耳之时，时间也照样挺进。”</p><p>（“克服恐惧最好的办法就是面对恐惧，加油，奥里给！”很熟悉的话语，其实和上面文学性的表述也差不多。逃避确实不会改变困难和事实，直面困难确实有时候让人从感官上觉得困难变小了。比如我自己经常的体会是：当你很冷的时候，你越是打哆嗦和缩成一团，你就越觉得冷。如果你昂首挺胸，直接面对寒冷，反而并不觉得那么冷了。但是想想这也绝非什么定理，有时候躲避恐惧确实不坏。比如小时候去那种类似鬼屋的地方玩，我吓得不行就直接闭眼睛，似乎困难也更小了）</p></li></ul><p>好了，近期想要记录的句子都整理完毕啦。《海边的卡夫卡》才看了不到二分之一呢。现在准备回家包饺子啦。明天回将乐去。生活很美好~</p>]]></content>
    
    
    <summary type="html">好久没写读书笔记了，最近在看《海边的卡夫卡》~</summary>
    
    
    
    <category term="书海徜徉" scheme="https://yangyy.top/categories/%E4%B9%A6%E6%B5%B7%E5%BE%9C%E5%BE%89/"/>
    
    
    <category term="阅读笔记" scheme="https://yangyy.top/tags/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>2022年终总结</title>
    <link href="https://yangyy.top/posts/536100284.html"/>
    <id>https://yangyy.top/posts/536100284.html</id>
    <published>2022-12-31T14:59:45.000Z</published>
    <updated>2023-01-12T14:03:35.339Z</updated>
    
    <content type="html"><![CDATA[<p>提笔还是这么困难。我曾以为写年终总结时的灵感会像我回忆这一年做过什么一样，思如泉涌。年终总结这事儿，倒不是第一次做了。最早的年终总结应该还是给小刘写小作文的时候，热恋时候的我们，会在新年、除夕等等时候给对方写上一大段话，表达爱意、许下承诺什么的。这种习惯没有坚持太久，热恋期过了就不再坚持了，能坚持固然很好、不能坚持也不觉得遗憾。后来也许是大一吧，还会在朋友圈发九图年终总结什么的。现在大家都这么做，我反而不愿意这样做了。如果本着想让大家知道这一年过的怎么样的想法，那么也许你费尽心思地挑图片、码字，在年终总结的洪流当中，也不会真的有谁在意；如果本着对自己的一年做总结，又何必发在朋友圈里呢，看的人多了自然和自己独自反思不同。我只想自己回顾这一年，有什么辛酸苦辣，自己知道就好。若有缘的朋友在博客上看到我的年终总结，自然也是一件趣事。</p><p>这一年值得回忆的事情太多，就让我用记流水账的形式记录下来吧。</p><h4 id="一月"><a class="header-anchor" href="#一月">¶</a>一月</h4><p>我还记得去年的这时候已经开始考试周了，大概是12月底的那一周和1月初的那一周是考试周，中间两场考试的间隔是四天左右，为跨年空出了时间。记得考完中间最后一场，我就飞奔出校，坐上了刚通的地铁，去往市区。中午应该是在舅舅家吃了午饭。然后下午去找小刘了，在某个地下商场吃了火锅。晚上在清冷的市区漫无目的地逛着。当时错过了回去的末班地铁，只能选择打车回学校。当时又不舍得分别，于是两人商量着向着津南校区的方向散步，这样既可以散步又可以缩短车程节省车费，哈哈。很喜欢那一夜的感觉，天津市的夜晚，冷清、干冷得很舒服。还记得我向小刘分享了我当时特别喜欢的赵雷的《理想》。</p><img src="/posts/536100284/p1.jpg" alt="image" style="zoom: 50%;"><p>还记得1.7考试周的最后一天我才考完。约了因为考试周复习饥渴已久的兄弟们，一考完试就马上离开学校，去了津门羊大爷，点了一桌子的肉，还去外面的小摊上买了烤冷面、烤鱿鱼，喝了一桌的啤酒。这似乎居然是今年最后一次和兄弟出学校吃肉喝酒。接着去了KTV唱歌。总体来看是非常美好的一夜。</p><img src="/posts/536100284/p.jpg" alt="image" style="zoom:50%;"><p>不幸的是，第二天，奥密克容登录中国，没错，天津是第一站，就在津南。那可是我国第一次迎战奥密克容。所幸当时我已经离开学校不在津南了。还记得当时学校的同学们逃荒似的逃离学校，当时没有出租车的时候，甚至有同学走路到西青区打车。反正大家都很狼狈，我也很狼狈地提早买了机票回上海了。为此，小刘还和我生气了。不过所幸，我做了正确的决定。能平安回家就不错了。</p><p>回家之后就是难忘的14天集中隔离了。其实当时蛮期待的。想挑战一下独处14天，看看自己是不是能很好地客服孤独感。我想我是挑战成功了。实际上我很怀念那一段日子。虽然一个人在房间里呆了十几天，但是买了弹力带坚持健身，也在里面写完了期末作业，睡了很多…总之就是和自己相处的很顺利。唯一难受的就是那时候吃的单调，想吃炸鸡火锅都吃不了。但是我还是经常会想过那样的生活的哈哈。那时候身材管理非常成功。</p><img src="/posts/536100284/p3.jpg" alt="image" style="zoom:50%;"><p>隔离出来就是一月底了，一月份似乎没什么值得回忆的了。</p><h4 id="二月份"><a class="header-anchor" href="#二月份">¶</a>二月份</h4><p>二月的第一天，去了欢乐谷。体验了绝顶雄风、谷木游龙、跳楼机等等。感觉玩的最多的就是各种各样的过山车。实际上过程中还是蛮怕的，但是非常刺激。一年玩一次还是不错的。记得跳楼机是最后玩的，已经怕的不敢玩了。全程都怕得要死。</p><img src="/posts/536100284/p4.jpg" alt="image" style="zoom:50%;"><p>二月中旬回将乐去了。见到了一些可爱的小屁孩，家里过年的感觉真好。</p><img src="/posts/536100284/p5.jpg" alt="image" style="zoom:50%;"><img src="/posts/536100284/p6.jpg" alt="image" style="zoom:50%;"><p>去年的二月份还去了三明，和高中的兄弟们约了一天。想来似乎也快一年没见了。和高中的兄弟们虽然联系不多，但是还是很怀念那时候的感情。我还是很期待什么时候跑男团能一起再去二中的操场上跑一个3km。一定会有的，我相信。</p><img src="/posts/536100284/p7.jpg" alt="i" style="zoom:50%;"><p>接着就是开学了。不过没能顺利返校，还是上了快两周的网课。回学校之后就是漫无天日的封校生活了。没有尽头的核酸检测，根本就出不去的校门。</p><h4 id="三月份"><a class="header-anchor" href="#三月份">¶</a>三月份</h4><p>三月份应该是我第一次萌生了转专业的想法。也意味着我大半年焦虑的开始。不过也很幸运，焦虑和努力有了回报。我也最终在十月份得偿所愿。</p><p>三月份照惯例参加了队里的二月二龙抬头。我突破了自己，平板支撑时间突破到了二十一分钟，幸运地拿下了队里的第三名。坚持会有意义的。</p><p>（其实已经跨过了2022年，感觉要写不完了，赶赶进度，哈哈）</p><p>甚至到了3.18天津还下了一场大雪。</p><img src="/posts/536100284/p8.jpg" style="zoom:50%;"><p>三月份已经开始感到大二下学期的push了。计组、数据库、算法导论、人工智能导论、软件安全、并行、计算方法。感觉是最累的一学期了（因为大三上转专业到软件了，往后的日子应该都不会有这么多作业和压力了）。</p><p>似乎也是这段时间，开始看东北往事那种抽象的乐子了。</p><h4 id="四月份"><a class="header-anchor" href="#四月份">¶</a>四月份</h4><p>四月份学期到了中间。</p><p>好吧，暂时停更了，今天不想写了。困了。。。。。。</p><p>------------------------------------------------------------------------------------手动分割线，我又回来了，在写完操作系统期末报告之后。1.1的中午，似乎也不算晚。</p><p>四月份，封校继续、抽检继续，大部分时候都是网课。这个时候我意识到了这学期是我提升排名的机会。因为之前不封校的时候，每到周末都会因为龙舟队的训练和聚餐而失去比较多的学习时间。在CS这么卷的专业，似乎显得比较奢侈。不过这么想倒不是说后悔什么的，只是当封校剥夺了我训练和玩乐的机会，那我就有更多的时间专心学习。上网课也去除了为了上课奔波的时间。总体来说除了不能出去玩略显遗憾，我还是非常享受那样的时光的。</p><p>四月份似乎也是上海疫情开始严重的时候。家里人在上海，老是担心他们感染了。在那时候看，感染新冠似乎还是一件非常可怕的事情，而在年底，我们却都感染过了。世殊事异啊。</p><img src="/posts/536100284/p9.jpg" style="zoom:50%;"><p>校外的美与热闹，还是牵动着校内同学们的心，校园集市上开始一年中经典的骂学校、骂市教委的现象。好在封校的日子不算长，闹的不算激烈。</p><p>也是在四月份，我当时开始晚上吃水煮餐。倒不是说多自律，实在是吃学校其他的东西吃腻了，觉得水煮也很好吃。于是就开始吃水煮餐了。平均下来一餐16左右吧，如果我正常吃快餐应该是11/12这样，还是贵了一些。</p><img src="/posts/536100284/p10.jpg" style="zoom:50%;"><p>因为疫情，没有比赛、没有健身房训练，队里状态松懈，好像开会还被老大骂了。记得这是当时被骂之后第一次训练。去了泰达之后，这样的氛围就渐渐离我远去了。</p><p>4.17是我的20岁生日，还记得喝完酒兄弟们给我买的小蛋糕和奇怪的蜡烛。一人一口把“生日蛋糕”吃完了。</p><p><img src="/posts/536100284/p11.jpg" alt=""></p><h4 id="五月份"><a class="header-anchor" href="#五月份">¶</a>五月份</h4><p>当时嫌弃天气太热，脑子一热就去剃了个寸头，当时好像是3mm的推子吧。丑是真的丑，但是凉快是真的凉快。</p><img src="/posts/536100284/p12.jpg" style="zoom:50%;"><p>云鹏在健身房训练的时候不小心把牙磕掉了一块，还好没有大碍。</p><p>也是在这个月底，大家被封得忍无可忍了，集市上大家的措辞越发激烈。感觉是有人在恶意渲染，推动一些运动。隔壁天大的学生居然在某个晚上集会了，一群人围在一起，大喊什么“打倒官僚主义、打倒资本主义”之类的，说实话感觉有点过了。不过学校倒是因此提前把学生遣返回家了。</p><p>因此五月底，在校的最后几天的主题就是送别。一方面云鹏和小明要去当兵了，一切顺利的话，在当时看来是没有机会再见了；另一方面是中华哥这个老大哥也毕业了。虽然最后云鹏和小明没走，但是这可能也是我们缘分未尽吧。也许是十五的月亮十六圆，我还要和云鹏作为配桨，为了队伍十六周年的荣誉奋斗呢！</p><p><img src="/posts/536100284/p13.jpg" alt=""></p><p><img src="/posts/536100284/p14.jpg" alt=""></p><p>当一切告别完毕，我就和小刘一块儿回家去了。</p><img src="/posts/536100284/p15.jpg" style="zoom:33%;"><h4 id="六月份"><a class="header-anchor" href="#六月份">¶</a>六月份</h4><p>刚回家就去乡下的爷爷家隔离去了。前一两天爷爷奶奶去三明了，就我一个人住。还记得自己煮的蛋花汤面简直和屎一样哈哈。</p><img src="/posts/536100284/p16.jpg" style="zoom:25%;"><p>十几天的居家隔离，应该是我第一次在爷爷家住这么久吧。他很高兴，给我煮各种好吃的。还去洋布买了一大盆羊肉，吃了好几天。</p><p>在乡下的日子不能运动十分难受，那时候开始跳刘耕宏健身操。感觉确实蛮有用的，不觉得累，但是心率很高，出汗很多，跳完效果也很好。只不过后来一直没有合适的场合再跳（还是觉得有些羞耻）。</p><img src="/posts/536100284/p17.jpg" style="zoom: 25%;"><p>隔离十几天之后我就迫不及待地去将乐了。办了健身房的卡，开始规律的健身生活。</p><p>六月份的ddl不算太多，相比七八月份还是轻松不少的。</p><p>六月份的尾巴，去驾校报名学车了。之前放假老是在上海待一段时间在将乐待一段时间，都没有完整的时间来学车。这次机缘巧合，就想着把车给学完了。</p><p>六月份很特殊地线上拍了南开龙例行的毕业照。</p><p><img src="/posts/536100284/p18.jpg" alt=""></p><p>六月份的蹲腿训练拍到了自己股四头肌最好的状态。后来膝盖受伤之后就逐渐退化了，唉。</p><img src="/posts/536100284/p20.jpg" style="zoom: 25%;"><h4 id="七月份"><a class="header-anchor" href="#七月份">¶</a>七月份</h4><p>七月份就开始很忙了。暑期实训、还未忙完的ddl、学车，全都挤在一起了。不过这也是很值得我骄傲的一段时间。学期末的ddl都非常出色的完成了，暑期实训也算圆满结束了。学车嘛，倒是刚开始。</p><p>健身也没落下，那时候练的比较凶，一天无氧+有氧+腹肌训练，连着练了很长一段时间，也没有休息日。跑步也是3/5km和变速跑交叉，当时刚发现变速跑的快乐，也刚刚掌握跑步机的技巧，甚至跑姿也让自己十分满意。然后就把自己的膝盖跑伤了。膝盖的伤病让我不舒服了半年，知道写年终总结的今天，我也不知道自己恢复的怎么样了。反正跑步成绩是距离从前差远了，希望新年能慢慢恢复吧。这算是2022年我最大的遗憾了吧。</p><p>学车蛮顺利的。</p><p>七月底和小刘以及朱子林夫妇去平潭旅游了。一年中唯一一次自由的气息。海边真的很美好。</p><img src="/posts/536100284/p22.jpg" style="zoom:25%;"><img src="/posts/536100284/p21.jpg" style="zoom:25%;"><h4 id="八月份"><a class="header-anchor" href="#八月份">¶</a>八月份</h4><p>八月份总体还是很忙碌。当时一面仍然在纠结转专业的事情，另一面主要就是开学考复习和学车的交织。一面想赶在去学校之前把驾驶证拿下，另一面要准备开学的六七门考试，当时队伍还召集我们提前去学校开展集训。权衡再三之后我还是把集训给推了，时至今日我也不后悔。</p><p>当时压力蛮大的，因为学车这件事不是决定学就能拿下的，还有可能因为自己的原因考证失败。当时一面紧凑地学车、准备考试，一面还要抓紧时间复习考试。</p><p>不过七八月的努力都在九月得到了回报。</p><h4 id="九月份"><a class="header-anchor" href="#九月份">¶</a>九月份</h4><p>九月份的开头就是开学考得到了回报，基本每一门考试都取得了令自己满意的成绩，学分绩提高了0.8分，排名也进步了十几名，算是阶段性的小胜利吧。</p><p>然后就是转专业的事情，焦虑担心了半年，终于如愿转到软件工程专业了，具体的情况之前也写帖子回顾过了。总之就是处境的豁然开朗。</p><p>当然也还是遇到了一些问题，就不一一细说了。</p><p>在九月下旬搬到了泰达校区。</p><p>一切都比我想象的令我适应。我非常喜欢在泰达的生活。唯一的不足就是运动器械的不足。</p><p>九月底也申请加入了张老师的课题组，做异常检测的项目。</p><p>总之，九月份是收获的一个月，也是过渡的一个月。</p><h4 id="十月份"><a class="header-anchor" href="#十月份">¶</a>十月份</h4><p>十月份我又开始吃水煮餐了。这一次减脂的原因是选了游泳课，还是希望在课上保持良好的身材的，因此就减脂了一段时间。后来因为疫情，游泳课也不怎么上了，加上水煮实在是性价比不高，价格贵了好多，就放弃了。</p><p>到了十月份膝盖还是很不舒服。申请出学校去泰达医院做了一次检查。医生倒是说膝盖没有什么明显的损伤，主要是下肢力线不正。感觉就是小腿外翻吧，确实是自己没有控制好运动量，把自己折腾伤了。</p><img src="/posts/536100284/p23.jpg" style="zoom:25%;"><p>医生推荐去外面的机构定制矫正垫，但是感觉是个坑，仔细权衡之后就放弃了。打算静养。有氧运动就改成游泳为主了。</p><p>十月中旬在津南笑言宿舍的空床搭起了自己的临时宿舍。在淘宝上买了一套一百元左右的床具，在津南也算有了落脚的地方。</p><p>月底开始追了《龙之家族》第一季，可惜剧集很短，意犹未尽之下我又把后续的故事都看完了。新的一季据说要2024才播，瞬间没有了期待。</p><img src="/posts/536100284/p24.jpg" style="zoom:25%;"><p>感觉权游世界非常和我胃口，没有其他美剧让我如此着迷了。</p><h4 id="十一月份"><a class="header-anchor" href="#十一月份">¶</a>十一月份</h4><p>十一月份的开始被创高给难住了。以前创高从来不会是令我头疼的问题，但是今年情况有些特殊。一方面是我自己的膝盖不舒服，急需休息；另一方面是我搬到了泰达校区，在津南能跑创高的日子并不多。但是却要一学期跑70km。显然还是比较头疼。后来就花钱解决问题——找人代跑了。</p><p>十一月份没有什么值得回忆和记录的。一切就都在正常当中推进。不过十一月份已经显露出要放开的迹象了。暴风雨前的平静。</p><p>十一月份的惊喜应该就是买了林筠皓的头戴式耳机，款式非常喜欢，头戴式耳机也让我更沉迷于听音乐了。</p><h4 id="十二月份"><a class="header-anchor" href="#十二月份">¶</a>十二月份</h4><p>果不其然，学校的防疫还是被击穿了，当校内有了阳性病例之后，学生的情绪非常难以控制。我们预想学校也不能组织线下的考试。民意沸腾之后，学校再次提前把我们遣返了。在十二月初，我时隔一年来到了上海，见到了爸妈姐姐奶奶。</p><p>虽然疫情还是比较严重，但是我还是没忍住办了健身卡。为了让自己尽量晚一些🐏，我选择戴口罩健身，这倒没什么不适。</p><p>不过该来的总会来的，在十二月的尾巴，我们家还是无一例外全部中招了。最难受的就是发烧的两天，加上嗓子疼，病程算三天吧，之后就没什么感觉了。不过为了保险起见，我一直没有运动。打算再休息一两天开始恢复运动了。</p><p>十二月份又找了一份科研实习的工作。这给了我的生活很大的压力。希望我能处理好吧。</p><h4 id="总结"><a class="header-anchor" href="#总结">¶</a>总结</h4><p>回顾这一年，可以说是值得回忆的、令我骄傲的一年。因为疫情原因，虽然没有很多玩乐值得回顾。但是在学习上确实算是丰收的一年。</p><p>养成了自己的学习习惯，会做好每一次的平时作业，会好好对待期末的任务。我想这是好的。</p><p>感觉自己成长了，也变得更努力了。之前也不是摆烂，但是总是感觉努力的很一般。转专业之后对于自己的定位变了，也有了相对明确的追求，push着自己慢慢走下去。</p><p>这一年看的书少了很多，希望自己重新捡起来。到了年底，因为疫情原因，养了一身的肥肉，新的一年也希望保持身材管理。</p><p>明年的年终总结就可以写下自己的去向了吧。北京还是华东，学硕、专硕还是直博，做什么方向，一切似乎都还是迷雾重重。希望我能选一条让自己满意的路吧。</p><p>我只希望一直这样走下去，慢慢地学习，慢慢地变好，慢慢地品味生活。我不是一个很努力、很执着于学习的人，但是我是一个想保持学习状态的人。一鸣哥的“承认自己走得慢”让我对自己的习惯有了新的看法。</p><p>希望未来能和更多的人接触，在放开之后，去看更精彩的世界。</p><p>2022，年终~</p>]]></content>
    
    
    <summary type="html">创建博客的第一年，开始第一篇属于自己的年终总结吧！</summary>
    
    
    
    <category term="生活的点点滴滴" scheme="https://yangyy.top/categories/%E7%94%9F%E6%B4%BB%E7%9A%84%E7%82%B9%E7%82%B9%E6%BB%B4%E6%BB%B4/"/>
    
    
    <category term="自言自语" scheme="https://yangyy.top/tags/%E8%87%AA%E8%A8%80%E8%87%AA%E8%AF%AD/"/>
    
    <category term="年终总结" scheme="https://yangyy.top/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>学习DCN模型</title>
    <link href="https://yangyy.top/posts/465045716.html"/>
    <id>https://yangyy.top/posts/465045716.html</id>
    <published>2022-12-19T08:49:44.000Z</published>
    <updated>2023-01-12T14:03:35.414Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言-v6"><a class="header-anchor" href="#前言-v6">¶</a>前言</h3><p>DCN模型由论文《Deep &amp; Cross Network for Ad Click Predictions》提出。本次对DCN模型的学习也就是对该论文的阅读。</p><h3 id="补充"><a class="header-anchor" href="#补充">¶</a>补充</h3><p>阅读论文到一半突然对这两篇关于DCN模型的论文有些开窍了。不管是DCN模型还是DCN-V2模型，他们都服务于广告点击率的预测，大致的任务也就是输入一则广告的相关信息（包含各类特征信息），我们的模型应该输出对于其点击率的预测。个人认为，这种任务和输入图像、输出分类结果没什么区别。所以其实一开始也就是采用DNN（深度神经网络）这种模型，只不过效果不算好，因此改进，加入交叉网络而已。</p><h3 id="论文阅读"><a class="header-anchor" href="#论文阅读">¶</a>论文阅读</h3><h4 id="简介-v2"><a class="header-anchor" href="#简介-v2">¶</a>简介</h4><p><strong>点击率（Click Through Rate，CTR）预测</strong>是一个大规模问题。</p><p>在广告业中，广告商向出版商付费，<strong>流行的支付模式是每次点击的成本</strong>。因此，出版商的收入很大程度依赖于准确预测CTR的能力。</p><p>**<font color="red">识别经常预测的特征同时探索少见的交叉特征</font>**是做出好的预测的关键。但是网络规模的推荐系统的数据大多是离散和分类的，导致特征空间大而稀疏。限制采用线性模型。</p><p>引入一种新的神经网络结构——交叉网络。该网络以自动的方式明确应用特征交叉。</p><p>联合训练交叉网络和深度神经网络。</p><h4 id="深度和交叉网络DCN"><a class="header-anchor" href="#深度和交叉网络DCN">¶</a>深度和交叉网络DCN</h4><p>在本节描述深度和交叉网络（DCN）模型的架构：从一个嵌入和堆叠层开始，然后是平行的交叉网络和深度网络，最后是组合层，将两个网格的输出结合起来。如下图所示：</p><img src="/posts/465045716/i1.png" alt="image" style="zoom: 40%;"><h5 id="嵌入和堆积层（Embedding-and-Stacking-Layer）"><a class="header-anchor" href="#嵌入和堆积层（Embedding-and-Stacking-Layer）">¶</a>嵌入和堆积层（Embedding and Stacking Layer）</h5><p>考虑具有稀疏和密集特征的输入数据。<font color="red">输入的数据大多是分类特征</font>，例如“country=usa”，这样的特征也会变**<font color="red">编码为独热向量</font><strong>，如[0, 1, 0]，但是这样会</strong><font color="red">导致特征空间很高</font>**。</p><p>为了降低维度，采用一个**<font color="red">嵌入程序，将这些二进制特征转化为密集的实值向量（嵌入向量）</font>**。</p><p><img src="/posts/465045716/i2.png" alt="image"></p><p>其中Xembed,i是嵌入向量，xi是第i类的二进制输入，Wembed,i是相应的嵌入矩阵，<strong>嵌入矩阵会随其他参数一起被优化</strong>。</p><p>最后，嵌入向量于归一化的密集特征堆叠（stack）成一个向量x0。</p><p><img src="/posts/465045716/i3.png" alt="image"></p><h5 id="交叉网络"><a class="header-anchor" href="#交叉网络">¶</a>交叉网络</h5><p>交叉网络的关键思想是**<font color="red">以一种有效的方式应用显示的特征交叉</font>**。</p><p>交叉网络由交叉层组成，每层有以下公式：</p><p><img src="/posts/465045716/i4.png" alt="image"></p><p><img src="/posts/465045716/i5.png" alt="image"></p><p>交叉网络的参数数量少，限制了模型的容量，为了捕捉高度非线性的相互作用，引入一个并行的深度网络。</p><h5 id="深度网络"><a class="header-anchor" href="#深度网络">¶</a>深度网络</h5><p>深度网络的结构相比其他的深度神经网络的结构会更简单一些，是一个**<font color="red">全连接的前馈神经网络</font>**，每层有以下公式：</p><p><img src="/posts/465045716/i6.png" alt="image"></p><h5 id="组合层"><a class="header-anchor" href="#组合层">¶</a>组合层</h5><p>组合层将两个网络的输出连接起来，将串联的向量送人标准的logits层。</p><p>以下是一个二分类概率的公式：</p><p><img src="/posts/465045716/i7.png" alt="image"></p><p>其中XL1是交叉网络的输出，hL2是深度网络的输出（<strong><font color="red">二者转置之后串联起来成为一个向量</font></strong>），Wlogits是组合层的权重向量，<font color="red">相当于两个一维向量点积得到一个数</font>，仍然会输入σ(x)=1/(1+exp(-x))。</p><p>损失函数如下：</p><p><img src="/posts/465045716/i8.png" alt="image"></p><p>yi是真实标签，pi是计算出的概率。</p><p><strong><font color="red">但是为什么输出的是概率？不是应该是点击率吗？还是说点击率也按区间转化成了标签？</font></strong></p><h4 id="交叉网络的分析"><a class="header-anchor" href="#交叉网络的分析">¶</a>交叉网络的分析</h4><p>主要就是分析DCN当中，交叉网络的有效性。</p><p>这里暂时也不太看得懂。</p><h4 id="实验结果"><a class="header-anchor" href="#实验结果">¶</a>实验结果</h4><p>这一节主要是评估DCN的性能。</p><h5 id="Criteo数据集"><a class="header-anchor" href="#Criteo数据集">¶</a>Criteo数据集</h5><p>Criteo数据集是为了预测广告的点击率，有13个整数特征和26个分类特征，每个类别都有很高的cardinality。因此对于这个数据集而言，0.001的logloss都是很有意义的。</p><p>该数据集包含7天（约4100万条记录），使用前6天的数据用于训练，第七天的数据随机拆分为同等大小的验证集和测试集。</p><h5 id="实施细节"><a class="header-anchor" href="#实施细节">¶</a>实施细节</h5><p>官方的代码是用TensorFlow实现的。</p><h4 id="结语"><a class="header-anchor" href="#结语">¶</a>结语</h4><p>论文的大致阅读就到这里了，对于复现模型而言，性能测试和基线方法的比较可以暂时不谈，毕竟支撑的知识还是太少太少。接下来开始具体的复现这个DCN模型吧。</p><h3 id="复现DCN模型"><a class="header-anchor" href="#复现DCN模型">¶</a>复现DCN模型</h3><h4 id="学习他人的复现："><a class="header-anchor" href="#学习他人的复现：">¶</a>学习他人的复现：</h4>]]></content>
    
    
    <summary type="html">在尝试利用RecBole复现DCN-V2模型的时候发现根本看不懂，所以先学习一下DCN模型吧。</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="科研实习" scheme="https://yangyy.top/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/"/>
    
    <category term="RecBole" scheme="https://yangyy.top/tags/RecBole/"/>
    
  </entry>
  
  <entry>
    <title>利用RecBole框架复现DCN-V2模型</title>
    <link href="https://yangyy.top/posts/2997172935.html"/>
    <id>https://yangyy.top/posts/2997172935.html</id>
    <published>2022-12-19T07:42:25.000Z</published>
    <updated>2023-01-12T14:03:35.398Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一、-论文来源"><a class="header-anchor" href="#一、-论文来源">¶</a>一、 论文来源</h4><p>本次复现的DCN-V2模型由《DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems》提出。</p><h4 id="二、-数据集"><a class="header-anchor" href="#二、-数据集">¶</a>二、 数据集</h4><p>本模型在Criteo数据集上运行。Criteo数据集通过RecBole框架提供的方法可转换为.INTER类型的原子文件。</p><h4 id="三、-论文阅读-模型复现"><a class="header-anchor" href="#三、-论文阅读-模型复现">¶</a>三、 论文阅读 &amp; 模型复现</h4><h5 id="1、-简介"><a class="header-anchor" href="#1、-简介">¶</a>1、 简介</h5><p>学习排名（Learning to Rank，LTR）是现代机器学习和深度学习的重要问题之一。在LTR模型中，<font color="red"><strong>学习有效的特征交叉</strong></font>吸引关注。</p><p>有效的特征交叉提供了超出个别特征的额外的交互信息。</p><p>为了更准确地捕获有效的特征交叉，常见的补救措施是通过<font color="red">更宽或者更深的网络</font>进一步增加模型的容量。但是<font color="red">提高模型性能的同时，降低了服务速度</font>。</p><p>Deep&amp;Cross Network（DCN）是有效和优雅的。</p><p>DCN-V2首先通过交叉层学习内部的显性特征交互，然后与深度网络结合，学习互补的隐性交互。DCN-V2的核心是交叉层。</p><h5 id="2、相关工作"><a class="header-anchor" href="#2、相关工作">¶</a>2、相关工作</h5><p>（好吧，其实读到这里都没有看懂，希望后面具体描述能看懂）</p><h5 id="3、DCN-V2"><a class="header-anchor" href="#3、DCN-V2">¶</a>3、DCN-V2</h5><p>（感觉有必要先学习一下DCN模型）</p>]]></content>
    
    
    <summary type="html">熟悉了RecBole框架之后，那就复现一篇论文中的模型吧</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="科研实习" scheme="https://yangyy.top/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/"/>
    
    <category term="RecBole" scheme="https://yangyy.top/tags/RecBole/"/>
    
  </entry>
  
  <entry>
    <title>熟悉RecBole框架</title>
    <link href="https://yangyy.top/posts/627310391.html"/>
    <id>https://yangyy.top/posts/627310391.html</id>
    <published>2022-12-18T08:31:23.000Z</published>
    <updated>2023-01-12T14:03:35.454Z</updated>
    
    <content type="html"><![CDATA[<p>最近在争取一份科研实习工作，第一步的任务是学习Pytorch框架（因为我之前没有用过）。肝了一天半的时间之后得到了第二个任务，熟悉RecBole框架。</p><p>这相对来说更难一些，因为这是老师课题组自己开发的框架，推荐算法也是我没有接触过的领域。但是继续冲吧。只是不知道什么时候可以做课内的ddl，课内的绩点也不能松懈呀。</p><p>RecBole框架相应的论文是《RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms》。先读这篇论文吧。</p><h4 id="摘要-v3"><a class="header-anchor" href="#摘要-v3">¶</a>摘要</h4><p>近年来涌现了大量的推荐算法。<font color="red">志在提出一个统一、全面、高效的推荐系统库，RecBole</font>。在28个数据集上实现了73个推荐算法。该框架是基于Pytorch实现的。</p><h4 id="1、简介"><a class="header-anchor" href="#1、简介">¶</a>1、简介</h4><p>推荐算法飞速发展，很难以统一的方式或框架实现比较的基线方法。但是实际上推荐算法的许多通用组件是比较相似的，所以提出了这个统一的框架。</p><p>RecBole库主要特点和能力如下：</p><ul><li>统一的推荐框架：采用PyTorch开发。三大核心部分：数据模块、模型模块、评估模块。封装通用的组件、函数或程序。</li><li>通用和可扩展的数据结构：在两个层面实现了支持的数据结构。用户层面：原子文件，表征四种主流推荐任务的输入；算法层面：引入通用的数据结构Interaction。</li><li>全面的基准模型和数据集：实现了73种推荐算法，纳入了28个常用的数据集来评估推荐系统。</li><li>高效的GPU加速执行：暂时不用学，我也没有可用的GPU算力。</li><li>广泛和标准的评估协议：…。</li></ul><h4 id="2、RecBole库"><a class="header-anchor" href="#2、RecBole库">¶</a>2、RecBole库</h4><h5 id="2-1-数据模块"><a class="header-anchor" href="#2-1-数据模块">¶</a>2.1  数据模块</h5><h6 id="2-1-1-整体数据流"><a class="header-anchor" href="#2-1-1-整体数据流">¶</a>2.1.1 整体数据流</h6><p>原始输入-&gt;原子文件-&gt;Dataset-&gt;Dataloader-&gt;算法。</p><p><font color="red">数据流涉及两种特殊的数据形式：用户和算法</font>。</p><p>对于数据准备，定义六种原子文件类型，统一用户层面的输入。<font color="red">原子文件能描述不同推荐任务所需的大多数形式的输入数据</font>。</p><p>在算法层面，引入灵活的数据结构交互，为不同的推荐算法提供了<font color="red">统一的内部数据表示</font>。</p><h6 id="2-1-2-原子文件"><a class="header-anchor" href="#2-1-2-原子文件">¶</a>2.1.2 原子文件</h6><p>原子文件是<strong>描述各种推荐任务的输入</strong>的基本组件。试图<strong>总结和统一主流推荐任务的最基本的输入形式</strong>。</p><p>总结出四种基本数据类型：token、token sequence、float、float_sequence。前两者表示离散特征（ID、类别），后两者表示连续特征（价格）。</p><ul><li>.INTER是所有推荐任务中<font color="red">必须使用的文件</font>，每一行由用户ID、项目ID、用户项目评分、时间戳、评论文本组成。</li><li>.USER是<font color="red">用户档案文件</font>。包含用户的分类和特征。</li><li>.ITEM是项目特征文件，描述项目特征。</li></ul><p><strong><font color="red">语境感知的推荐一般就需要用到inter、user、item了</font></strong>。</p><ul><li><p>.KG是知识图谱文件，用于基于知识的推荐。每一行对应一个&lt;头实体、尾实体、关联ID&gt;的三元组。</p></li><li><p>.LINK用于基于知识的推荐。记录了推荐系统项目和知识图谱实体之间的对应关系。</p></li><li><p>.NET是社会网络文件，用于社会推荐。</p></li></ul><p><strong>原子文件可以覆盖大多数主流推荐任务的输入，如果不足以支持，可以以灵活的方式引入新的原子文件。</strong></p><h6 id="2-1-3-推荐任务的输入文件"><a class="header-anchor" href="#2-1-3-推荐任务的输入文件">¶</a>2.1.3 推荐任务的输入文件</h6><p>基于上述的原子文件，<font color="red">可以利用其<strong>组合</strong>来推进五种主流的推荐任务</font>：一般推荐、情境感知推荐、基于知识的推荐、顺序推荐和社交推荐。（目前实现前四种）</p><h6 id="2-1-4-内部数据结构Interaction"><a class="header-anchor" href="#2-1-4-内部数据结构Interaction">¶</a>2.1.4 内部数据结构Interaction</h6><p>Interaction被<font color="red">输入到推荐算法</font>中。</p><p>为了使其统一灵活，实现为基于<strong>python.dict</strong>的新的抽象数据类型（一个键值索引的数据结构）。键值对应输入的特征；值对应张量tensor。</p><h5 id="2-2-模型模块"><a class="header-anchor" href="#2-2-模型模块">¶</a>2.2 模型模块</h5><p>模型模块组织推荐算法的实现。</p><h6 id="2-2-1-统一的实现接口"><a class="header-anchor" href="#2-2-1-统一的实现接口">¶</a>2.2.1 统一的实现接口</h6><p>实现一个新的模型，只需重新规定接口与输入和评估模块连接。利用接口函数calculate_loss()进行训练，predict用于测试。同时建立了许多损失函数作为组件。</p><h6 id="2-2-2-已实现的模型"><a class="header-anchor" href="#2-2-2-已实现的模型">¶</a>2.2.2 已实现的模型</h6><p>实现了73个推荐模型。</p><h6 id="2-2-3-丰富的辅助功能"><a class="header-anchor" href="#2-2-3-丰富的辅助功能">¶</a>2.2.3 丰富的辅助功能</h6><p>比如说自动参数调整：给一个参数集，搜索最佳性能的最佳值。还有模型保存和模型加载。支持从存储的中断点恢复模型学习。等等等等</p><h5 id="2-3-评价模块"><a class="header-anchor" href="#2-3-评价模块">¶</a>2.3 评价模块</h5><p>功能是实现推荐系统常用的评价协议。不同的模式可以在同一个评价模块下进行比较。</p><h6 id="2-3-1-评价指标"><a class="header-anchor" href="#2-3-1-评价指标">¶</a>2.3.1 评价指标</h6><p>同时支持<font color="red">基于价值和基于排名的评价指标</font>。</p><p>基于价值的评价就是利用均方根误差、平均误差，衡量真实值和预测值之间的预测误差；基于排名的评价，用于top-K项目推荐，衡量算法生成的推荐名单的排名性能。</p><h6 id="2-3-2-评价设置"><a class="header-anchor" href="#2-3-2-评价设置">¶</a>2.3.2 评价设置</h6><p><strong><font color="red">主要分歧在于“基于排名top-K项目推荐”的评价</font></strong>。重点不是提供最合适的评价，而是提供大多数广泛采用的评价。</p><p>同时，本库提供了一种可能性，就是可以比较各种模型在不同评价协议下的性能。</p><p>为了方便各种评估设置，将相关函数封装成四个主要部分：Group、Split、Order、NegSample。</p><h6 id="2-3-3-Top评价的加速策略"><a class="header-anchor" href="#2-3-3-Top评价的加速策略">¶</a>2.3.3 Top评价的加速策略</h6><p>计算Top-K评价指标通常很耗时。原因在于，需要评估每个用户-项目对的得分。不同用户在测试集有不同数量的真实项目，所以补全，没有的就说负无穷。</p><p>…</p><h6 id="2-4-4-效率和可扩展性"><a class="header-anchor" href="#2-4-4-效率和可扩展性">¶</a>2.4.4 效率和可扩展性</h6><h4 id="3-该库的使用实例"><a class="header-anchor" href="#3-该库的使用实例">¶</a>3. 该库的使用实例</h4><h5 id="3-1-运行现有的模型"><a class="header-anchor" href="#3-1-运行现有的模型">¶</a>3.1 运行现有的模型</h5><p>库中包含的模型可以用固定参数或自动调参来运行。</p><h6 id="3-1-1-固定参数的模型运行"><a class="header-anchor" href="#3-1-1-固定参数的模型运行">¶</a>3.1.1 固定参数的模型运行</h6><p>（i）对数据集进行格式化：首先用户选择数据集，根据脚本进行格式化，可以为不同的数据集生成所需的原子文件。代码：atomic_file=PreProcess(dataset)</p><p>（ii）生成配置。可以通过不同的方式生成配置。可以写一个配置文件，在主函数当中读取这个文件；可以在命令行包含参数；可以在代码中写入参数字典。（<font color="red">这里的参数不是模型的参数，是程序的参数</font>）</p><p>（iii）筛选和拆分数据集。提供丰富的辅助功能来过滤和分割数据集。</p><p>（iv）加载模型。</p><p>（v）训练和评估。</p><h6 id="3-1-2-参数调整（-font-color-red-说是超参数调整更为合理-font-）"><a class="header-anchor" href="#3-1-2-参数调整（-font-color-red-说是超参数调整更为合理-font-）">¶</a>3.1.2 参数调整（<font color="red">说是超参数调整更为合理</font>）</h6><p>可以根据所提供的超参数范围来优化一个给定的模型。</p><p>（i）设置参数范围：在hyper.test文件中提供候选参数。在该文件中，每一行的格式都是参数=[value1, value2, …, valuen]</p><p>（ii）设置调优方法：在库hyperopt的基础上实现的。给定一组参数值，有四种调优方法“网格搜索”、“随机搜索”…代码：hy =HyperTuning(objective, tuning_method,range_file)</p><p>（iii）用户通过hy.run()启动调优进程。输出保存最优参数和相应的模型性能。</p><h5 id="3-2-实现新的模型"><a class="header-anchor" href="#3-2-实现新的模型">¶</a>3.2 实现新的模型</h5><p>基于RecBole，通过实例化以下三个函数实现一个新模型：</p><p>（i）实现<strong>init()函数</strong>：在这个函数中，用户执行参数初始化、全局变量定义等。新的模型是提供的抽象模型类的子类，目前为止，已经实现了一般四类推荐算法的抽象类。</p><p>（ii）实现<strong>calculate_loss()函数</strong>：计算新模型要优化的损失，库将自动调用不同的优化方法学习模型。</p><p>（iii）<strong>predict()函数</strong>：从输入数据到输出结果。</p><h4 id="4-与现有的库的比较"><a class="header-anchor" href="#4-与现有的库的比较">¶</a>4. 与现有的库的比较</h4><p>其实此前也有一些推荐系统库。编程语言逐渐从C/C++/JAVA到Python/Tensorflow/PyTorch。</p><p><strong><font color="red">RecBole的目标是简化新算法的开发过程。</font></strong></p>]]></content>
    
    
    <summary type="html">争取科研实习的第二步，熟悉RecBole框架。</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="科研实习" scheme="https://yangyy.top/tags/%E7%A7%91%E7%A0%94%E5%AE%9E%E4%B9%A0/"/>
    
    <category term="RecBole" scheme="https://yangyy.top/tags/RecBole/"/>
    
  </entry>
  
  <entry>
    <title>“导师有约下午茶”听汪定老师分享有感</title>
    <link href="https://yangyy.top/posts/2873798393.html"/>
    <id>https://yangyy.top/posts/2873798393.html</id>
    <published>2022-11-17T08:16:50.000Z</published>
    <updated>2023-01-12T14:03:35.371Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="65ea7a07bc74c12c524ca377095765be63f32c9bd0f43b2d6b95763dd1f52eab">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">这篇文章不能给你看哦，不过你要是猜对了密码，那就看叭~</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">也许应该多听一些讲座和经验之谈，对吗？</summary>
    
    
    
    <category term="漫无目的地思考" scheme="https://yangyy.top/categories/%E6%BC%AB%E6%97%A0%E7%9B%AE%E7%9A%84%E5%9C%B0%E6%80%9D%E8%80%83/"/>
    
    
    <category term="自言自语" scheme="https://yangyy.top/tags/%E8%87%AA%E8%A8%80%E8%87%AA%E8%AF%AD/"/>
    
  </entry>
  
  <entry>
    <title>大三上第十二周组会回顾</title>
    <link href="https://yangyy.top/posts/2942378457.html"/>
    <id>https://yangyy.top/posts/2942378457.html</id>
    <published>2022-11-16T07:45:41.000Z</published>
    <updated>2023-01-12T14:03:35.414Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言-v5"><a class="header-anchor" href="#前言-v5">¶</a>前言</h4><p>上上周的组会主要是确定了数据集的问题，老师安排了来自字节跳动和挑战赛的两份数据集。</p><p>学长让我处理挑战赛的数据集，学长本人处理字节跳动的数据集。</p><p>上周的组会的内容主要就是我们两人汇报自己的处理结果。</p><p>老师认为挑战赛的数据比较适合研究，再加上由于我知识有限，处理的不太到位，以及学长需要本人了解数据集的结构，所以上周学长再次处理了挑战赛的数据集，就没给我安排其他工作。</p><p>由于我自己处理，得出了对这份数据集一个初步的认知，今天组会学长和老师再根据这份数据集进行了讨论，让我有了更多的收获。</p><h4 id="正文-v3"><a class="header-anchor" href="#正文-v3">¶</a>正文</h4><p>数据集来源是一个集群，里面有node-1~node-6六个节点（物理机）。</p><p>我们需要研究的是部署在其上的服务（service），多个服务共同构成一个微服务架构。</p><p>实际上部署服务的节点主要是node-5和node-6。</p><p>不同的服务分散在这两个节点上（二者有共同的服务），并且每个service有三个实例（封装成container）。</p><p>服务之间有相对复杂的调用关系和拓扑结构。</p><p>因此各个服务的时间序列异常也许会有较强的关联。</p><p>每个service约有40+个指标（KPI）。</p><p>因此实际上会有成百上千个时间序列。</p><p>目标是在微服务的层次上进行多指标时间序列异常检测。</p><h4 id="总结-v2"><a class="header-anchor" href="#总结-v2">¶</a>总结</h4><p>实际上在上周处理AIOps挑战赛复赛数据集的时候就已经对此有一个大致的认知了，但是这个认知是模糊的。此前对于课题组的大致方向也不是很理解。</p><p>但是经过学长重新整理了一遍数据框架，以及听了学长和老师的讨论之后，不管是对数据集还是对研究方向都有了更深的认知。</p>]]></content>
    
    
    <summary type="html">记录一下本周组会的收获~</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="异常检测" scheme="https://yangyy.top/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
    <category term="组会" scheme="https://yangyy.top/tags/%E7%BB%84%E4%BC%9A/"/>
    
  </entry>
  
  <entry>
    <title>GraphStructures01阅读笔记</title>
    <link href="https://yangyy.top/posts/38813817.html"/>
    <id>https://yangyy.top/posts/38813817.html</id>
    <published>2022-11-15T10:31:28.000Z</published>
    <updated>2023-01-12T14:03:35.347Z</updated>
    
    <content type="html"><![CDATA[<p>本次阅读的论文是《<strong>Learning Graph Structures With Transformer for Multivariate Time-Series Anomaly Detection in IoT</strong>》，主要目的是了解图结构在异常检测当中的应用。</p><h4 id="摘要"><a class="header-anchor" href="#摘要">¶</a>摘要</h4><p>IoT系统会产生大量的<strong>多变量时间序列数据</strong>。异常检测对此非常重要。但是和此前研读的论文不同的是，物联网是一个整体系统，我们不能孤立地考虑其中的时间序列数据，需要考虑<strong>传感器之间复杂的拓扑关系和非线性关系</strong>（需要对此建模）。</p><p>本文介绍GTA，用于<strong>多变量时间序列异常检测</strong>，包括：<strong>自动学习一个图结构</strong>、图卷积、使用基于转换器的架构对时间依赖性建模。</p><h4 id="一-简介"><a class="header-anchor" href="#一-简介">¶</a>一. 简介</h4><p>IoT系统传感器很多，多个传感器形成多个指标，因此形成多变量时间序列。<strong>整体状态的检测</strong>更为重要。多个传感器的数据以<strong>复杂</strong>的拓扑逻辑和非线性方式<strong>相关</strong>。</p><p>现实的问题是：<strong>传感器依赖关系不可见</strong>，获取成本极高，可否自己建立？</p><p>现有的方法分为两条线：<font color="red">基于重建的模型（R-model）</font>和<font color="red">基于预测的模型（F-model）</font>。</p><p>图卷积网络（GCNs），图形神经网络（图形节点通过结构传播信息），<strong>对传感器的拓扑关系进行建模</strong>。</p><p>大多数现有的基于图的方法：通过测量<strong>传感器嵌入之间的余弦相似度</strong>学习图结构，并将最前K个节点定义为源节点连接，然后用graph attention convolution捕捉信息传播过程。</p><p>对此提出问题：</p><p>（1）传感器之间的点乘（余弦相似度）导致时间和空间的复杂性。</p><p>（2）空间距离紧密不代表拓扑结构强关联。</p><p>对此，提出了GTA（异常检测的图形学习与转化器）。通过基于Gumbel-Softmax采样技巧的连接学习策略，从学习涉及实体内所有传感器的全局双定向图结构的角度设计。</p><p>可以在训练过程中自动发现隐藏的关联。</p><h4 id="二-相关工作"><a class="header-anchor" href="#二-相关工作">¶</a>二. 相关工作</h4><h5 id="A-单变量时间序列异常检测"><a class="header-anchor" href="#A-单变量时间序列异常检测">¶</a>A. 单变量时间序列异常检测</h5><p>传统的框架：估计&amp;检测，。<strong>估计值与实际值的差异</strong>，也是我最早理解的方法。</p><h5 id="B-多变量时间序列异常检测"><a class="header-anchor" href="#B-多变量时间序列异常检测">¶</a>B. 多变量时间序列异常检测</h5><p>利用多个变量之间的<strong>关联性</strong>来<strong>提高</strong>异常检测的<strong>准确性</strong>。</p><h4 id="三-问题陈述"><a class="header-anchor" href="#三-问题陈述">¶</a>三. 问题陈述</h4><p>考虑的是<strong>一个实体内</strong>，由多个传感器检测数据构成的多变量（还没有到跨实体的层次）。</p><p><strong>他们认为正常数据和异常数据高度不平衡，因此只在正常数据上建模（继承基于预测的策略）</strong>。基于窗口大小为n的历史数据，预测x(t)。</p><p>每个测试时间点有一个异常分数，由此决定是否异常。</p><p><strong>定义与图有关的基本概念：</strong></p><p><strong>有向图</strong>的定义G（V, E）和之前一样，点代表传感器。</p><p><strong>节点的领域</strong>，就是和节点相邻的点。</p><h4 id="四-方法论"><a class="header-anchor" href="#四-方法论">¶</a>四. 方法论</h4><p>物联网系统中，传感器系统有复杂的拓扑关系。此前的方法应用<strong>不同的距离计量方法</strong>衡量节点的关系，选最接近的K个作为邻居。</p><p>现在，设计一个有向图结构的LP，自动学习节点间的关系矩阵。LP的核心被命名为Gumbel-Softmax。</p><p>被发现的隐藏关联随后被送入图卷积层。</p>]]></content>
    
    
    <summary type="html">学习Graph Structures的第一篇论文笔记</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="异常检测" scheme="https://yangyy.top/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
    <category term="Graph Structures" scheme="https://yangyy.top/tags/Graph-Structures/"/>
    
  </entry>
  
  <entry>
    <title>leetcodeT5最长回文子串题解</title>
    <link href="https://yangyy.top/posts/4207987680.html"/>
    <id>https://yangyy.top/posts/4207987680.html</id>
    <published>2022-11-15T07:13:22.000Z</published>
    <updated>2023-01-12T14:03:35.371Z</updated>
    
    <content type="html"><![CDATA[<h4 id="题目描述如下："><a class="header-anchor" href="#题目描述如下：">¶</a>题目描述如下：</h4><p><img src="/posts/4207987680/i1.png" alt="i"></p><h4 id="题解："><a class="header-anchor" href="#题解：">¶</a>题解：</h4><p>对于一个回文子串，比如“abjba”，去除首尾字符得到的“bjb”仍然是回文子串，由这样的结构容易想到<strong>动态规划</strong>。</p><p>s[i,j]表示i到j的子串。</p><p>s[i,j]是回文串的前提是<strong>s[i+1,j-1]是回文串且s[i]==s[j]</strong>，这也就能成立递归了。</p><p>直接开一个n*n的bool型数组IF[i] [j]，其值为1表示s[i,j]是回文串，否则不是，我们只需要考虑i&lt;=j的情况就好了。</p><p>首先IF[i] [i]直接赋1，因为单个字符一定是回文串。</p><p>接着按照长度开始遍历，2~n，起始点均从0开始，注意边界。需要单独处理的是长度为2/3的子串**（只需要s[i]==s[j]）**，否则按照递归条件处理。</p><p>对于是回文串的子串，需要和已存储的最长回文子串进行比较。</p><p>如此则完成。</p><p>时间复杂度O(n^2)。</p><h4 id="源代码："><a class="header-anchor" href="#源代码：">¶</a>源代码：</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">longestPalindrome</span><span class="params">(string s)</span> </span>{</span><br><span class="line">        <span class="keyword">int</span> len = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (len == <span class="number">1</span>){</span><br><span class="line">            <span class="keyword">return</span> s;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">int</span> maxLen = <span class="number">1</span>;</span><br><span class="line">        string maxs = <span class="string">""</span>;</span><br><span class="line">        maxs += s[<span class="number">0</span>];</span><br><span class="line">        vector&lt;vector &lt;<span class="keyword">bool</span>&gt;&gt; <span class="built_in">IF</span>(len, vector&lt;<span class="keyword">bool</span>&gt;(len));</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++){</span><br><span class="line">            IF[i][i] = <span class="literal">true</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> l=<span class="number">2</span>;l&lt;=len;l++){</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;len;i++){</span><br><span class="line">                <span class="keyword">int</span> j = i + l <span class="number">-1</span>;</span><br><span class="line">                <span class="keyword">if</span>(j &gt; (len<span class="number">-1</span>)){</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                }</span><br><span class="line">                <span class="keyword">if</span>(s[i] != s[j]){</span><br><span class="line">                    IF[i][j] = <span class="literal">false</span>;</span><br><span class="line">                }</span><br><span class="line">                <span class="keyword">else</span>{</span><br><span class="line">                    <span class="keyword">if</span>((j - i <span class="number">-1</span> &lt; <span class="number">2</span>)){</span><br><span class="line">                        IF[i][j] = <span class="literal">true</span>;</span><br><span class="line">                    }</span><br><span class="line">                    <span class="keyword">else</span>{</span><br><span class="line">                        <span class="keyword">if</span>(IF[i+<span class="number">1</span>][j<span class="number">-1</span>]){</span><br><span class="line">                            IF[i][j] = <span class="literal">true</span>;</span><br><span class="line">                        }</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">                <span class="keyword">if</span>(IF[i][j]){</span><br><span class="line">                    <span class="keyword">int</span> length = j - i + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">if</span>(length &gt; maxLen){</span><br><span class="line">                        maxs = <span class="string">""</span>;</span><br><span class="line">                        <span class="keyword">for</span>(<span class="keyword">int</span> k=i;k&lt;=j;k++){</span><br><span class="line">                            maxs += s[k];</span><br><span class="line">                        }</span><br><span class="line">                        maxLen = length;</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> maxs;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>以上。</p>]]></content>
    
    
    <summary type="html">太久没更新了，写一篇题解吧。</summary>
    
    
    
    <category term="算法学习" scheme="https://yangyy.top/categories/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="leetcode" scheme="https://yangyy.top/tags/leetcode/"/>
    
    <category term="动态规划" scheme="https://yangyy.top/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>异常检测挑战赛数据绘制</title>
    <link href="https://yangyy.top/posts/1394742614.html"/>
    <id>https://yangyy.top/posts/1394742614.html</id>
    <published>2022-11-01T14:02:55.000Z</published>
    <updated>2023-01-12T14:03:35.430Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言：-v2"><a class="header-anchor" href="#前言：-v2">¶</a>前言：</h4><p>上周学长和我说如果论文读的差不多了可以把数据集上的数据用matplotlib画出来。想着操纵一下真实的数据想必能对项目有更深的理解，于是开始学习。</p><p>matplotlib是Python的第三方库，确实很重要。</p><p>学习本身我也就是相当于学会怎么用”锤子“，所以也没什么值得分享和记录的。</p><p>读者有需要的可以去看莫凡Python的网课，链接如下：</p><blockquote><p><a href="https://www.bilibili.com/video/BV1Jx411L7LU/?spm_id_from=333.337.search-card.all.click&amp;vd_source=9d8000465f487fa8d1bdc7f47736df48">【莫烦Python】Matplotlib Python 画图教程_哔哩哔哩_bilibili</a></p></blockquote><h4 id="正文：-v3"><a class="header-anchor" href="#正文：-v3">¶</a>正文：</h4><p>本次操作数据集其实需要的matplotlib知识并不多。</p><h5 id="1-先了解csv文件里面数据存储的形式"><a class="header-anchor" href="#1-先了解csv文件里面数据存储的形式">¶</a>1.先了解csv文件里面数据存储的形式</h5><p>有四列属性，第一列是时间戳，第二列是节点名称，第三列是KPI名称（一个文件当中都相同），第四列是具体数值。大致如下图所示。</p><p><img src="/posts/1394742614/1.png" alt=""></p><p>一个文件基本上有四十个节点，每个节点对应一个KPI序列，因此KPI序列的x轴是timestamp，y轴是value。</p><h5 id="2-画出第一个时间序列"><a class="header-anchor" href="#2-画出第一个时间序列">¶</a>2.画出第一个时间序列</h5><p>对于每个节点，都有timestamp和value两个list，直接采用折线图（plot）的形式，即可画出序列，但是注意调整figsize（宽高比例）。</p><h5 id="3-利用subplot画出多个子图"><a class="header-anchor" href="#3-利用subplot画出多个子图">¶</a>3.利用subplot画出多个子图</h5><p>subplot就是让一个figure（一张图片）里面有多个子图（时间序列）</p><p><img src="/posts/1394742614/01.png" alt=""></p><h5 id="4-画出多个csv文件"><a class="header-anchor" href="#4-画出多个csv文件">¶</a>4.画出多个csv文件</h5><p>编写程序，遍历整个文件夹下所有csv文件，每个csv文件创建一个文件夹，每个文件夹存储该csv文件中所有时间序列。主要就是python的文件输出流了。</p><p>大抵是完成任务了。</p><p>（感觉需要记录一下，但是又不想细写，也没什么值得写的，就这样吧）</p><p>源代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个节点都有自己的名字和时间序列（timestamp&amp;value)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.time_stamp = []</span><br><span class="line">        self.value = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">url = <span class="string">"D:\MTSAD\data\初赛评分数据\\2022-05-01\cloudbed\metric\container"</span></span><br><span class="line">file = os.listdir(url)</span><br><span class="line"><span class="comment"># 这一步是因为我已经手动画过这几个文件了，所以remove</span></span><br><span class="line">file.remove(<span class="string">".DS_Store"</span>)</span><br><span class="line">file.remove(<span class="string">"kpi_container_cpu_cfs_periods.csv"</span>)</span><br><span class="line">file.remove(<span class="string">"kpi_container_cpu_cfs_throttled_periods.csv"</span>)</span><br><span class="line">file.remove(<span class="string">"kpi_container_cpu_cfs_throttled_seconds.csv"</span>)</span><br><span class="line">file.remove(<span class="string">"kpi_container_cpu_load_average_10s.csv"</span>)</span><br><span class="line">file.remove(<span class="string">"kpi_container_cpu_system_seconds.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> F <span class="keyword">in</span> file:</span><br><span class="line">    <span class="comment"># 处理单个csv文件</span></span><br><span class="line">    <span class="comment"># 每个csv文件对应一系列节点</span></span><br><span class="line">    node_list = []</span><br><span class="line">    temp = url + <span class="string">"\\"</span> + F</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(temp, mode=<span class="string">"r"</span>, encoding=<span class="string">"utf-8-sig"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        reader = csv.reader(f)</span><br><span class="line">        header = <span class="built_in">next</span>(reader)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">            time = <span class="built_in">int</span>(row[<span class="number">0</span>]) - <span class="number">1651334400</span></span><br><span class="line">            value = <span class="built_in">float</span>(row[<span class="number">3</span>])</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> node_list:</span><br><span class="line">                <span class="keyword">if</span> node.name == row[<span class="number">1</span>]:  <span class="comment"># 已经找到所属节点</span></span><br><span class="line">                    node.time_stamp.append(time)</span><br><span class="line">                    node.value.append(value)</span><br><span class="line">                    flag = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> flag:</span><br><span class="line">                temp_node = Node(row[<span class="number">1</span>])</span><br><span class="line">                temp_node.time_stamp.append(time)</span><br><span class="line">                temp_node.value.append(value)</span><br><span class="line">                node_list.append(temp_node)</span><br><span class="line"></span><br><span class="line">    tmp = F[:-<span class="number">4</span>]</span><br><span class="line">    save_path = <span class="string">"D:\MTSAD\data\img\\2022-5-01\container\\"</span> + tmp</span><br><span class="line">    os.mkdir(save_path)  <span class="comment"># 创建所属文件夹</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 画图</span></span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">24</span>, <span class="number">6</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            node = node_list[index]</span><br><span class="line">            fig.add_subplot(<span class="number">4</span>, <span class="number">1</span>, i + <span class="number">1</span>)</span><br><span class="line">            plt.plot(node.time_stamp, node.value)</span><br><span class="line">            plt.title(node.name, loc=<span class="string">'left'</span>)</span><br><span class="line">            plt.subplots_adjust(wspace=<span class="number">0</span>, hspace=<span class="number">0.6</span>)  <span class="comment"># 调整子图间距</span></span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 存储</span></span><br><span class="line">        plt.savefig(save_path + <span class="string">"\\"</span> + <span class="string">f"0<span class="subst">{j}</span>.png"</span>)</span><br><span class="line">    <span class="comment"># plt.show()</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <summary type="html">matplotlib的初步学习与应用 &amp; 异常检测数据集可视化</summary>
    
    
    
    <category term="科研" scheme="https://yangyy.top/categories/%E7%A7%91%E7%A0%94/"/>
    
    
    <category term="异常检测" scheme="https://yangyy.top/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
    <category term="matplotlib" scheme="https://yangyy.top/tags/matplotlib/"/>
    
    <category term="Python" scheme="https://yangyy.top/tags/Python/"/>
    
  </entry>
  
</feed>
